{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is an example of hypothetical sense of data of a production machine. Each row contains information about a production of a particular part, it's an associated part number. So, our goal is to predict asperity which is highly correlated with the effect if the part is healthy or faulty based under observed sense of edges. So, which column can be used to create a simple rule for this? First, we have to observe that all three columns are aggregations of some sort of sense of value, so we're losing information, but this is fine for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp1= {'partno' : 100, 'maxtemp':35, 'mintemp':35, 'maxvibration':12, 'asperity':0.32}\n",
    "dp2={'partno' : 101,'maxtemp':46,'mintemp':35,'maxvibration':21,'asperity':0.34}\n",
    "dp3={'partno' : 130,'maxtemp':56,'mintemp':46,'maxvibration':3142,'asperity':12.42}\n",
    "dp4={'partno' : 131,'maxtemp':58,'mintemp':48, 'maxvibration':3542, 'asperity':13.43}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(db):\n",
    "    if db['maxvibration']>100:\n",
    "        return 13\n",
    "    else:\n",
    "        return 0.33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict (dp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict (dp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict (dp3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict (dp4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So, now let's see if we can do better without hard coding a rule. This formula is called a linear regression model. It's called regression because it predicts a continuous value based on observations X, and weights W. Let's create our first machine learning algorithm called linear regression in Python. So, remember that we have to create a linear combination between our input fields and some parameters, W.\n",
    "### y= w0x0 + x1w1 + x2w2+..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So, let's try to adjust those values which we depict, we just take the numbers of our dataset and play around until we get a better result.\n",
    "w1=0.30\n",
    "w2=0\n",
    "w3=0\n",
    "w4=13/3412.0 # 3412.0 is must to get the answer\n",
    "def mlpredict(db):\n",
    "    return w1+w2*dp['maxtemp']+w3*dp['mintemp']+w4*dp['maxvibration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3457209847596717"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlpredict(dp4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In a real world scenario of course, those parameters W will be set by an optimizer which is part of the machine learning training. Maybe you've noticed that there is one X missing in order to get equal size vector. Therefore, we define X_0 as one. This is the bias term or the offset of the linear regression. So now, we can multiply X with W because both vectors have the same name. If check right down what we have learned before, you will come up with the following. If it doesn't look this like linear regression,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with Apache SparkMl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "u'Path does not exist: cos://advanceddatascience-donotdelete-pr-wi2dpsm1nwyabr.os_d677618706764068bebe9e144db03560_configs/hmp.parquet;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-724b4871abd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# The SparkSession object is already initialized for you.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# The following variable contains the path to your file on your IBM Cloud Object Storage.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mpath_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hmp.parquet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'advanceddatascience-donotdelete-pr-wi2dpsm1nwyabr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mpath_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/src/spark21master/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, *paths)\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'string'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'month'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'day'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \"\"\"\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/src/spark21master/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/src/spark21master/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: u'Path does not exist: cos://advanceddatascience-donotdelete-pr-wi2dpsm1nwyabr.os_d677618706764068bebe9e144db03560_configs/hmp.parquet;'"
     ]
    }
   ],
   "source": [
    "\n",
    "import ibmos2spark\n",
    "\n",
    "# @hidden_cell\n",
    "credentials = {\n",
    "    'endpoint': 'https://s3-api.us-geo.objectstorage.service.networklayer.com',\n",
    "    'api_key': 'JgVr3TfkAXS1NLjS9FZfKSwmZJo67UYeoTiYEkab9p8t',\n",
    "    'service_id': 'iam-ServiceId-8d66ef68-3f6b-47ad-83db-53846e304673',\n",
    "    'iam_service_endpoint': 'https://iam.bluemix.net/oidc/token'}\n",
    "\n",
    "configuration_name = 'os_d677618706764068bebe9e144db03560_configs'\n",
    "cos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "# Please read the documentation of PySpark to learn more about the possibilities to load data files.\n",
    "# PySpark documentation: https://spark.apache.org/docs/2.0.1/api/python/pyspark.sql.html#pyspark.sql.SparkSession\n",
    "# The SparkSession object is already initialized for you.\n",
    "# The following variable contains the path to your file on your IBM Cloud Object Storage.\n",
    "path_1 = spark.read.parquet(cos.url('hmp.parquet', 'advanceddatascience-donotdelete-pr-wi2dpsm1nwyabr'))\n",
    "path_1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression using APache SparkML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load diabetes dataset from scikitlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-6798d365b7a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdiabetes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_diabetes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdiabetes_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiabetes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdiabets_x_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiabetes_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdiabetes_x_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiabtetes_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdiabetes_y_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiabetes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'datasets' is not defined"
     ]
    }
   ],
   "source": [
    "diabetes = datasets.load_diabetes()\n",
    "diabetes_x = diabetes.data[:, np.newaxis,2]\n",
    "diabets_x_train = diabetes_x[:-20]\n",
    "diabetes_x_test = diabtetes_x[-20:]\n",
    "diabetes_y_train = np.matrix(diabetes.target[:-20]).T\n",
    "diabetes_y_test = np.matrix(diabetes.target[-20:]).T\n",
    "\n",
    "plt.scatter (diabetes_x_train, diabetes_y_train, color='black')\n",
    "plt.scatter (diabetes_x_test, diabetes_y_test, color='yellow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The x-axis denotes the input features, and the y-axis denotes the response.\n",
    "### the response values are between 0 and 400.\n",
    "### Our goal here is to train a linear regression model which attempts to find a line somewhere here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which we can then use to predict the response value which is this given the input features. In data cell, we'll denote the slope of this line by the V W and the intercept with biased B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement 3 different algorithms to train linear regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm 1   Linear regression : Direct Solve (NO Regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The built-in function solve computes the least squares solution, Ax = b, such that the norm, Ax- b, is minimized.\n",
    "### It is important to note that this function can operate only on small to medium sized input matrix.\n",
    "### Linear regression model assumes that the relationship between input feature and the response variable is linear.\n",
    "\n",
    "### The goal, then, is to estimate the regression coefficient w. We use the square loss, given here differentiating the loss with respect to w, we get this expression.\n",
    "\n",
    "### We set this expression to 0 and we arrive at the expression given here. The expression here can be computed using solve built in function, where A is x is transpose x and b is x transpose y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "script = \"\"\"\n",
    "# add constant feature to X to model intercept\n",
    "ones = matrix(1, rows=nrow(X), cols=1)\n",
    "X = cbind (X,ones)\n",
    "A = t(X) %*% X\n",
    "b = t(X) %*% y\n",
    "w = solve (A,b)\n",
    "bias  = as.sealer(w[nrow(w), 1])\n",
    "w=w[1:nrow-1,]\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The goal, then, is to estimate the regression coefficient w. We use the square loss, given here differentiating the loss with respect to w, we get this expression.\n",
    "### We set this expression to 0 and we arrive at the expression given here. The expression here can be computed using solve built in function, where A is x is transpose x and b is x transpose y. The dml script for linear regression done as simply a solve function, by first setting a and b. Setting a to x transpose x, and b to x transpose y.\n",
    "### Since we don't know whether the line passes through the origin, we add a bias. This is done by first appending and solve it all ones, using cbind function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For training, we need to pass the values for the variables x and y using the input method.\n",
    "### In this case, x is the NumPy matrix, diabetes_x_train, and y is the NumPy matrix, diabetes_y_train. Since we want to find the line with the slope w and intercept bias, we mark the WM bias as output and we get democratic seclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dml' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-b41d1568485b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscript\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdiabetes_X_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiabetes_y_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bias'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbias\u001b[0m  \u001b[0;34m=\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bias'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtonumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dml' is not defined"
     ]
    }
   ],
   "source": [
    "prog = dml(script).input (X=diabetes_X_train, y = diabetes_y_train).output('w', 'bias')\n",
    "w,bias  =ml.execute (prog).get('w', 'bias')\n",
    "w = w.tonumpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-ce272a66d620>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdiabetes_X_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiabetes_y_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'black'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdiabetes_X_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiabetes_y_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'yellow'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdiabetes_X_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdiabetes_X_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'blue'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinestyle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'dotted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.scatter (diabetes_X_train, diabetes_y_train, color='black')\n",
    "plt.scatter (diabetes_X_test, diabetes_y_test, color='yellow')\n",
    "\n",
    "plt.plot (diabetes_X_test, (w*diabetes_X_test)+ bias, color='blue', linestyle = 'dotted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm 2:   Linear Regression - Batch Gradient Descent (No Regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm: \n",
    "### Step 1: Start with an initial point w\n",
    "### while (not converged) {\n",
    "### Step2: compute gradient dw\n",
    "### Step3: compute stepsize alpha\n",
    "### step4: Update: w_new = w_old - alpha*dw \n",
    "### }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A drawback of direct sole method is that the inputs to the sole building function should be small enough. That is, both X transpose X, and X transpose Y should fit in the driver here. Hence, this method does not work for larger dataset. \n",
    "### To get around this problem, we'll implement two iterative algorithm, that's key to a larger data set: Batch Gradient Descent and Conjugate Gradient Method. Batch Gradient Descent is an extremely simple algorithm.\n",
    "### Assume that you are somewhere on the hill, and you want to reach to the bottom of the hill. Unfortunately, you don't have a GPS but you have a device that tells you slope at a given point. For higher dimensional surfaces, the slope is called as a gradient. In other word, gradient is just another name for derivative of a function and is a vector that points in the direction of the greatest increase of the function. \n",
    "### Therefore, to reach the bottom of the hill, all one has to do is compute the gradient at the current location and take a step in the opposite direction of the gradient. Then, compute the gradient at that location and take a step in the opposite direction of that gradient, and so on and so forth.\n",
    "### As shown in the figure, if the surface has multiple local minimum, different initial point can lead as two different minimum.\n",
    "### We initialize the starting point W to zero, then we take exactly 100 steps. In each step, we compute the gradient DW, and the step size alpha using the above formula. Then, we compute the new W by subtracting alpha times DW from old W. Again, as in previous example, we plot the line from the line W. As appendid below the whole work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script = \"\"\"\n",
    "# add constant feature to X to model intercept\n",
    "ones = matrix(1, rows=nrow(X), cols=1)\n",
    "X = cbind (X,ones)\n",
    "max_iter = 100\n",
    "w = matrix(0, rows=ncol(X), cols=1)\n",
    "for (i in 1:max_iter) {\n",
    "XtX = t(X) %*% X\n",
    "dw = XtX %*% w -t(X) %*% y\n",
    "alpha = (t(dw) %*% dw) / (t(dw) %*% XtX %*% dw)\n",
    "w = w - dw*alpha\n",
    "} \n",
    "\n",
    "bias  = as.sealer(w[nrow(w), 1])\n",
    "w=w[1:nrow-1,]\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prog = dml(script).input (X=diabetes_X_train, y = diabetes_y_train).output('w', 'bias')\n",
    "w,bias  =ml.execute (prog).get('w', 'bias')\n",
    "w = w.tonumpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter (diabetes_X_train, diabetes_y_train, color='black')\n",
    "plt.scatter (diabetes_X_test, diabetes_y_test, color='yellow')\n",
    "\n",
    "plt.plot (diabetes_X_test, (w*diabetes_X_test)+ bias, color='blue', linestyle = 'dotted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm 2:   Linear Regression - Conjugate Gradient (No Regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### his method benefits from using conjugacy information during optimization and usually requires far fewer steps. \n",
    "### To converge, lets compact two batch gradient descent. \n",
    "### The exact algorithm is given here. I'll skip the details of the algorithm and refer you to the key gradient. So, the exact algorithm is given here. And here is the DML script for conjugate gradient method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm: \n",
    "### Step 1: Start with an initial point w\n",
    "### while (not converged) {\n",
    "### Step2: compute gradient dw\n",
    "### Step3: compute stepsize alpha\n",
    "### step4: compute next direction p by enforcing conjugacy with previous direction\n",
    "### Step4: Update: w_new = w_old - alpha*dw \n",
    "### }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Like previous method, we'll plot the learned line.\n",
    "### If you prefer not to write the custom algorithm, but instead invoke standard of the shelf algorithm, then you can use by ten cord given in example three and example four. In example three, we invoke, we implemented algorithm by using dmlFromResource method and ML context object. \n",
    "### The pre-implemented algorithms have a label under script folder in our GitHub. All one has to do is create a script object from dmlFromResource, pass it the input feature and the response variable, that is X and Y using the input method.\n",
    "### And like previous example, we'll plot the learned line. Example four is targeted for a scikit-learn user. A scikit-learn user may want to only create a linear regression object and call the fit method. The fit method accepts the input features and response variable as Numpy matrices. That user can simply use our mllearn API, mllearn API allows a Python programmer to invoke systemML'S algorithm using a scikit-learn like API, where the input data can be Numpy Arrays, scifi matrices, all Panda data frame as well as Spark's MLPipeline API where the input data is a Spark data frame.\n",
    "### Since these APIs conform to MLPipeline's estimator interface, they can be used in tandem with ML as feature extractors, transformers, scoring, and cross validation classes. So, here's the fit method, predict method, and we'll plot the learned line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dml script\n",
    "script = \"\"\"\n",
    "# add constant feature to X to model intercept\n",
    "X = cbind (X,matrix (1, rows=nrow(X), cols=1))\n",
    "m = ncol(X); i=1;\n",
    "max_iter=20;\n",
    "w = matrix(0, rows=m, cols=1) # initialize weight to zero\n",
    "dw = t(X) %*% y; p=-dw;       # dw = (X*X)w - (X*y)\n",
    "norm_r2 = sum (dw ^ 2);\n",
    "for (i in 1:max_iter) {\n",
    "q=t(X) %*%  (X%*%p)\n",
    "alpha = norm_r2 /sum (p*q);   # minimizes f(w - alpha*r)\n",
    "w = w + alpha*p;              # update weights\n",
    "dw= dw + alpha*q;\n",
    "old_norm_r2 = norm_r2; norm_r2 = sum (dw ^ 2);\n",
    "p= -dw + (norm_r2 / old_norm_r2) * p;  # next direction - conjugacy to previous direction \n",
    "i=i+1;\n",
    "}\n",
    "\n",
    "ones = matrix(1, rows=nrow(X), cols=1)\n",
    "X = cbind (X,ones)\n",
    "max_iter = 100\n",
    "w = matrix(0, rows=ncol(X), cols=1)\n",
    "for (i in 1:max_iter) {\n",
    "XtX = t(X) %*% X\n",
    "dw = XtX %*% w -t(X) %*% y\n",
    "alpha = (t(dw) %*% dw) / (t(dw) %*% XtX %*% dw)\n",
    "w = w - dw*alpha\n",
    "} \n",
    "bias  = as.sealer(w[nrow(w), 1])\n",
    "w=w[1:nrow-1,]\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prog = dml(script).input (X=diabetes_X_train, y = diabetes_y_train).output('w', 'bias')\n",
    "w,bias  =ml.execute (prog).get('w', 'bias')\n",
    "w = w.tonumpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter (diabetes_X_train, diabetes_y_train, color='black')\n",
    "plt.scatter (diabetes_X_test, diabetes_y_test, color='yellow')\n",
    "\n",
    "plt.plot (diabetes_X_test, (w*diabetes_X_test)+ bias, color='blue', linestyle = 'dotted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, we see how to use Keras inside SystemML. There are three different ways to implement a deep learning model in SystemML using the DML bodied ML library, using the experimental Caffe2DML API, and using the experimental Keras2DML API.\n",
    "### Keras2DML and Caffe2DML accept the deep planning model expressed think Keras or Caffe former and then underneath genre T because learning DML script. \n",
    "### In this example, we train a LeNet network using MNIST dataset. We first load the MNIST dataset, then we create LeNet using Keras API. Keras model given you has two convolution layers with ReLU activation and same padding with MaxPooling layer in between. They are followed by two densely connected layers with dropout. Once we have created the Keras model, we can pass it to SystemML using Keras2DML class. We can then call fit or predict method other than mllearn API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting , Training , Validating,  Overfitting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So, let's have a look, splitting your training data is a requirement in machine learning. So consider the complete training data set. If you train a machine learning algorithm on the complete training dataset, you eventually would be very lucky because you get really good results. But now what happens if you show unseen data to the same algorithm which you have been trained? You most probably receive very bad results.\n",
    "### So let's consider a classification and a regression example. The left-hand side, you see a classification boundary. So what do you think? Which of those two lines are good classification boundaries? So one line tries to classify each and every data point correctly, on the other hand, the other line is not.\n",
    "### he same thing in regression, one line tries to match each point correctly and the other one is just straight line which somehow matches all the points, but not very precisely. So we called this process overfitting,\n",
    "### n order to prevent overfitting, a good practice is to split your training data into two sets. One remains called training data, the other one is called validation data. Good splits are, for example, on 70 or 80 percent training data and 20 or 30 percent validation data.\n",
    "### et's consider, we are doing a good job on the training data, but a bad job on the unseen validation data. This is called overfitting, because we are fitting the model too strongly to the training data, and therefore, it's not generalizing good enough to unseen data.\n",
    "### On the other hand, if you're doing a bad job on the training and on a validation data, we call this underfitting.\n",
    "### o, in summary, to make sure that your model is doing a good job on unseen data, it's essential that you split some data apart from your training data, call it validation data and have a look how your train model performs on that validation data. If it performs more or less the same than on the training data, you are doing everything exactly right. If it doesn't perform under validation data, but it performs on the training data, you are overfitting. Specifically for each mission learning models, there are some parameters you can tune in order to prevent overfitting, and we will cover those for each and every individual algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It is important to know how good or how bad your algorithm is doing on your data. So, knowing how good or how bad you are is called evaluation. o, evaluation are measures of model prediction performance.\n",
    "###  In the case of classification, this is pretty simple and straightforward. So, here I will only cover accuracy which is the one which I mostly use. It's really straightforward. You just check on your dataset, how many of the data points you are predicting correctly, and how many you are not. Then you count those up, and then you take the correctly identified examples and divided by the number of overall examples, and then you're done. So in this case, I've classified four out of six examples correctly, that means it's four over six which gives us 66 percent of accuracy.\n",
    "### 4/6 = 66.66 ( I am good in four samples out of 6 is called accuracy)\n",
    "### In the case of regression, it's slightly more complicated. It's the accuracy measure in classification. I really like measures which are normalized, that means which gives us values between zero and one. One means good, zero means bad. This is the case for accuracy.\n",
    "### therefore in regression, I really like the R2 measure. So R2 is also called R squared and is calculated as follows. \n",
    "### irst of all, you take SS total. For calculating SS total, you just subtract the mean of your target variable from each individual target variable, you square it, and you sum it up. And you do the same thing for each predicted variable. So, FI in this case is the predicted variable. So, you subtract the mean from your predicted variable, you square it, and you sum all those up, and you get SS reg. Then, R squared is nothing else than one minus SS res or over SS total. \n",
    "### o, if those measures you can assess the performance of your classifiers and your regression models. Accuracy you can use for classification, and R squared for regression. This should give you enough for now to understand how you can test the performance of your moderates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It turns out that parts with an asperity greater than one are not usable. So we consider than to be faulty or broken.\n",
    "### Let's change our table to reflect this.\n",
    "### Let's code it.\n",
    "### So let's change this regression data set into a binary classification data set. Then our rule gets more simple and also more precise.\n",
    "### And in addition, it's changed our linear regression model to an regression one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp1= {'partno' : 100, 'maxtemp':35, 'mintemp':35, 'maxvibration':12, 'asperity':0}\n",
    "dp2={'partno' : 101,'maxtemp':46,'mintemp':35,'maxvibration':21,'asperity':0}\n",
    "dp3={'partno' : 130,'maxtemp':56,'mintemp':46,'maxvibration':3142,'asperity':1}\n",
    "dp4={'partno' : 131,'maxtemp':58,'mintemp':48, 'maxvibration':3542, 'asperity':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(db):\n",
    "    if db['maxvibration']>100:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict (dp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict (dp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict (dp3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So this means we can easily turn our linear regression model into a logistic regression model and create binary classifier.\n",
    "### n order to achieve this, just add a sigmoid computation step to our site\n",
    "### So our newer site is between zero and one. And by selecting a threshold in the middle, so we can basically turn this model into binary classification model. So this looks brilliant already. So the logistics sigmoid function squashes a range from minus infinity to plus infinity to a range between zero and one.\n",
    "### Let's wait. You have to define a threshold between zero and one in order to get a binary classifier in order to get a clear separation between the classes instead of a continuous range.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def sigmoid (x):\n",
    "    return (1/1+ math.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression Model will also be changed\n",
    "# So, let's try to adjust those values which we depict, we just take the numbers of our dataset and play around until we get a better result.\n",
    "w1=0.30\n",
    "w2=0\n",
    "w3=0\n",
    "w4=13/3412.0 # 3412.0 is must to get the answer\n",
    "def mlpredict(dp):\n",
    "    return 1 if sigmoid (w1+w2*dp['maxtemp']+w3*dp['mintemp']+w4*dp['maxvibration']) > 0.7 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlpredict(dp4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with Apache SparkMl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Again, we access the file from the object store, and create a data frame out of it.\n",
    "### The below is a dummpy one as the file is not available with credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ibmos2spark\n",
    "\n",
    "# @hidden_cell\n",
    "credentials = {\n",
    "    'endpoint': 'https://s3-api.us-geo.objectstorage.service.networklayer.com',\n",
    "    'api_key': 'JgVr3TfkAXS1NLjS9FZfKSwmZJo67UYeoTiYEkab9p8t',\n",
    "    'service_id': 'iam-ServiceId-8d66ef68-3f6b-47ad-83db-53846e304673',\n",
    "    'iam_service_endpoint': 'https://iam.bluemix.net/oidc/token'}\n",
    "\n",
    "configuration_name = 'os_d677618706764068bebe9e144db03560_configs'\n",
    "cos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "# Please read the documentation of PySpark to learn more about the possibilities to load data files.\n",
    "# PySpark documentation: https://spark.apache.org/docs/2.0.1/api/python/pyspark.sql.html#pyspark.sql.SparkSession\n",
    "# The SparkSession object is already initialized for you.\n",
    "# The following variable contains the path to your file on your IBM Cloud Object Storage.\n",
    "path_1 = spark.read.parquet(cos.url('hmp.parquet', 'advanceddatascience-donotdelete-pr-wi2dpsm1nwyabr'))\n",
    "path_1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So now we create a random split 80, 30%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits  =df.randomSplit([.8, .2])\n",
    "df_train = splits [0]\n",
    "df_test = splits [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And we create our pipeline containing an indexer, a vector assembler, and a normalizer, so nothing new so far. Now we import a logistic regression model, and we instantiate it.\n",
    "### We're using the same parameters like in linear regression, because logistic regression basically is nothing else than linear regression and a sigmoid function applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder  \n",
    "from pyspark.ml.feature import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler \n",
    "from pyspark.ml.feature import Normalizer\n",
    "\n",
    "indexer = StringIndexer (inputCol = 'class', outputCol = 'classIndex')\n",
    "\n",
    "vectorAssembler = VectorAssembler (inputCols = ['x','y','z'], outputCol='features')\n",
    "\n",
    "normalizer = Normalizer (inputCol = 'features', outputCol = 'features_norm', p=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr= LogisticRegression (maxIter=10, regParam=0.3, elasticNetParam=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline (stages = [indexer, vectorAssembler, normalizer, lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So this trick goes in Apache's backdrop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model  = pipeline.fit (df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So now we compute the predictions by executing model.transform(df_train)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.transfrom (df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And it's consistent if we evaluate other models, so that's the cool advantage of pipelining.\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### o let's instantiate this multiclass specification evaluator. We call it eval.\n",
    "### And we set the metric, it'll be accuracy\n",
    "### And then we set the label to be label, and the predictions,\n",
    "### Its predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval  = MulticlassClassificationEvaluator().setMetricName ('accuracy').setLabelCol ('label').setPredictionCol ('prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we evaluate, we just pass the predictions to the evaluate method of the evaluator. And after some time, we should see the accuracy. So we got 13%, which is not good, but later in the course it will of course improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval.evaluate (prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And now we compute our predictions using the model.transform(df_test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit (df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.transform (df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval.evaluate (prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Okay, cool, it's nearly 13% as well. And that's actually cool, if we get nearly the same number but a slightly lower number, that means everything is all right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilities Refresher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hi and welcome to my sessional Naive Bayes. Naive Bayes is a very popular technique. It's used everywhere in machine learning. It's a very common in declassification but it has really a wide range of obligations.\n",
    "### So, here I have, say two boxes and these two boxes contain two types of fruits, apples and oranges, and I have a red box and a blue box. And let's say that I randomly pick one of the two boxes. Let's say that, I pick the red box with the probability of 40 percent, and I pick the blue box with a probability of 60 percent.\n",
    "### if they are mutually exclusive, their probabilities will sum up to one.\n",
    "### Marginal Probability is is the probability of the event occurring when the event is not conditioned on any other event?  So, in my case the probability of selecting the red box say, doesn't depend on anything. Now it's always 40 percent. So, the probability of B equal red, which is four-tenths is a marginal probability.\n",
    "### Joint Probability is the probability of events occurring together\n",
    "### for example, what is the probability of selecting a specific fruit from a specific box. So maybe, what's the probability of selecting the red box and the fruit, I will use F as the random variable for my fruit being an apple. A will be apple, r the orange. Right. So, that's a question we could ask. What's the probability of getting an apple from the red box. And this is what we call joint probability.\n",
    "### conditional probability is the probability of an event occurring given that another event has occurred.\n",
    "### What is the probability of the fruit being apple given that I have already selected the red box. By the way this one is fairly easy to compute because the probability of getting an apple if I have already selected the red box is what the just the fraction of apples in the red box. So, this will be, we have two over six, seven, eight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rules of probability & Bayes Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marginal Probability = P(X)\n",
    "### Joint Probability = p(X,Y)\n",
    "### Conditional Probability = p(Y/X)\n",
    "### Sum Rule P(X) = p(Y1/X1)P(X1)  (+ P (Y2/X2)P(X2) + ... incase of others)\n",
    "### The first rule that we'll use is called the sum rule and I'll express it using these two variables X and Y. So, the sum rule tells us that the marginal probability of X, one of the random variables is the sum of the joint probabilities of X and Y over Y. That's the sum rule.\n",
    "### Product Rule;  P(X,Y)= p(Y/X)p(X)\n",
    "### The second rule is the product rule, and product rule gives us the joint probabilities this time of X and Y. It tells us that the joint probability of X and Y is the conditional probability of Y given X times the marginal probability of X. \n",
    "### hese two rules are very important and they allow us to solve all kinds of problems.\n",
    "### Bayes rule; and it's a very important rule that's at the core of Naive Bayes. So, we have the sum and the product rule here. And I will start by tweaking a bit the product rule.\n",
    "### Now, there is something in probabilities that we call the symmetry property that tells us, I will write it up here, that the probability, the joint probability of X and Y is the same as the joint probability of Y and X. They're still equivalent.\n",
    "### P(X,Y) = P(Y,X)\n",
    "### o, what I will do here, I will just rewrite the numerator, which is the joint probability of X and Y, and reverse the variables. I will say that that's the joint probability of Y and X over P_X. Which is the same, \n",
    "### I'm just swapping the variables and because of the symmetry rule, it is still valid. Now, the next thing I will do is I will grab this joint probability P of Y and X, and I will actually apply again the product rule to rewrite this as a conditional probability.\n",
    "### So, I have here the probability, this probability of Y and X and now applying the product rule, but we have to be careful because the variables are now reversed. This will be the conditional probability of X given Y times the marginal probability of Y. And I will plug this here now and I will get the conditional probability of X give Y times the marginal probability of Y over the marginal probability of X.\n",
    "### Let me rewrite this here. So, at the end, we get that the conditional probability of Y given X is the conditional probability of X given Y times the marginal probability of Y over the marginal probability of X. And this is the Bayes rule.4\n",
    "### If you think about it, it allows us to reverse the direction of the conditional probability because we start with Y given X and we can express it using X given Y.\n",
    "### Another way to think about this rule is it helps us to describe the probability of an event based on some prior knowledge of events that might be related to this event. And this rule is very important.\n",
    "### bayes Rule, P(Y/X) =  P(X/Y)P(Y) / P(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Gaussian Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t's a very common continuous distribution. It is also called the normal distribution. Occurs naturally in many situations. Here are a few examples, heights of people, salaries, blood pressure, and so on.\n",
    "### It is used to represent real valued random data.\n",
    "### Here, that's the definition of the Gaussian distribution for a single real valued variable X. We can define the Gaussian using the mean of the distribution here and the variance. That's the formula that will give us the Gaussian distribution for specific parameters.\n",
    "### I plotted different versions of the Gaussian, so, they all have the same mean, the mean is zero but then the variance changes. So, sigma is the standard deviation, sigma squared is the variance.\n",
    "### You can see them here in the stable. Now because this is a valid probability density, the area under the curve sums up to one\n",
    "### small variance = maximum height with narrow spreading on the x-axis, the curve line. So, if you reduce the variance, if you have like small variance, then this curve becomes more pointy. It has to preserve this area under the curve.\n",
    "### large variance = minimum height with more spreading on the x-axis The curve line. If data is spread more, like your variance is larger, like five in this case, then it has this different shape which is lower and more spread over the x-axis. But again, this area is one.\n",
    "### It is also used to represent measurement errors, IQ tests. There are just many examples, physical quantities and so on.\n",
    "### Central Limit Theorem (CLT) : It's very useful because of something called the Central Limit Theorem (CLT) that basically tells us that if we have any distribution it doesn't have to be normal, it can be something very bizarre\n",
    "### It can be something like this, let's say that the curve looks like this. If we sample this distribution, so let's say we take an equal, I don't know, 30 samples and we take like 30 random samples from this distribution. And then we compute the average, I would say that's the average, you'll get some number. We do another sample X_2, and again we sample these distribution and we take the average, and so on\n",
    "### These numbers here that we end up with will be normally distributed. So, if you could plot them, you will get the Gaussian distribution. So we can use the Gaussian to model large number of processes that they can separately have very different distribution between and take the means of their samples or the sums they are actually Gaussian.\n",
    "### o, this is very useful property and the Gaussian keeps coming up again and again in machine learning, and it's used in many different machine learning algorithms. So this was a side note. The important thing is that's the Gaussian and using the mean in a variance we can generate different Gaussians with different shape based on how much the data is spread. Okay. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's combine our knowledge on the Gaussian distribution and also Bayes rule to introduce Bayesian inference. o, this is a method of inference where the probability of a hypothesis is updated as new evidence becomes available, which essentially means that we have some kind of a hypothesis, new data comes in and then we update these hypotheses to accommodate this new data into our historical data.\n",
    "### Inference: a conclusion reached on the basis of evidence and reasoning\n",
    "### A method of inference where the probability of hypothesis is updated as new evidence becomes available\n",
    "### e start with some existing knowledge about our random variable, and we assume as you can see here this is a Gaussian distribution that this variable is normally distributed. The next thing we do is we acquire some new data, so we get some your observations and maybe this new data will have a different mean, maybe it will have a different variance, so we plot it here on the right hand side\n",
    "### Then what we do is we compute something called the likelihood, we compute how compatible this new data is to our prior knowledge to our existing data. The final bit we do is we adjust our hypothesis, we obtain what's called the posterior, that's the probability of our hypothesis given the observed that it is\n",
    "### So, we see how probable our assumption is giving the new data that just came in\n",
    "### Computing the posterior ( P(Y/X)) = likelihood( P(X/Y)) * Prior (P(Y)) / Marginal Probability of liklihood P(X)\n",
    "### ( P(H/E)) = likelihood( P(E/H)) * Prior (P(H)) / Marginal Probability of liklihood P(E)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Inference Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data extracted from Bureau of Labor Statistics US \n",
    "### So, now talking in machine learning terms, this is my class. So, my class is in M or F, male or female, and I want to be able to answer questions like, what is the probability of the height being, I don't know 69 inches, given that the class is male? What is the probability of the height, giving 69, are being 69 given that the class is female? If I can answer these two questions, then I can make a prediction\n",
    "### What I want to show you now is how we can use Bayesian inference to actually model these probabilities and compute likelihoods, and priors, and so on, using these datas. \n",
    "### So, I start by splitting my data set into two sub-sets. I get one for the males and one for females. So, these are the males, these are the females. The data set is much larger. \n",
    "###  The data set is much larger. I've just sampled five, six, five examples for each class just to keep the calculation simple\n",
    "### So, my first step in the process is split the two classes, and then compute some statistics for the two samples. So, first of all, let's compute the prior. So, my initial hypothesis. I'm, again, focusing on the height into two subsets.\n",
    "### So, what is the prior which is the probability of the class being M, so, male? Well, that's fairly simple. This will be the number of observations that I have, 1, 2, 3, 4, 5, over the total number of data points, which is, I have another five here, 1, 2, 3, 4, 5. So, my size of the data set is 10. So, this will be five over 10. So, my prior is 0.5.\n",
    "### Of course, it's identical for the other class. We can either, again, count five and divide, or because these are mutually exclusive events, just take one minus 0.5, and get again 0.5. All right. Now, these are my priors.\n",
    "### The next thing I will do is I will compute the mean for the two sub-sets. \n",
    "### Now, I need the standard deviation for both of the classes. So, this will be, Sigma for the males will be the sum, again, over i of x_i, which are the individual samples in this data set, minus the mean, which I have just computed over here, squared over n, and I need to take the square root of this.\n",
    "### Now, say that new data comes in, running out of space, I can use this thing. Say that new data comes in, and the new data, my new evidence is a height of 69.\n",
    "### I will use the Gaussian and we had the formula right here. So, let's compute the probability using the Gaussian. Oops, yeah. So, the probability or the likelihood in this case of x being 69 given that the class is males, will be, and this is where we plugged the Gaussian formula, \n",
    "### This is the likelihood of the new data coming from this class and this class respectively, A_1 and A_2. Now, we can use this to make a prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Posterior Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### We use something called Maximum a posteriori estimation. We have the prior, we have the likelihood. We now have to compute the posterior. We will select the class which maximizes our posterior; which makes this new data\n",
    "### more compatible with our hypothesis which is CM or CF.\n",
    "### Well, our prediction I will say CMAP for maximum a posteriori will be argmax, we'll select the class which can be M or F, which maximizes the probability of the class given the evidence. This is how maximum a posteriori works. You pick the class which maximizes the the posterior.\n",
    "### We use base rule. That's the posterior we are after. So, we know how to compute the likelihood using the Gaussian, we know the priors. Now, here is the problem with the marginal likelihood, that's something we don't know.\n",
    "### So, we want to compute to do argmax to select the class which maximizes and maybe I don't have to stick to the example I can say, the class variable no matter which class it is that maximizes X given C times marginal probability of C over the probability of X. \n",
    "### So, this guy here, this is our likelihood, this is A1, A2 in our example. This we already know. This is 0.5 for the male, 0.5 for the females, it doesn't matter. Now this guy.\n",
    "### This Px, as you can see, doesn't depend on the class. It will always be the same. So for both classes. So in this case, we can treat it as a constant. So, if you want to find the biggest of these V1, V2, V3, V4 numbers and each of them is divided by the same constant. You actually don't care about the constant. If this is the biggest number and you ditch the constants, this still will be the biggest number. We are not aiming to get a valid probability here, we're just selecting the class that maximizes the posterior. So, what we can do is, we can just ditch this guy, we don't have to compute this. So we end up with argmax over the classes of PX given C P_C. This is how maximum a posteriori estimation works. In our case, we have to compute A1 times the prior here A2 and see which one is bigger. And this will be our prediction.\n",
    " ### Cmap =argmax P(C/X) = C {m,F} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Inference in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See video , python program covers all the steps which can be implemented through one command of scikit learn.\n",
    "### The backend program is written.\n",
    "### if you have a more complex dataset, if you have something more flexible, then all you should probably go with something like a SystemML or a scikit-learn or so on depending on the volumes of your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why naive bayes \"naive\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: X is a vector with many dimensions. f we compute the likelihood for, let's say, X_i given class J, and if X is a vector, then we'll have to compute this probability X one for the first element of the vector, given X two, X three, and so on until X_d end the class, times the probability of the second component X two given X three, so on until CJ, times the fourth component and so on and so on and so on. So times the probability of X_d given CJ times the marginal probability of the class.\n",
    "### And that's a very difficult computation to make. These individual attributes conditioned on the joint probabilities of everything else. \n",
    "### o to handle this, we assume that X one, X two, X three and so on until X_d are conditionally independent from each other. Well, given the class. So if this assumption here that these attributes are conditionally independent of each other given the class holds, then we can rewrite this as the probability of X one, X two, and so on X_d, which is this guy here.\n",
    "### Given the class, this simplifies to the probability of X one given the class times the probability effects to given the class times and so on the probability of X_d given the class.\n",
    "### Now, the problem is that this assumption here is a very bold assumption. It's a very naive assumption to make.\n",
    "### Now, if you're doing something like text classification and your features here are actually words, for example, you are writing a spam harm classifier. You want to detect spam email messages, right? And your features are words. If one of your words is, let's say, 'win,' and another feature here is 'money,' and that's one message, then the probability of 'money' appearing together with 'win' would be greater than say the probability of 'win.'\n",
    "### And you can easily, easily write a spam classifier that has very very high percentage of accuracy using this assumption, over 90 percent. \n",
    "### Anyway, again, this was a sideline that kind of tells you why a Naive bayes is called naive bayes because of this assumption of conditional independence that we make.\n",
    "### he final thing I would like to mention is that we talked about Gaussian Naive Bayes that's not the only type of naive bayes algorithm that you can use.\n",
    "### here are different types of implementations in general. For example, using different distribution like binomial naive bayes. For example, if your features are not continuous like the features that we used in our example, which use a continuous normal distribution, maybe you have binomial features, maybe again going back to the spam detector, your features are actually binary.\n",
    "### these are all different ways of using naive bayes, Gaussian, Binomial and multiple\n",
    "### You have a distribution, I use bayes role to compute the likelihood, and you use the priors and the likelihood to get the posterior and you use maximum posteriori."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support vector machine have been the hottest machine learning model for long. Now, they have somehow lost terrain against gradient boosting.\n",
    "###  Support vector machines makes them much and classifiers. That means, that the Support Vector Machine will always get the ideas, separation, hyperplane between point clouds. So, initially, a Support Vector Machine is the classifier, and it is a binary classifier. \n",
    "### So, here CH3 is the best operation hyperplane between the two point clouds, and that something is part of the vector machine automatically computes.\n",
    "### As you might have noticed, a hyperplane is a linear separation. So, in order to turn a Support Vector Machine into a nonlinear classifier, you can use the so-called corner trick. So, you can transform your training data using a so-called corner into another space, the so-called future space. On the left hand side, in the input space X, you definitely can't draw a line of separation between the two types of points. But, once you have transform those into another future space, you can.\n",
    "### Here I give you another intuitive example. So, the inner circle has to be separated from the outer one. Definitely not a linear function. But, now let's use that corner. A transfer function which transforms this 2D space into a 3D space. Let's use the following function. So, we take X1 and X2 and as output, we output X1 and X2. But, in addition the third dimension is now X1 squared plus X2 squared.\n",
    "### o, this looks like the following. So, we are basically propping up the points in the outer circle. Now, we can draw a hyperplane between those points and we have a separation.\n",
    "### the Support Vector Machine is other assistance to outliers. This means, it also a clear boundary between the point clouds, even if there are outliers. \n",
    "### The optimization objective of a support vector machine is convex. That's a really cool feature because you are guaranteed they've been optimizing over the cost function of a Support Vector Machine, you can always end up in the global minima.\n",
    "###  A Support Vector Machine is a binary classifier. What can we do to turn a Support Vector Machine into a mighty class classifier? This is pretty straightforward. So, let's consider this training data with different classes. So, now we just take one class and then we assign another class to all the other examples. We do this for each and every class and finally, we will end up the so-called one versus all classifier, where we just take one class versus the others and times and then we output the one with the best call.\n",
    "###  Support Vector Machine is a binary linear classifier. It always finds the best hyperplane of separation because of the context nature of the optimization objective. We can turn a Support Vector Machine into a nonlinear model using corners. We can turn the Support Vector Machine into a mighty class classifier using the one versus all approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine Using Apache SparkML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And after this process we should have a data frame. I'm intentionally not using objects still here because I want to run this notebook later on my local machine. I will explain you later why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType \n",
    "schema = StructType ([\n",
    "    StructField ('x', IntegerType(), True),\n",
    "    StructField ('y', IntegerType(), True),\n",
    "    StructField ('z', IntegerType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['README.txt',\n",
       " 'Climb_stairs',\n",
       " 'Getup_bed',\n",
       " 'MANUAL.txt',\n",
       " 'final.py',\n",
       " 'Eat_soup',\n",
       " 'Comb_hair',\n",
       " 'Liedown_bed',\n",
       " 'Sitdown_chair',\n",
       " 'Eat_meat',\n",
       " 'Descend_stairs',\n",
       " 'Brush_teeth',\n",
       " 'impdata.py',\n",
       " 'Pour_water',\n",
       " 'Walk',\n",
       " '.git',\n",
       " 'Drink_glass',\n",
       " 'Standup_chair',\n",
       " '.idea',\n",
       " 'Use_telephone']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir('HMP_Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list=os.listdir('HMP_Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object <genexpr> at 0x7fb469daf1e0>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list_filtered=(s for s in file_list if '_' in s)\n",
    "file_list_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Climb_stairs\n",
      "Getup_bed\n",
      "Eat_soup\n",
      "Comb_hair\n",
      "Liedown_bed\n",
      "Sitdown_chair\n",
      "Eat_meat\n",
      "Descend_stairs\n",
      "Brush_teeth\n",
      "Pour_water\n",
      "Drink_glass\n",
      "Standup_chair\n",
      "Use_telephone\n"
     ]
    }
   ],
   "source": [
    "for line in file_list_filtered:\n",
    "    print (line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accelerometer-2011-05-30-20-54-39-climb_stairs-f1.txt\n",
      "Accelerometer-2012-06-06-08-54-08-climb_stairs-m5.txt\n",
      "Accelerometer-2011-05-30-22-00-32-climb_stairs-m2.txt\n",
      "Accelerometer-2012-06-06-09-38-17-climb_stairs-m6.txt\n",
      "Accelerometer-2012-06-07-10-53-34-climb_stairs-f1.txt\n",
      "Accelerometer-2012-06-06-09-06-49-climb_stairs-m5.txt\n",
      "Accelerometer-2012-05-30-22-08-15-climb_stairs-m2.txt\n",
      "Accelerometer-2011-05-30-10-32-53-climb_stairs-m1.txt\n",
      "Accelerometer-2011-05-30-08-21-38-climb_stairs-f1.txt\n",
      "Accelerometer-2011-05-31-15-06-32-climb_stairs-f1.txt\n",
      "Accelerometer-2012-06-06-08-59-53-climb_stairs-m5.txt\n",
      "Accelerometer-2012-06-06-08-54-29-climb_stairs-m5.txt\n",
      "Accelerometer-2012-06-06-09-33-15-climb_stairs-m6.txt\n",
      "Accelerometer-2012-06-06-09-01-08-climb_stairs-m5.txt\n",
      "Accelerometer-2012-06-06-14-05-05-climb_stairs-m7.txt\n",
      "Accelerometer-2011-04-11-11-58-30-climb_stairs-f1.txt\n",
      "Accelerometer-2012-06-06-09-32-58-climb_stairs-m6.txt\n",
      "Accelerometer-2012-05-30-18-32-56-climb_stairs-f3.txt\n",
      "Accelerometer-2012-05-29-17-17-51-climb_stairs-m3.txt\n",
      "Accelerometer-2012-06-06-09-06-28-climb_stairs-m5.txt\n",
      "Accelerometer-2011-03-24-10-24-39-climb_stairs-f1.txt\n",
      "Accelerometer-2012-06-06-09-00-24-climb_stairs-m5.txt\n",
      "Accelerometer-2012-05-30-22-06-47-climb_stairs-m2.txt\n",
      "Accelerometer-2011-05-30-08-30-37-climb_stairs-f1.txt\n",
      "Accelerometer-2012-06-06-14-12-49-climb_stairs-m7.txt\n",
      "Accelerometer-2012-06-06-09-32-36-climb_stairs-m6.txt\n",
      "Accelerometer-2012-06-06-09-43-14-climb_stairs-m6.txt\n",
      "Accelerometer-2012-06-06-14-04-51-climb_stairs-m7.txt\n",
      "Accelerometer-2012-06-06-09-38-43-climb_stairs-m6.txt\n",
      "Accelerometer-2012-06-06-14-08-57-climb_stairs-m7.txt\n",
      "Accelerometer-2012-05-28-17-56-03-climb_stairs-m1.txt\n",
      "Accelerometer-2011-06-02-10-37-44-climb_stairs-f1.txt\n",
      "Accelerometer-2011-06-06-11-00-50-climb_stairs-f1.txt\n",
      "Accelerometer-2012-05-30-19-14-51-climb_stairs-m4.txt\n",
      "Accelerometer-2011-05-30-21-02-43-climb_stairs-f1.txt\n",
      "Accelerometer-2012-06-07-10-49-35-climb_stairs-f1.txt\n",
      "Accelerometer-2012-05-30-19-12-19-climb_stairs-m4.txt\n",
      "Accelerometer-2012-06-06-14-13-05-climb_stairs-m7.txt\n",
      "Accelerometer-2011-06-06-10-57-43-climb_stairs-f1.txt\n",
      "Accelerometer-2012-06-06-09-44-02-climb_stairs-m6.txt\n",
      "Accelerometer-2011-06-06-10-57-19-climb_stairs-f1.txt\n",
      "Accelerometer-2012-06-06-09-37-23-climb_stairs-m6.txt\n",
      "Accelerometer-2012-06-07-10-54-12-climb_stairs-f1.txt\n",
      "Accelerometer-2012-05-29-16-53-12-climb_stairs-f2.txt\n",
      "Accelerometer-2011-05-30-21-36-58-climb_stairs-m2.txt\n",
      "Accelerometer-2011-06-06-10-59-58-climb_stairs-f1.txt\n",
      "Accelerometer-2011-05-31-14-58-41-climb_stairs-f1.txt\n",
      "Accelerometer-2012-06-06-09-32-04-climb_stairs-m6.txt\n",
      "Accelerometer-2011-06-02-10-39-21-climb_stairs-f1.txt\n",
      "Accelerometer-2012-06-06-09-43-47-climb_stairs-m6.txt\n",
      "Accelerometer-2012-06-07-10-49-17-climb_stairs-f1.txt\n",
      "Accelerometer-2011-04-11-11-57-50-climb_stairs-f1.txt\n",
      "Accelerometer-2011-05-30-09-38-52-climb_stairs-f1.txt\n",
      "Accelerometer-2012-05-30-18-31-52-climb_stairs-f3.txt\n",
      "Accelerometer-2012-06-06-14-13-20-climb_stairs-m7.txt\n",
      "Accelerometer-2011-06-06-11-02-24-climb_stairs-f1.txt\n",
      "Accelerometer-2011-05-30-21-14-55-climb_stairs-f1.txt\n",
      "Accelerometer-2011-04-05-18-32-29-climb_stairs-f1.txt\n",
      "Accelerometer-2012-06-07-10-54-29-climb_stairs-f1.txt\n",
      "Accelerometer-2012-06-07-10-53-53-climb_stairs-f1.txt\n",
      "Accelerometer-2012-06-06-14-09-24-climb_stairs-m7.txt\n",
      "Accelerometer-2012-06-06-08-53-47-climb_stairs-m5.txt\n",
      "Accelerometer-2012-05-29-17-17-05-climb_stairs-m3.txt\n",
      "Accelerometer-2012-06-06-14-12-35-climb_stairs-m7.txt\n",
      "Accelerometer-2012-05-30-22-07-15-climb_stairs-m2.txt\n",
      "Accelerometer-2011-05-31-15-20-50-climb_stairs-f1.txt\n",
      "Accelerometer-2011-03-29-09-55-46-climb_stairs-f1.txt\n",
      "Accelerometer-2011-05-30-10-20-47-climb_stairs-m1.txt\n",
      "Accelerometer-2011-06-02-10-40-10-climb_stairs-f1.txt\n",
      "Accelerometer-2012-06-06-14-04-18-climb_stairs-m7.txt\n",
      "Accelerometer-2012-06-06-14-05-18-climb_stairs-m7.txt\n",
      "Accelerometer-2012-06-06-09-05-28-climb_stairs-m5.txt\n",
      "Accelerometer-2012-06-06-09-00-51-climb_stairs-m5.txt\n",
      "Accelerometer-2012-05-30-22-07-44-climb_stairs-m2.txt\n",
      "Accelerometer-2012-05-30-18-31-08-climb_stairs-f3.txt\n",
      "Accelerometer-2012-05-30-22-06-17-climb_stairs-m2.txt\n",
      "Accelerometer-2012-05-30-18-33-57-climb_stairs-f3.txt\n",
      "Accelerometer-2011-06-06-11-01-27-climb_stairs-f1.txt\n",
      "Accelerometer-2011-05-31-16-32-44-climb_stairs-f1.txt\n",
      "Accelerometer-2012-06-06-09-42-55-climb_stairs-m6.txt\n",
      "Accelerometer-2011-04-05-18-21-22-climb_stairs-f1.txt\n",
      "Accelerometer-2011-06-02-10-37-15-climb_stairs-f1.txt\n",
      "Accelerometer-2012-06-06-14-09-12-climb_stairs-m7.txt\n",
      "Accelerometer-2012-06-07-10-48-24-climb_stairs-f1.txt\n",
      "Accelerometer-2011-03-24-10-25-44-climb_stairs-f1.txt\n",
      "Accelerometer-2012-06-06-09-06-03-climb_stairs-m5.txt\n",
      "Accelerometer-2011-06-06-10-59-08-climb_stairs-f1.txt\n",
      "Accelerometer-2011-05-30-08-39-09-climb_stairs-f1.txt\n",
      "Accelerometer-2011-05-30-09-43-37-climb_stairs-f1.txt\n",
      "Accelerometer-2012-06-06-14-08-44-climb_stairs-m7.txt\n",
      "Accelerometer-2012-05-29-16-54-38-climb_stairs-f2.txt\n",
      "Accelerometer-2012-06-06-09-37-58-climb_stairs-m6.txt\n",
      "Accelerometer-2011-05-30-21-46-05-climb_stairs-m2.txt\n",
      "Accelerometer-2011-05-30-09-29-58-climb_stairs-f1.txt\n",
      "Accelerometer-2011-05-31-16-26-09-climb_stairs-f1.txt\n",
      "Accelerometer-2012-05-30-19-13-50-climb_stairs-m4.txt\n",
      "Accelerometer-2012-06-07-10-48-08-climb_stairs-f1.txt\n",
      "Accelerometer-2011-04-11-11-44-35-climb_stairs-f1.txt\n",
      "Accelerometer-2012-05-28-17-56-40-climb_stairs-m1.txt\n",
      "Accelerometer-2011-05-30-10-41-10-climb_stairs-m1.txt\n",
      "Accelerometer-2012-06-06-08-53-29-climb_stairs-m5.txt\n",
      "Accelerometer-2012-05-30-19-12-57-climb_stairs-m4.txt\n",
      "Accelerometer-2011-06-02-17-00-08-getup_bed-f4.txt\n",
      "Accelerometer-2012-06-12-15-57-21-getup_bed-f3.txt\n",
      "Accelerometer-2011-06-02-16-57-57-getup_bed-f4.txt\n",
      "Accelerometer-2011-05-31-16-30-18-getup_bed-f1.txt\n",
      "Accelerometer-2012-05-30-21-53-38-getup_bed-m2.txt\n",
      "Accelerometer-2012-06-07-22-16-42-getup_bed-f4.txt\n",
      "Accelerometer-2012-06-07-22-28-50-getup_bed-f4.txt\n",
      "Accelerometer-2012-06-07-22-30-31-getup_bed-f4.txt\n",
      "Accelerometer-2012-06-20-20-33-22-getup_bed-f5.txt\n",
      "Accelerometer-2012-06-20-21-58-13-getup_bed-m10.txt\n",
      "Accelerometer-2011-05-30-21-13-57-getup_bed-f1.txt\n",
      "Accelerometer-2011-05-30-20-53-40-getup_bed-f1.txt\n",
      "Accelerometer-2012-06-12-15-30-07-getup_bed-m4.txt\n",
      "Accelerometer-2012-06-07-22-16-05-getup_bed-f4.txt\n",
      "Accelerometer-2012-06-07-22-24-33-getup_bed-f4.txt\n",
      "Accelerometer-2012-06-12-15-29-15-getup_bed-m4.txt\n",
      "Accelerometer-2011-03-29-09-21-17-getup_bed-f1.txt\n",
      "Accelerometer-2012-06-12-18-43-26-getup_bed-f2.txt\n",
      "Accelerometer-2012-06-12-15-31-56-getup_bed-m4.txt\n",
      "Accelerometer-2012-05-30-21-54-12-getup_bed-m2.txt\n",
      "Accelerometer-2012-06-12-15-28-34-getup_bed-m4.txt\n",
      "Accelerometer-2012-06-12-15-54-03-getup_bed-f3.txt\n",
      "Accelerometer-2012-06-07-22-23-46-getup_bed-f4.txt\n",
      "Accelerometer-2011-05-30-21-59-46-getup_bed-m2.txt\n",
      "Accelerometer-2012-06-20-20-34-30-getup_bed-f5.txt\n",
      "Accelerometer-2012-06-12-18-44-04-getup_bed-f2.txt\n",
      "Accelerometer-2012-06-12-15-50-48-getup_bed-f3.txt\n",
      "Accelerometer-2012-05-28-17-54-37-getup_bed-m1.txt\n",
      "Accelerometer-2012-06-20-21-53-06-getup_bed-m10.txt\n",
      "Accelerometer-2011-04-11-13-34-05-getup_bed-f1.txt\n",
      "Accelerometer-2011-05-30-21-36-01-getup_bed-m2.txt\n",
      "Accelerometer-2012-06-12-15-35-29-getup_bed-m4.txt\n",
      "Accelerometer-2012-06-12-15-31-01-getup_bed-m4.txt\n",
      "Accelerometer-2012-05-30-21-51-41-getup_bed-m2.txt\n",
      "Accelerometer-2012-06-12-15-56-41-getup_bed-f3.txt\n",
      "Accelerometer-2012-06-12-15-58-01-getup_bed-f3.txt\n",
      "Accelerometer-2012-06-20-20-29-11-getup_bed-f5.txt\n",
      "Accelerometer-2011-05-30-10-39-27-getup_bed-m1.txt\n",
      "Accelerometer-2012-06-20-20-30-45-getup_bed-f5.txt\n",
      "Accelerometer-2012-05-30-21-52-18-getup_bed-m2.txt\n",
      "Accelerometer-2011-05-31-16-25-09-getup_bed-f1.txt\n",
      "Accelerometer-2012-06-20-20-32-40-getup_bed-f5.txt\n",
      "Accelerometer-2012-06-20-20-29-49-getup_bed-f5.txt\n",
      "Accelerometer-2011-05-31-15-19-41-getup_bed-f1.txt\n",
      "Accelerometer-2012-06-12-18-51-33-getup_bed-m3.txt\n",
      "Accelerometer-2012-06-07-22-20-59-getup_bed-f4.txt\n",
      "Accelerometer-2012-06-12-15-36-02-getup_bed-m4.txt\n",
      "Accelerometer-2011-05-30-10-27-01-getup_bed-m1.txt\n",
      "Accelerometer-2011-05-30-09-42-46-getup_bed-f1.txt\n",
      "Accelerometer-2012-06-12-18-41-40-getup_bed-f2.txt\n",
      "Accelerometer-2012-05-30-21-53-05-getup_bed-m2.txt\n",
      "Accelerometer-2011-03-29-09-24-50-getup_bed-f1.txt\n",
      "Accelerometer-2011-06-02-16-58-46-getup_bed-f4.txt\n",
      "Accelerometer-2012-06-12-18-40-26-getup_bed-f2.txt\n",
      "Accelerometer-2011-05-30-09-33-26-getup_bed-f1.txt\n",
      "Accelerometer-2012-06-20-22-00-55-getup_bed-m10.txt\n",
      "Accelerometer-2011-05-30-08-38-15-getup_bed-f1.txt\n",
      "Accelerometer-2012-06-07-22-15-16-getup_bed-f4.txt\n",
      "Accelerometer-2011-05-31-14-57-24-getup_bed-f1.txt\n",
      "Accelerometer-2012-06-12-18-48-47-getup_bed-m3.txt\n",
      "Accelerometer-2011-05-31-15-02-55-getup_bed-f1.txt\n",
      "Accelerometer-2012-05-28-17-54-04-getup_bed-m1.txt\n",
      "Accelerometer-2012-06-12-18-52-06-getup_bed-m3.txt\n",
      "Accelerometer-2011-06-02-17-22-34-getup_bed-m1.txt\n",
      "Accelerometer-2012-06-12-15-54-33-getup_bed-f3.txt\n",
      "Accelerometer-2011-04-11-11-53-01-getup_bed-f1.txt\n",
      "Accelerometer-2011-05-30-20-59-57-getup_bed-f1.txt\n",
      "Accelerometer-2012-06-12-18-44-53-getup_bed-f2.txt\n",
      "Accelerometer-2012-06-12-15-52-04-getup_bed-f3.txt\n",
      "Accelerometer-2012-06-12-18-52-37-getup_bed-m3.txt\n",
      "Accelerometer-2011-03-29-09-49-03-getup_bed-f1.txt\n",
      "Accelerometer-2012-06-20-21-52-24-getup_bed-m10.txt\n",
      "Accelerometer-2012-06-12-18-50-08-getup_bed-m3.txt\n",
      "Accelerometer-2012-06-20-20-28-27-getup_bed-f5.txt\n",
      "Accelerometer-2012-06-12-18-42-44-getup_bed-f2.txt\n",
      "Accelerometer-2012-06-12-15-51-25-getup_bed-f3.txt\n",
      "Accelerometer-2012-06-12-18-50-50-getup_bed-m3.txt\n",
      "Accelerometer-2012-06-12-15-53-20-getup_bed-f3.txt\n",
      "Accelerometer-2012-06-12-15-52-46-getup_bed-f3.txt\n",
      "Accelerometer-2012-06-07-22-19-25-getup_bed-f4.txt\n",
      "Accelerometer-2012-06-20-21-53-40-getup_bed-m10.txt\n",
      "Accelerometer-2012-06-20-20-27-19-getup_bed-f5.txt\n",
      "Accelerometer-2011-05-30-21-42-30-getup_bed-m2.txt\n",
      "Accelerometer-2012-06-20-20-31-40-getup_bed-f5.txt\n",
      "Accelerometer-2012-05-28-17-55-04-getup_bed-m1.txt\n",
      "Accelerometer-2012-06-12-18-49-31-getup_bed-m3.txt\n",
      "Accelerometer-2012-06-12-15-34-40-getup_bed-m4.txt\n",
      "Accelerometer-2012-06-20-21-55-30-getup_bed-m10.txt\n",
      "Accelerometer-2012-06-12-15-32-38-getup_bed-m4.txt\n",
      "Accelerometer-2012-06-20-21-59-57-getup_bed-m10.txt\n",
      "Accelerometer-2012-06-07-22-26-32-getup_bed-f4.txt\n",
      "Accelerometer-2012-06-07-22-17-45-getup_bed-f4.txt\n",
      "Accelerometer-2011-04-05-18-28-21-getup_bed-f1.txt\n",
      "Accelerometer-2012-06-20-21-57-33-getup_bed-m10.txt\n",
      "Accelerometer-2012-06-12-15-34-01-getup_bed-m4.txt\n",
      "Accelerometer-2011-05-30-08-26-56-getup_bed-f1.txt\n",
      "Accelerometer-2012-06-07-22-31-18-getup_bed-f4.txt\n",
      "Accelerometer-2012-05-28-17-52-54-getup_bed-m1.txt\n",
      "Accelerometer-2012-06-12-18-39-37-getup_bed-f2.txt\n",
      "Accelerometer-2012-05-28-17-53-29-getup_bed-m1.txt\n",
      "Accelerometer-2012-06-20-21-56-36-getup_bed-m10.txt\n",
      "Accelerometer-2011-03-24-13-33-22-eat_soup-f1.txt\n",
      "Accelerometer-2011-03-24-13-56-42-eat_soup-f1.txt\n",
      "Accelerometer-2011-03-24-13-44-18-eat_soup-f1.txt\n",
      "Accelerometer-2011-03-24-10-57-40-comb_hair-f1.txt\n",
      "Accelerometer-2011-06-06-10-03-47-comb_hair-f1.txt\n",
      "Accelerometer-2011-05-30-08-32-58-comb_hair-f1.txt\n",
      "Accelerometer-2011-06-02-10-44-17-comb_hair-f1.txt\n",
      "Accelerometer-2011-06-02-16-12-44-comb_hair-f1.txt\n",
      "Accelerometer-2011-06-06-10-05-40-comb_hair-f1.txt\n",
      "Accelerometer-2011-06-06-10-10-30-comb_hair-f1.txt\n",
      "Accelerometer-2011-06-02-17-39-02-comb_hair-m1.txt\n",
      "Accelerometer-2011-06-02-17-39-54-comb_hair-m1.txt\n",
      "Accelerometer-2011-06-02-17-02-50-comb_hair-f4.txt\n",
      "Accelerometer-2011-03-24-10-26-33-comb_hair-f1.txt\n",
      "Accelerometer-2011-06-02-17-18-58-comb_hair-m1.txt\n",
      "Accelerometer-2011-06-02-16-48-41-comb_hair-f4.txt\n",
      "Accelerometer-2011-06-06-10-08-56-comb_hair-f1.txt\n",
      "Accelerometer-2011-06-02-16-56-03-comb_hair-f4.txt\n",
      "Accelerometer-2011-06-02-16-49-27-comb_hair-f4.txt\n",
      "Accelerometer-2011-05-30-10-36-26-comb_hair-m1.txt\n",
      "Accelerometer-2011-05-31-15-14-55-comb_hair-f1.txt\n",
      "Accelerometer-2011-06-02-16-14-07-comb_hair-f1.txt\n",
      "Accelerometer-2011-06-02-16-20-04-comb_hair-f1.txt\n",
      "Accelerometer-2011-03-24-16-10-36-comb_hair-f2.txt\n",
      "Accelerometer-2011-06-02-10-41-33-comb_hair-f1.txt\n",
      "Accelerometer-2011-05-30-09-39-49-comb_hair-f1.txt\n",
      "Accelerometer-2011-03-24-09-44-34-comb_hair-f1.txt\n",
      "Accelerometer-2011-04-11-13-31-24-comb_hair-f1.txt\n",
      "Accelerometer-2011-06-02-17-32-57-comb_hair-m1.txt\n",
      "Accelerometer-2011-03-29-08-57-24-comb_hair-f1.txt\n",
      "Accelerometer-2011-05-30-21-57-03-comb_hair-m2.txt\n",
      "Accelerometer-2011-06-02-16-18-54-comb_hair-f1.txt\n",
      "Accelerometer-2011-06-02-17-03-33-comb_hair-f4.txt\n",
      "Accelerometer-2011-05-30-21-08-40-comb_hair-f1.txt\n",
      "Accelerometer-2011-04-05-18-27-12-liedown_bed-f1.txt\n",
      "Accelerometer-2011-05-30-10-26-14-liedown_bed-m1.txt\n",
      "Accelerometer-2011-05-30-21-13-15-liedown_bed-f1.txt\n",
      "Accelerometer-2011-05-30-21-35-19-liedown_bed-m2.txt\n",
      "Accelerometer-2011-03-29-09-19-22-liedown_bed-f1.txt\n",
      "Accelerometer-2011-05-30-20-52-31-liedown_bed-f1.txt\n",
      "Accelerometer-2011-05-31-16-29-43-liedown_bed-f1.txt\n",
      "Accelerometer-2011-05-30-20-59-04-liedown_bed-f1.txt\n",
      "Accelerometer-2011-05-30-21-41-32-liedown_bed-m2.txt\n",
      "Accelerometer-2011-05-30-09-32-42-liedown_bed-f1.txt\n",
      "Accelerometer-2011-04-11-11-52-20-liedown_bed-f1.txt\n",
      "Accelerometer-2011-05-30-21-59-16-liedown_bed-m2.txt\n",
      "Accelerometer-2011-05-30-08-37-27-liedown_bed-f1.txt\n",
      "Accelerometer-2011-06-02-16-57-35-liedown_bed-f4.txt\n",
      "Accelerometer-2011-03-29-09-45-46-liedown_bed-f1.txt\n",
      "Accelerometer-2011-05-31-14-56-04-liedown_bed-f1.txt\n",
      "Accelerometer-2011-03-29-09-23-22-liedown_bed-f1.txt\n",
      "Accelerometer-2011-05-31-16-24-36-liedown_bed-f1.txt\n",
      "Accelerometer-2011-05-31-15-02-19-liedown_bed-f1.txt\n",
      "Accelerometer-2011-04-11-13-33-26-liedown_bed-f1.txt\n",
      "Accelerometer-2011-06-02-16-59-13-liedown_bed-f4.txt\n",
      "Accelerometer-2011-05-30-09-41-49-liedown_bed-f1.txt\n",
      "Accelerometer-2011-06-02-17-21-57-liedown_bed-m1.txt\n",
      "Accelerometer-2011-06-02-17-00-38-liedown_bed-f4.txt\n",
      "Accelerometer-2011-05-30-08-25-59-liedown_bed-f1.txt\n",
      "Accelerometer-2011-05-31-15-18-50-liedown_bed-f1.txt\n",
      "Accelerometer-2011-05-30-10-38-41-liedown_bed-m1.txt\n",
      "Accelerometer-2011-06-02-16-58-20-liedown_bed-f4.txt\n",
      "Accelerometer-2011-12-05-09-48-48-sitdown_chair-f1.txt\n",
      "Accelerometer-2011-06-02-17-42-45-sitdown_chair-m1.txt\n",
      "Accelerometer-2011-12-11-08-24-32-sitdown_chair-m2.txt\n",
      "Accelerometer-2012-05-28-17-40-02-sitdown_chair-m1.txt\n",
      "Accelerometer-2011-06-02-16-43-56-sitdown_chair-f4.txt\n",
      "Accelerometer-2011-05-30-21-04-13-sitdown_chair-f1.txt\n",
      "Accelerometer-2011-06-02-16-45-19-sitdown_chair-f4.txt\n",
      "Accelerometer-2011-05-30-09-23-12-sitdown_chair-f1.txt\n",
      "Accelerometer-2011-06-06-09-36-06-sitdown_chair-f1.txt\n",
      "Accelerometer-2011-12-05-09-46-34-sitdown_chair-f1.txt\n",
      "Accelerometer-2011-06-02-17-44-20-sitdown_chair-m1.txt\n",
      "Accelerometer-2011-06-02-16-15-55-sitdown_chair-f1.txt\n",
      "Accelerometer-2011-12-11-08-13-16-sitdown_chair-f4.txt\n",
      "Accelerometer-2011-12-11-08-21-20-sitdown_chair-m2.txt\n",
      "Accelerometer-2012-05-25-18-27-57-sitdown_chair-f4.txt\n",
      "Accelerometer-2011-05-30-21-37-52-sitdown_chair-m2.txt\n",
      "Accelerometer-2011-06-02-16-46-43-sitdown_chair-f4.txt\n",
      "Accelerometer-2011-06-02-17-26-23-sitdown_chair-m1.txt\n",
      "Accelerometer-2011-06-02-17-04-59-sitdown_chair-f4.txt\n",
      "Accelerometer-2011-12-11-08-23-42-sitdown_chair-m2.txt\n",
      "Accelerometer-2012-05-28-17-41-12-sitdown_chair-m1.txt\n",
      "Accelerometer-2011-05-30-08-31-30-sitdown_chair-f1.txt\n",
      "Accelerometer-2011-06-02-17-27-18-sitdown_chair-m1.txt\n",
      "Accelerometer-2011-06-01-14-38-38-sitdown_chair-f1.txt\n",
      "Accelerometer-2011-12-11-08-22-36-sitdown_chair-m2.txt\n",
      "Accelerometer-2011-06-02-16-21-36-sitdown_chair-f1.txt\n",
      "Accelerometer-2011-06-06-09-35-11-sitdown_chair-f1.txt\n",
      "Accelerometer-2011-05-30-10-21-50-sitdown_chair-m1.txt\n",
      "Accelerometer-2011-05-30-08-22-28-sitdown_chair-f1.txt\n",
      "Accelerometer-2011-06-02-16-21-01-sitdown_chair-f1.txt\n",
      "Accelerometer-2011-04-12-21-42-24-sitdown_chair-m8.txt\n",
      "Accelerometer-2011-05-31-16-22-37-sitdown_chair-f1.txt\n",
      "Accelerometer-2011-06-06-09-34-12-sitdown_chair-f1.txt\n",
      "Accelerometer-2011-04-08-17-34-35-sitdown_chair-f3.txt\n",
      "Accelerometer-2011-05-30-20-55-28-sitdown_chair-f1.txt\n",
      "Accelerometer-2011-06-01-14-45-42-sitdown_chair-f1.txt\n",
      "Accelerometer-2011-06-01-14-43-55-sitdown_chair-f1.txt\n",
      "Accelerometer-2011-12-05-09-50-54-sitdown_chair-f1.txt\n",
      "Accelerometer-2011-12-11-08-12-14-sitdown_chair-f4.txt\n",
      "Accelerometer-2011-05-30-21-33-15-sitdown_chair-m2.txt\n",
      "Accelerometer-2011-03-24-09-50-16-sitdown_chair-f1.txt\n",
      "Accelerometer-2011-03-24-16-09-29-sitdown_chair-f2.txt\n",
      "Accelerometer-2011-05-30-20-48-57-sitdown_chair-f1.txt\n",
      "Accelerometer-2011-05-30-20-49-32-sitdown_chair-f1.txt\n",
      "Accelerometer-2012-03-26-04-58-33-sitdown_chair-f2.txt\n",
      "Accelerometer-2011-03-24-10-27-33-sitdown_chair-f1.txt\n",
      "Accelerometer-2011-04-11-11-45-28-sitdown_chair-f1.txt\n",
      "Accelerometer-2012-05-25-18-33-29-sitdown_chair-f4.txt\n",
      "Accelerometer-2011-06-01-14-37-35-sitdown_chair-f1.txt\n",
      "Accelerometer-2011-04-08-18-09-40-sitdown_chair-m4.txt\n",
      "Accelerometer-2011-06-02-16-54-34-sitdown_chair-f4.txt\n",
      "Accelerometer-2011-06-02-17-45-12-sitdown_chair-m1.txt\n",
      "Accelerometer-2011-12-05-09-39-34-sitdown_chair-f1.txt\n",
      "Accelerometer-2011-12-11-08-19-32-sitdown_chair-m2.txt\n",
      "Accelerometer-2011-05-30-21-48-01-sitdown_chair-m2.txt\n",
      "Accelerometer-2012-03-26-04-54-57-sitdown_chair-f2.txt\n",
      "Accelerometer-2011-04-05-18-22-14-sitdown_chair-f1.txt\n",
      "Accelerometer-2011-05-30-09-24-48-sitdown_chair-f1.txt\n",
      "Accelerometer-2011-06-01-14-50-18-sitdown_chair-f1.txt\n",
      "Accelerometer-2011-12-11-08-14-09-sitdown_chair-f4.txt\n",
      "Accelerometer-2011-12-05-09-53-32-sitdown_chair-f1.txt\n",
      "Accelerometer-2011-04-12-21-43-15-sitdown_chair-m8.txt\n",
      "Accelerometer-2011-12-11-08-15-21-sitdown_chair-f4.txt\n",
      "Accelerometer-2011-06-02-16-17-10-sitdown_chair-f1.txt\n",
      "Accelerometer-2012-03-23-03-48-40-sitdown_chair-m9.txt\n",
      "Accelerometer-2011-06-02-16-55-35-sitdown_chair-f4.txt\n",
      "Accelerometer-2011-06-01-14-46-43-sitdown_chair-f1.txt\n",
      "Accelerometer-2011-12-11-08-11-11-sitdown_chair-f4.txt\n",
      "Accelerometer-2011-06-02-16-44-37-sitdown_chair-f4.txt\n",
      "Accelerometer-2011-06-02-16-53-58-sitdown_chair-f4.txt\n",
      "Accelerometer-2012-05-25-18-36-06-sitdown_chair-f4.txt\n",
      "Accelerometer-2011-04-05-18-47-38-sitdown_chair-f1.txt\n",
      "Accelerometer-2011-05-31-14-59-27-sitdown_chair-f1.txt\n",
      "Accelerometer-2011-05-31-14-53-37-sitdown_chair-f1.txt\n",
      "Accelerometer-2011-06-02-17-05-36-sitdown_chair-f4.txt\n",
      "Accelerometer-2012-03-23-03-45-26-sitdown_chair-m9.txt\n",
      "Accelerometer-2011-06-06-09-35-43-sitdown_chair-f1.txt\n",
      "Accelerometer-2011-06-01-14-39-48-sitdown_chair-f1.txt\n",
      "Accelerometer-2012-03-26-05-04-48-sitdown_chair-m3.txt\n",
      "Accelerometer-2012-03-26-05-02-58-sitdown_chair-m3.txt\n",
      "Accelerometer-2011-05-30-10-31-12-sitdown_chair-m1.txt\n",
      "Accelerometer-2011-05-31-15-07-32-sitdown_chair-f1.txt\n",
      "Accelerometer-2011-06-02-17-32-07-sitdown_chair-m1.txt\n",
      "Accelerometer-2011-06-01-14-41-42-sitdown_chair-f1.txt\n",
      "Accelerometer-2011-12-05-09-44-08-sitdown_chair-f1.txt\n",
      "Accelerometer-2011-12-11-08-20-24-sitdown_chair-m2.txt\n",
      "Accelerometer-2011-05-31-16-27-00-sitdown_chair-f1.txt\n",
      "Accelerometer-2011-05-31-16-33-27-sitdown_chair-f1.txt\n",
      "Accelerometer-2012-05-28-17-46-13-sitdown_chair-m1.txt\n",
      "Accelerometer-2011-06-02-17-25-34-sitdown_chair-m1.txt\n",
      "Accelerometer-2011-06-01-14-51-28-sitdown_chair-f1.txt\n",
      "Accelerometer-2011-12-11-08-16-20-sitdown_chair-f4.txt\n",
      "Accelerometer-2011-06-02-17-41-43-sitdown_chair-m1.txt\n",
      "Accelerometer-2012-05-25-18-28-46-sitdown_chair-f4.txt\n",
      "Accelerometer-2011-03-29-09-06-48-sitdown_chair-f1.txt\n",
      "Accelerometer-2011-06-01-14-40-31-sitdown_chair-f1.txt\n",
      "Accelerometer-2011-04-08-17-33-10-sitdown_chair-f3.txt\n",
      "Accelerometer-2011-12-05-09-58-23-sitdown_chair-f1.txt\n",
      "Accelerometer-2011-03-29-09-12-27-sitdown_chair-f1.txt\n",
      "Accelerometer-2011-06-02-16-46-07-sitdown_chair-f4.txt\n",
      "Accelerometer-2011-03-24-13-17-42-eat_meat-f1.txt\n",
      "Accelerometer-2011-03-24-13-06-15-eat_meat-f1.txt\n",
      "Accelerometer-2011-03-24-13-21-39-eat_meat-f1.txt\n",
      "Accelerometer-2011-03-24-13-12-52-eat_meat-f1.txt\n",
      "Accelerometer-2011-03-24-13-10-14-eat_meat-f1.txt\n",
      "Accelerometer-2011-05-30-10-37-30-descend_stairs-m1.txt\n",
      "Accelerometer-2011-05-30-08-34-06-descend_stairs-f1.txt\n",
      "Accelerometer-2011-06-02-10-38-52-descend_stairs-f1.txt\n",
      "Accelerometer-2011-05-31-16-28-22-descend_stairs-f1.txt\n",
      "Accelerometer-2011-06-06-11-01-54-descend_stairs-f1.txt\n",
      "Accelerometer-2011-05-30-21-53-35-descend_stairs-m2.txt\n",
      "Accelerometer-2011-03-29-16-16-34-descend_stairs-f1.txt\n",
      "Accelerometer-2011-05-31-15-15-48-descend_stairs-f1.txt\n",
      "Accelerometer-2011-05-30-09-40-50-descend_stairs-f1.txt\n",
      "Accelerometer-2011-03-24-10-24-02-descend_stairs-f1.txt\n",
      "Accelerometer-2011-05-30-21-39-11-descend_stairs-m2.txt\n",
      "Accelerometer-2011-06-06-10-59-38-descend_stairs-f1.txt\n",
      "Accelerometer-2011-03-29-16-17-18-descend_stairs-f1.txt\n",
      "Accelerometer-2011-06-06-10-58-14-descend_stairs-f1.txt\n",
      "Accelerometer-2011-05-30-09-29-15-descend_stairs-f1.txt\n",
      "Accelerometer-2011-05-31-14-52-31-descend_stairs-f1.txt\n",
      "Accelerometer-2011-05-30-20-56-43-descend_stairs-f1.txt\n",
      "Accelerometer-2011-06-02-10-36-42-descend_stairs-f1.txt\n",
      "Accelerometer-2011-05-30-09-31-43-descend_stairs-f1.txt\n",
      "Accelerometer-2011-05-30-10-19-56-descend_stairs-m1.txt\n",
      "Accelerometer-2011-06-06-10-58-40-descend_stairs-f1.txt\n",
      "Accelerometer-2011-06-02-10-38-15-descend_stairs-f1.txt\n",
      "Accelerometer-2011-06-06-11-00-26-descend_stairs-f1.txt\n",
      "Accelerometer-2011-05-30-08-20-55-descend_stairs-f1.txt\n",
      "Accelerometer-2011-04-11-13-32-24-descend_stairs-f1.txt\n",
      "Accelerometer-2011-05-30-20-48-10-descend_stairs-f1.txt\n",
      "Accelerometer-2011-03-29-09-15-58-descend_stairs-f1.txt\n",
      "Accelerometer-2011-06-06-10-56-55-descend_stairs-f1.txt\n",
      "Accelerometer-2011-03-29-16-17-50-descend_stairs-f1.txt\n",
      "Accelerometer-2011-03-29-16-18-14-descend_stairs-f1.txt\n",
      "Accelerometer-2011-05-30-21-09-46-descend_stairs-f1.txt\n",
      "Accelerometer-2011-05-31-15-00-30-descend_stairs-f1.txt\n",
      "Accelerometer-2011-03-24-11-20-43-descend_stairs-f1.txt\n",
      "Accelerometer-2011-05-31-16-21-49-descend_stairs-f1.txt\n",
      "Accelerometer-2011-05-30-10-24-51-descend_stairs-m1.txt\n",
      "Accelerometer-2011-05-30-08-24-58-descend_stairs-f1.txt\n",
      "Accelerometer-2011-04-05-18-26-06-descend_stairs-f1.txt\n",
      "Accelerometer-2011-06-02-10-36-04-descend_stairs-f1.txt\n",
      "Accelerometer-2011-03-24-10-25-11-descend_stairs-f1.txt\n",
      "Accelerometer-2011-06-06-10-56-23-descend_stairs-f1.txt\n",
      "Accelerometer-2011-04-11-11-51-27-descend_stairs-f1.txt\n",
      "Accelerometer-2011-05-30-21-32-18-descend_stairs-m2.txt\n",
      "Accelerometer-2011-04-11-13-29-54-brush_teeth-f1.txt\n",
      "Accelerometer-2011-06-02-10-45-50-brush_teeth-f1.txt\n",
      "Accelerometer-2011-06-06-10-48-05-brush_teeth-f1.txt\n",
      "Accelerometer-2011-04-11-13-28-18-brush_teeth-f1.txt\n",
      "Accelerometer-2011-05-30-10-34-16-brush_teeth-m1.txt\n",
      "Accelerometer-2011-06-06-10-45-27-brush_teeth-f1.txt\n",
      "Accelerometer-2011-05-30-08-35-11-brush_teeth-f1.txt\n",
      "Accelerometer-2011-05-30-21-10-57-brush_teeth-f1.txt\n",
      "Accelerometer-2011-05-30-09-36-50-brush_teeth-f1.txt\n",
      "Accelerometer-2011-05-30-21-55-04-brush_teeth-m2.txt\n",
      "Accelerometer-2011-06-02-10-42-22-brush_teeth-f1.txt\n",
      "Accelerometer-2011-05-31-15-16-47-brush_teeth-f1.txt\n",
      "Accelerometer-2011-06-06-09-42-11-pour_water-f1.txt\n",
      "Accelerometer-2012-06-07-21-27-46-pour_water-f4.txt\n",
      "Accelerometer-2011-04-11-12-55-26-pour_water-f1.txt\n",
      "Accelerometer-2012-06-09-22-57-12-pour_water-m11.txt\n",
      "Accelerometer-2012-03-26-04-53-59-pour_water-f2.txt\n",
      "Accelerometer-2011-05-31-16-34-32-pour_water-f1.txt\n",
      "Accelerometer-2011-06-06-09-45-45-pour_water-f1.txt\n",
      "Accelerometer-2011-06-02-17-06-44-pour_water-f4.txt\n",
      "Accelerometer-2011-03-24-13-30-01-pour_water-f1.txt\n",
      "Accelerometer-2011-03-24-10-51-12-pour_water-f1.txt\n",
      "Accelerometer-2012-06-07-21-34-47-pour_water-f4.txt\n",
      "Accelerometer-2012-06-07-21-32-12-pour_water-f4.txt\n",
      "Accelerometer-2012-05-30-19-52-29-pour_water-f2.txt\n",
      "Accelerometer-2012-05-29-17-10-30-pour_water-m3.txt\n",
      "Accelerometer-2011-06-01-14-12-53-pour_water-f1.txt\n",
      "Accelerometer-2011-06-02-16-50-40-pour_water-f4.txt\n",
      "Accelerometer-2012-06-07-21-29-48-pour_water-f4.txt\n",
      "Accelerometer-2011-06-01-14-16-35-pour_water-f1.txt\n",
      "Accelerometer-2012-03-23-03-46-21-pour_water-m9.txt\n",
      "Accelerometer-2011-05-30-21-51-05-pour_water-m2.txt\n",
      "Accelerometer-2012-05-30-19-57-40-pour_water-m3.txt\n",
      "Accelerometer-2012-05-28-17-41-55-pour_water-m1.txt\n",
      "Accelerometer-2012-05-25-18-34-46-pour_water-f4.txt\n",
      "Accelerometer-2012-03-23-03-55-46-pour_water-m9.txt\n",
      "Accelerometer-2011-03-24-10-04-32-pour_water-f1.txt\n",
      "Accelerometer-2012-05-30-18-27-15-pour_water-f3.txt\n",
      "Accelerometer-2012-05-30-19-08-50-pour_water-m4.txt\n",
      "Accelerometer-2012-06-07-21-34-22-pour_water-f4.txt\n",
      "Accelerometer-2012-05-30-19-50-42-pour_water-f2.txt\n",
      "Accelerometer-2011-05-31-15-11-25-pour_water-f1.txt\n",
      "Accelerometer-2012-06-09-22-56-17-pour_water-m11.txt\n",
      "Accelerometer-2011-06-06-09-50-17-pour_water-f1.txt\n",
      "Accelerometer-2012-06-07-21-30-16-pour_water-f4.txt\n",
      "Accelerometer-2011-05-30-21-05-39-pour_water-f1.txt\n",
      "Accelerometer-2012-05-25-18-37-06-pour_water-f4.txt\n",
      "Accelerometer-2011-06-06-09-48-59-pour_water-f1.txt\n",
      "Accelerometer-2012-05-30-19-54-24-pour_water-f2.txt\n",
      "Accelerometer-2012-06-07-21-35-28-pour_water-f4.txt\n",
      "Accelerometer-2011-06-01-14-27-40-pour_water-f1.txt\n",
      "Accelerometer-2012-05-30-19-56-40-pour_water-m3.txt\n",
      "Accelerometer-2012-05-28-17-43-20-pour_water-m1.txt\n",
      "Accelerometer-2011-06-06-09-49-51-pour_water-f1.txt\n",
      "Accelerometer-2011-06-01-14-13-29-pour_water-f1.txt\n",
      "Accelerometer-2012-06-07-21-32-37-pour_water-f4.txt\n",
      "Accelerometer-2012-05-30-18-27-41-pour_water-f3.txt\n",
      "Accelerometer-2012-06-07-21-28-23-pour_water-f4.txt\n",
      "Accelerometer-2011-06-06-09-55-25-pour_water-f1.txt\n",
      "Accelerometer-2012-03-26-05-01-39-pour_water-m3.txt\n",
      "Accelerometer-2011-06-02-16-52-30-pour_water-f4.txt\n",
      "Accelerometer-2012-06-09-22-55-08-pour_water-m11.txt\n",
      "Accelerometer-2011-06-01-14-17-02-pour_water-f1.txt\n",
      "Accelerometer-2012-05-30-18-26-49-pour_water-f3.txt\n",
      "Accelerometer-2012-06-09-22-58-08-pour_water-m11.txt\n",
      "Accelerometer-2012-05-29-17-12-07-pour_water-m3.txt\n",
      "Accelerometer-2012-05-30-19-07-43-pour_water-m4.txt\n",
      "Accelerometer-2012-05-29-17-19-55-pour_water-m3.txt\n",
      "Accelerometer-2012-06-07-21-30-37-pour_water-f4.txt\n",
      "Accelerometer-2011-06-02-17-23-16-pour_water-m1.txt\n",
      "Accelerometer-2012-05-29-16-43-41-pour_water-f2.txt\n",
      "Accelerometer-2012-06-07-21-28-48-pour_water-f4.txt\n",
      "Accelerometer-2011-06-06-09-46-20-pour_water-f1.txt\n",
      "Accelerometer-2011-06-06-09-39-30-pour_water-f1.txt\n",
      "Accelerometer-2012-06-09-22-55-42-pour_water-m11.txt\n",
      "Accelerometer-2012-05-25-18-31-58-pour_water-f4.txt\n",
      "Accelerometer-2011-06-06-09-51-53-pour_water-f1.txt\n",
      "Accelerometer-2012-05-25-18-29-48-pour_water-f4.txt\n",
      "Accelerometer-2011-06-01-16-46-50-pour_water-f1.txt\n",
      "Accelerometer-2012-05-30-19-08-20-pour_water-m4.txt\n",
      "Accelerometer-2011-04-05-18-54-14-pour_water-f1.txt\n",
      "Accelerometer-2012-06-07-21-31-05-pour_water-f4.txt\n",
      "Accelerometer-2012-03-23-03-54-26-pour_water-m9.txt\n",
      "Accelerometer-2011-06-06-09-56-28-pour_water-f1.txt\n",
      "Accelerometer-2011-06-02-17-28-27-pour_water-m1.txt\n",
      "Accelerometer-2011-06-02-17-06-15-pour_water-f4.txt\n",
      "Accelerometer-2012-03-26-04-55-40-pour_water-f2.txt\n",
      "Accelerometer-2011-06-02-17-45-45-pour_water-m1.txt\n",
      "Accelerometer-2012-06-09-22-56-53-pour_water-m11.txt\n",
      "Accelerometer-2011-06-06-09-40-34-pour_water-f1.txt\n",
      "Accelerometer-2012-05-28-17-49-34-pour_water-m1.txt\n",
      "Accelerometer-2012-06-09-22-58-28-pour_water-m11.txt\n",
      "Accelerometer-2011-06-01-14-32-38-pour_water-f1.txt\n",
      "Accelerometer-2012-06-07-21-31-24-pour_water-f4.txt\n",
      "Accelerometer-2012-06-07-21-29-09-pour_water-f4.txt\n",
      "Accelerometer-2012-05-28-17-46-43-pour_water-m1.txt\n",
      "Accelerometer-2011-06-06-09-47-48-pour_water-f1.txt\n",
      "Accelerometer-2012-06-07-21-33-47-pour_water-f4.txt\n",
      "Accelerometer-2012-05-29-16-49-00-pour_water-f2.txt\n",
      "Accelerometer-2012-06-09-22-54-40-pour_water-m11.txt\n",
      "Accelerometer-2012-06-07-21-31-48-pour_water-f4.txt\n",
      "Accelerometer-2011-06-01-14-18-36-pour_water-f1.txt\n",
      "Accelerometer-2012-06-07-21-35-50-pour_water-f4.txt\n",
      "Accelerometer-2012-03-26-05-03-27-pour_water-m3.txt\n",
      "Accelerometer-2011-06-02-17-04-09-pour_water-f4.txt\n",
      "Accelerometer-2012-05-30-19-59-17-pour_water-m3.txt\n",
      "Accelerometer-2012-06-07-21-33-21-pour_water-f4.txt\n",
      "Accelerometer-2011-06-01-14-25-16-pour_water-f1.txt\n",
      "Accelerometer-2012-05-30-18-26-02-pour_water-f3.txt\n",
      "Accelerometer-2011-06-02-17-33-30-pour_water-m1.txt\n",
      "Accelerometer-2011-06-06-09-44-12-pour_water-f1.txt\n",
      "Accelerometer-2012-05-30-19-07-14-pour_water-m4.txt\n",
      "Accelerometer-2011-06-01-14-24-51-drink_glass-f1.txt\n",
      "Accelerometer-2012-05-29-16-44-12-drink_glass-f2.txt\n",
      "Accelerometer-2011-06-01-14-33-59-drink_glass-f1.txt\n",
      "Accelerometer-2012-05-29-17-20-28-drink_glass-m3.txt\n",
      "Accelerometer-2012-05-25-18-32-32-drink_glass-f4.txt\n",
      "Accelerometer-2011-04-08-18-10-09-drink_glass-m4.txt\n",
      "Accelerometer-2012-03-26-04-56-11-drink_glass-f2.txt\n",
      "Accelerometer-2012-05-30-19-51-49-drink_glass-f2.txt\n",
      "Accelerometer-2011-03-24-11-14-00-drink_glass-f1.txt\n",
      "Accelerometer-2012-05-30-21-48-13-drink_glass-m2.txt\n",
      "Accelerometer-2011-06-02-16-52-53-drink_glass-f4.txt\n",
      "Accelerometer-2011-06-01-15-25-43-drink_glass-f1.txt\n",
      "Accelerometer-2011-06-02-17-52-26-drink_glass-m1.txt\n",
      "Accelerometer-2011-06-02-17-30-51-drink_glass-m1.txt\n",
      "Accelerometer-2012-05-28-17-47-49-drink_glass-m1.txt\n",
      "Accelerometer-2011-06-02-17-32-31-drink_glass-m1.txt\n",
      "Accelerometer-2012-03-26-05-02-06-drink_glass-m3.txt\n",
      "Accelerometer-2012-05-28-17-47-23-drink_glass-m1.txt\n",
      "Accelerometer-2011-05-30-21-51-59-drink_glass-m2.txt\n",
      "Accelerometer-2011-06-01-14-20-46-drink_glass-f1.txt\n",
      "Accelerometer-2011-06-01-14-23-15-drink_glass-f1.txt\n",
      "Accelerometer-2012-05-30-21-48-45-drink_glass-m2.txt\n",
      "Accelerometer-2011-06-02-17-24-39-drink_glass-m1.txt\n",
      "Accelerometer-2011-06-02-17-29-50-drink_glass-m1.txt\n",
      "Accelerometer-2011-06-01-15-09-37-drink_glass-f1.txt\n",
      "Accelerometer-2011-06-01-14-28-50-drink_glass-f1.txt\n",
      "Accelerometer-2012-05-29-17-11-01-drink_glass-m3.txt\n",
      "Accelerometer-2012-05-28-17-44-02-drink_glass-m1.txt\n",
      "Accelerometer-2011-06-01-16-45-39-drink_glass-f1.txt\n",
      "Accelerometer-2011-05-31-16-35-41-drink_glass-f1.txt\n",
      "Accelerometer-2012-05-28-17-50-04-drink_glass-m1.txt\n",
      "Accelerometer-2011-06-02-17-40-40-drink_glass-m1.txt\n",
      "Accelerometer-2011-06-01-14-30-43-drink_glass-f1.txt\n",
      "Accelerometer-2012-03-26-04-53-07-drink_glass-f2.txt\n",
      "Accelerometer-2012-05-30-19-58-10-drink_glass-m3.txt\n",
      "Accelerometer-2012-03-23-03-47-36-drink_glass-m9.txt\n",
      "Accelerometer-2011-04-11-13-17-55-drink_glass-f1.txt\n",
      "Accelerometer-2011-05-30-21-51-35-drink_glass-m2.txt\n",
      "Accelerometer-2011-05-30-21-06-15-drink_glass-f1.txt\n",
      "Accelerometer-2011-05-31-15-12-36-drink_glass-f1.txt\n",
      "Accelerometer-2011-06-02-17-19-56-drink_glass-m1.txt\n",
      "Accelerometer-2012-05-30-19-57-08-drink_glass-m3.txt\n",
      "Accelerometer-2011-05-30-21-06-42-drink_glass-f1.txt\n",
      "Accelerometer-2011-06-01-14-15-49-drink_glass-f1.txt\n",
      "Accelerometer-2012-05-30-19-59-40-drink_glass-m3.txt\n",
      "Accelerometer-2011-05-31-15-12-05-drink_glass-f1.txt\n",
      "Accelerometer-2011-06-01-16-53-24-drink_glass-f1.txt\n",
      "Accelerometer-2011-06-02-17-29-13-drink_glass-m1.txt\n",
      "Accelerometer-2011-03-24-10-07-02-drink_glass-f1.txt\n",
      "Accelerometer-2012-05-25-18-33-57-drink_glass-f4.txt\n",
      "Accelerometer-2011-06-02-17-20-47-drink_glass-m1.txt\n",
      "Accelerometer-2012-05-30-22-10-16-drink_glass-m2.txt\n",
      "Accelerometer-2011-06-02-17-23-53-drink_glass-m1.txt\n",
      "Accelerometer-2011-06-01-14-14-22-drink_glass-f1.txt\n",
      "Accelerometer-2011-06-01-16-49-14-drink_glass-f1.txt\n",
      "Accelerometer-2012-05-25-18-36-27-drink_glass-f4.txt\n",
      "Accelerometer-2012-05-25-18-31-18-drink_glass-f4.txt\n",
      "Accelerometer-2011-05-31-15-13-09-drink_glass-f1.txt\n",
      "Accelerometer-2011-06-01-14-14-49-drink_glass-f1.txt\n",
      "Accelerometer-2011-04-11-12-55-39-drink_glass-f1.txt\n",
      "Accelerometer-2012-05-29-16-48-07-drink_glass-f2.txt\n",
      "Accelerometer-2011-06-01-16-47-29-drink_glass-f1.txt\n",
      "Accelerometer-2012-05-30-19-53-49-drink_glass-f2.txt\n",
      "Accelerometer-2012-03-26-05-03-53-drink_glass-m3.txt\n",
      "Accelerometer-2011-04-05-18-55-33-drink_glass-f1.txt\n",
      "Accelerometer-2012-05-25-18-30-30-drink_glass-f4.txt\n",
      "Accelerometer-2011-03-24-16-08-29-drink_glass-f2.txt\n",
      "Accelerometer-2011-04-12-21-44-55-drink_glass-m8.txt\n",
      "Accelerometer-2011-03-24-10-16-02-drink_glass-f1.txt\n",
      "Accelerometer-2011-03-24-13-17-06-drink_glass-f1.txt\n",
      "Accelerometer-2011-03-24-13-09-29-drink_glass-f1.txt\n",
      "Accelerometer-2012-05-30-19-58-44-drink_glass-m3.txt\n",
      "Accelerometer-2012-05-28-17-44-31-drink_glass-m1.txt\n",
      "Accelerometer-2011-06-01-15-14-42-drink_glass-f1.txt\n",
      "Accelerometer-2011-03-24-10-46-25-drink_glass-f1.txt\n",
      "Accelerometer-2011-06-01-15-04-40-drink_glass-f1.txt\n",
      "Accelerometer-2011-05-31-16-37-39-drink_glass-f1.txt\n",
      "Accelerometer-2011-03-24-16-08-10-drink_glass-f2.txt\n",
      "Accelerometer-2011-05-30-21-52-25-drink_glass-m2.txt\n",
      "Accelerometer-2011-06-01-16-57-10-drink_glass-f1.txt\n",
      "Accelerometer-2011-03-24-13-31-22-drink_glass-f1.txt\n",
      "Accelerometer-2012-05-30-19-53-19-drink_glass-f2.txt\n",
      "Accelerometer-2011-06-02-17-20-17-drink_glass-m1.txt\n",
      "Accelerometer-2011-04-08-17-35-00-drink_glass-f3.txt\n",
      "Accelerometer-2012-03-23-03-54-54-drink_glass-m9.txt\n",
      "Accelerometer-2011-06-02-16-51-58-drink_glass-f4.txt\n",
      "Accelerometer-2012-05-28-17-42-29-drink_glass-m1.txt\n",
      "Accelerometer-2011-05-31-16-35-12-drink_glass-f1.txt\n",
      "Accelerometer-2011-06-01-14-13-57-drink_glass-f1.txt\n",
      "Accelerometer-2012-05-29-16-42-17-drink_glass-f2.txt\n",
      "Accelerometer-2011-04-08-17-33-35-drink_glass-f3.txt\n",
      "Accelerometer-2012-05-30-22-10-40-drink_glass-m2.txt\n",
      "Accelerometer-2011-06-01-14-15-19-drink_glass-f1.txt\n",
      "Accelerometer-2012-05-30-19-54-48-drink_glass-f2.txt\n",
      "Accelerometer-2011-05-30-21-07-10-drink_glass-f1.txt\n",
      "Accelerometer-2011-06-01-14-21-52-drink_glass-f1.txt\n",
      "Accelerometer-2011-06-02-16-51-25-drink_glass-f4.txt\n",
      "Accelerometer-2012-03-23-03-47-02-drink_glass-m9.txt\n",
      "Accelerometer-2011-06-01-16-44-44-drink_glass-f1.txt\n",
      "Accelerometer-2011-04-12-21-43-48-drink_glass-m8.txt\n",
      "Accelerometer-2011-12-11-08-15-55-standup_chair-f4.txt\n",
      "Accelerometer-2011-06-02-16-44-18-standup_chair-f4.txt\n",
      "Accelerometer-2011-12-05-09-52-58-standup_chair-f1.txt\n",
      "Accelerometer-2011-12-11-08-19-06-standup_chair-m2.txt\n",
      "Accelerometer-2011-05-30-21-07-51-standup_chair-f1.txt\n",
      "Accelerometer-2011-05-30-20-50-30-standup_chair-f1.txt\n",
      "Accelerometer-2012-05-28-17-45-43-standup_chair-m1.txt\n",
      "Accelerometer-2011-06-02-17-41-11-standup_chair-m1.txt\n",
      "Accelerometer-2011-05-31-16-27-45-standup_chair-f1.txt\n",
      "Accelerometer-2011-06-01-14-40-19-standup_chair-f1.txt\n",
      "Accelerometer-2012-05-28-17-50-40-standup_chair-m1.txt\n",
      "Accelerometer-2011-05-30-08-24-19-standup_chair-f1.txt\n",
      "Accelerometer-2011-12-11-08-19-58-standup_chair-m2.txt\n",
      "Accelerometer-2012-03-23-03-48-20-standup_chair-m9.txt\n",
      "Accelerometer-2012-05-25-18-28-24-standup_chair-f4.txt\n",
      "Accelerometer-2011-04-08-18-09-06-standup_chair-m4.txt\n",
      "Accelerometer-2012-05-29-16-47-10-standup_chair-f2.txt\n",
      "Accelerometer-2011-12-11-08-20-56-standup_chair-m2.txt\n",
      "Accelerometer-2011-05-30-20-56-05-standup_chair-f1.txt\n",
      "Accelerometer-2011-06-02-17-26-02-standup_chair-m1.txt\n",
      "Accelerometer-2012-05-29-17-14-27-standup_chair-m3.txt\n",
      "Accelerometer-2011-03-24-16-09-19-standup_chair-f2.txt\n",
      "Accelerometer-2011-06-02-16-55-12-standup_chair-f4.txt\n",
      "Accelerometer-2011-12-05-09-48-17-standup_chair-f1.txt\n",
      "Accelerometer-2011-06-01-14-45-33-standup_chair-f1.txt\n",
      "Accelerometer-2011-06-01-14-41-32-standup_chair-f1.txt\n",
      "Accelerometer-2011-03-23-10-47-59-standup_chair-f1.txt\n",
      "Accelerometer-2011-05-30-08-32-03-standup_chair-f1.txt\n",
      "Accelerometer-2011-06-02-16-45-48-standup_chair-f4.txt\n",
      "Accelerometer-2011-12-05-09-55-36-standup_chair-f1.txt\n",
      "Accelerometer-2011-04-11-11-48-09-standup_chair-f1.txt\n",
      "Accelerometer-2011-12-11-08-14-55-standup_chair-f4.txt\n",
      "Accelerometer-2012-03-26-04-54-30-standup_chair-f2.txt\n",
      "Accelerometer-2011-12-11-08-22-05-standup_chair-m2.txt\n",
      "Accelerometer-2011-04-11-13-24-10-standup_chair-f1.txt\n",
      "Accelerometer-2012-03-26-05-02-40-standup_chair-m3.txt\n",
      "Accelerometer-2011-12-05-09-40-57-standup_chair-f1.txt\n",
      "Accelerometer-2011-06-02-16-53-39-standup_chair-f4.txt\n",
      "Accelerometer-2012-05-28-17-40-45-standup_chair-m1.txt\n",
      "Accelerometer-2011-12-11-08-23-15-standup_chair-m2.txt\n",
      "Accelerometer-2011-06-01-14-43-42-standup_chair-f1.txt\n",
      "Accelerometer-2011-06-02-16-44-59-standup_chair-f4.txt\n",
      "Accelerometer-2012-03-23-03-44-39-standup_chair-m9.txt\n",
      "Accelerometer-2011-12-05-09-38-00-standup_chair-f1.txt\n",
      "Accelerometer-2012-05-25-18-27-29-standup_chair-f4.txt\n",
      "Accelerometer-2011-04-05-18-25-12-standup_chair-f1.txt\n",
      "Accelerometer-2011-05-31-14-59-53-standup_chair-f1.txt\n",
      "Accelerometer-2011-05-31-16-38-19-standup_chair-f1.txt\n",
      "Accelerometer-2011-12-11-08-12-46-standup_chair-f4.txt\n",
      "Accelerometer-2011-06-02-16-54-15-standup_chair-f4.txt\n",
      "Accelerometer-2011-12-11-08-24-09-standup_chair-m2.txt\n",
      "Accelerometer-2011-05-31-16-23-03-standup_chair-f1.txt\n",
      "Accelerometer-2012-03-26-04-58-13-standup_chair-f2.txt\n",
      "Accelerometer-2011-12-11-08-13-41-standup_chair-f4.txt\n",
      "Accelerometer-2011-04-08-17-34-10-standup_chair-f3.txt\n",
      "Accelerometer-2011-03-23-10-42-01-standup_chair-f1.txt\n",
      "Accelerometer-2011-03-24-11-19-29-standup_chair-f1.txt\n",
      "Accelerometer-2011-06-02-16-46-25-standup_chair-f4.txt\n",
      "Accelerometer-2011-03-29-09-10-24-standup_chair-f1.txt\n",
      "Accelerometer-2011-05-30-10-31-41-standup_chair-m1.txt\n",
      "Accelerometer-2011-06-01-14-46-35-standup_chair-f1.txt\n",
      "Accelerometer-2011-03-29-09-04-40-standup_chair-f1.txt\n",
      "Accelerometer-2011-06-02-17-04-40-standup_chair-f4.txt\n",
      "Accelerometer-2011-06-02-17-31-39-standup_chair-m1.txt\n",
      "Accelerometer-2012-05-28-17-39-31-standup_chair-m1.txt\n",
      "Accelerometer-2011-06-01-14-39-38-standup_chair-f1.txt\n",
      "Accelerometer-2011-05-30-21-33-37-standup_chair-m2.txt\n",
      "Accelerometer-2011-12-11-08-09-59-standup_chair-f4.txt\n",
      "Accelerometer-2011-06-02-17-44-52-standup_chair-m1.txt\n",
      "Accelerometer-2012-05-29-16-43-05-standup_chair-f2.txt\n",
      "Accelerometer-2011-04-08-17-32-45-standup_chair-f3.txt\n",
      "Accelerometer-2011-05-30-21-38-19-standup_chair-m2.txt\n",
      "Accelerometer-2011-05-30-09-25-14-standup_chair-f1.txt\n",
      "Accelerometer-2011-06-01-14-51-21-standup_chair-f1.txt\n",
      "Accelerometer-2011-05-30-20-50-04-standup_chair-f1.txt\n",
      "Accelerometer-2012-05-29-17-12-51-standup_chair-m3.txt\n",
      "Accelerometer-2011-05-30-21-52-55-standup_chair-m2.txt\n",
      "Accelerometer-2012-03-26-05-04-33-standup_chair-m3.txt\n",
      "Accelerometer-2011-06-02-17-26-55-standup_chair-m1.txt\n",
      "Accelerometer-2011-06-01-14-38-03-standup_chair-f1.txt\n",
      "Accelerometer-2011-06-02-17-05-17-standup_chair-f4.txt\n",
      "Accelerometer-2011-06-02-17-42-24-standup_chair-m1.txt\n",
      "Accelerometer-2011-04-12-21-42-53-standup_chair-m8.txt\n",
      "Accelerometer-2011-12-05-09-45-47-standup_chair-f1.txt\n",
      "Accelerometer-2011-03-24-10-38-17-standup_chair-f1.txt\n",
      "Accelerometer-2011-04-12-21-45-21-standup_chair-m8.txt\n",
      "Accelerometer-2011-05-30-10-23-55-standup_chair-m1.txt\n",
      "Accelerometer-2011-03-24-09-49-36-standup_chair-f1.txt\n",
      "Accelerometer-2011-05-31-15-13-49-standup_chair-f1.txt\n",
      "Accelerometer-2011-03-23-10-45-51-standup_chair-f1.txt\n",
      "Accelerometer-2011-06-02-16-43-22-standup_chair-f4.txt\n",
      "Accelerometer-2011-06-01-14-50-10-standup_chair-f1.txt\n",
      "Accelerometer-2011-05-30-09-24-04-standup_chair-f1.txt\n",
      "Accelerometer-2011-12-11-08-11-36-standup_chair-f4.txt\n",
      "Accelerometer-2011-05-31-14-54-09-standup_chair-f1.txt\n",
      "Accelerometer-2012-05-25-18-33-08-standup_chair-f4.txt\n",
      "Accelerometer-2011-06-02-17-25-13-standup_chair-m1.txt\n",
      "Accelerometer-2012-05-25-18-35-48-standup_chair-f4.txt\n",
      "Accelerometer-2011-06-02-17-43-41-standup_chair-m1.txt\n",
      "Accelerometer-2011-03-23-10-39-18-standup_chair-f1.txt\n",
      "Accelerometer-2011-12-05-09-50-23-standup_chair-f1.txt\n",
      "Accelerometer-2011-06-01-14-52-02-standup_chair-f1.txt\n",
      "Accelerometer-2011-05-30-10-28-13-use_telephone-m1.txt\n",
      "Accelerometer-2011-03-29-09-30-56-use_telephone-f1.txt\n",
      "Accelerometer-2011-05-31-15-03-53-use_telephone-f1.txt\n",
      "Accelerometer-2011-05-30-08-27-44-use_telephone-f1.txt\n",
      "Accelerometer-2011-04-05-18-29-19-use_telephone-f1.txt\n",
      "Accelerometer-2011-05-30-21-43-16-use_telephone-m2.txt\n",
      "Accelerometer-2011-05-30-09-34-12-use_telephone-f1.txt\n",
      "Accelerometer-2011-04-11-11-54-10-use_telephone-f1.txt\n",
      "Accelerometer-2011-04-11-11-53-34-use_telephone-f1.txt\n",
      "Accelerometer-2011-05-31-16-31-11-use_telephone-f1.txt\n",
      "Accelerometer-2011-03-29-09-32-59-use_telephone-f1.txt\n",
      "Accelerometer-2011-05-30-21-00-40-use_telephone-f1.txt\n",
      "Accelerometer-2011-03-29-09-52-41-use_telephone-f1.txt\n"
     ]
    }
   ],
   "source": [
    "# get list of folders/files in Folder HMP_Dataset\n",
    "file_list=os.listdir('HMP_Dataset')\n",
    "# filter list for folder containing data\n",
    "file_list_filtered=(s for s in file_list if '_' in s)\n",
    "from pyspark.sql.functions import lit\n",
    "# create pandas dataframe for all the data\n",
    "df = None\n",
    "from pyspark.sql.functions import lit\n",
    "for category in file_list_filtered:\n",
    "    data_files=os.listdir('HMP_Dataset/'+(category))  \n",
    "    for data_file in data_files:\n",
    "        print (data_file)        \n",
    "        temp_df = spark.read.option (\"header\", \"false\").option(\"delimiter\",\" \").csv('HMP_Dataset/'+(category)+'/'+(data_file),schema=schema)\n",
    "        temp_df = temp_df.withColumn('class',lit(category))\n",
    "        temp_df = temp_df.withColumn ('source', lit (data_file))\n",
    "        if df is None:\n",
    "            df = temp_df\n",
    "        else:\n",
    "            df=df.union(temp_df)               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+------------+--------------------+\n",
      "|  x|  y|  z|       class|              source|\n",
      "+---+---+---+------------+--------------------+\n",
      "| 37| 34| 43|Climb_stairs|Accelerometer-201...|\n",
      "| 37| 34| 43|Climb_stairs|Accelerometer-201...|\n",
      "| 35| 39| 46|Climb_stairs|Accelerometer-201...|\n",
      "| 33| 39| 48|Climb_stairs|Accelerometer-201...|\n",
      "| 31| 36| 44|Climb_stairs|Accelerometer-201...|\n",
      "| 29| 38| 47|Climb_stairs|Accelerometer-201...|\n",
      "| 28| 42| 50|Climb_stairs|Accelerometer-201...|\n",
      "| 25| 39| 46|Climb_stairs|Accelerometer-201...|\n",
      "| 20| 47| 56|Climb_stairs|Accelerometer-201...|\n",
      "| 19| 45| 54|Climb_stairs|Accelerometer-201...|\n",
      "| 19| 38| 48|Climb_stairs|Accelerometer-201...|\n",
      "| 19| 35| 46|Climb_stairs|Accelerometer-201...|\n",
      "| 20| 38| 44|Climb_stairs|Accelerometer-201...|\n",
      "| 18| 48| 51|Climb_stairs|Accelerometer-201...|\n",
      "| 18| 41| 45|Climb_stairs|Accelerometer-201...|\n",
      "| 18| 41| 42|Climb_stairs|Accelerometer-201...|\n",
      "| 20| 39| 40|Climb_stairs|Accelerometer-201...|\n",
      "| 19| 37| 42|Climb_stairs|Accelerometer-201...|\n",
      "| 17| 39| 38|Climb_stairs|Accelerometer-201...|\n",
      "| 14| 40| 39|Climb_stairs|Accelerometer-201...|\n",
      "+---+---+---+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = df.randomSplit([0.8,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = splits[0]\n",
    "df_test = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder  \n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler \n",
    "from pyspark.ml.feature import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer (inputCol='class', outputCol='label')\n",
    "#vectorAssembler= VectorAssembler (inputCols=['x','y','x'], outputCol='features')\n",
    "#normalizer = Normalizer (inputCol='features', outputCol='features_norm',p=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder (inputCol='label', outputCol='labelVec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorAssembler= VectorAssembler (inputCols=['x','y','x'], outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And we specify p=1, that's a parameter we can tune later.\n",
    "normalizer = Normalizer (inputCol='features', outputCol='features_norm',p=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### o now we import the linear support vector machine classifier, but now the problem is we are on Spark 2.1, and it's only supported on Spark 2.2. SO luckily, since we are an open source based cloud, let's just use open sourced Spark 2.3 on my laptop, and we just copy paste the whole notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name LinearSVC",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-ff9ee49c9247>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearSVC\u001b[0m \u001b[0;31m# Error generated as cannot import name LinearSVC, as its supported on saprk 2.2 instead of 2.1 spark.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# Roemo used on laptop for further progress as it is open source\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name LinearSVC"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LinearSVC # Error generated as cannot import name LinearSVC, as its supported on saprk 2.2 instead of 2.1 spark.\n",
    "# Roemo used on laptop for further progress as it is open source\n",
    "lsvc=LinearSVC(maxIter=1.0, regParam=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at an example that opens a string. So now we are creating the data frame from the CSV files residing on my laptop.\n",
    "### So let's inspect the data frame, Again, the Spark drop is triggered here, but that Spark drop now is not running on a cluster. It's only on my laptop, but it looks nice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'createorReplaceTempView'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-3e3c217217f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateorReplaceTempView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'df'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_two_classes\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"select * from df where class in ('Use_telephone', Standup_chair)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/src/spark21master/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    971\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m             raise AttributeError(\n\u001b[0;32m--> 973\u001b[0;31m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[1;32m    974\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'createorReplaceTempView'"
     ]
    }
   ],
   "source": [
    "df.createorReplaceTempView('df')\n",
    "df_two_class= spark.sql(\"select * from df where class in ('Use_telephone', Standup_chair)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits=df_two_class.randomSplit([0.8, 0.2])\n",
    "df_train= splits[0]\n",
    "df_train=splits[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So at the moment, the linear support vector machine classifier only supports binary classification, so let's actually create a new data set containing only two classes. And we are creating a pipeline containing the pre-processing steps and the support vector machine classifier stage. So indexer,coder,vectorAssembler, Normalizer, and the linear supported machine classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[indexer, encoder, vectorAssembler, normalizer, lsvc])\n",
    "model=pipeline.fit (df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.transform (df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And it's consistent if we evaluate other models, so that's the cool advantage of pipelining.\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And we need to specify the raw prediction column, Because the raw prediction contains the score, and the reprediction only contains zero or one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator  = BinaryClassificationEvaluator(rawPredictionCol='rawPrediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And we should see a score which hopefully is better than the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.evaluate (prediction) # answer is 0.93889..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Awesome, 93%. So let's see if we are not over-fitting. I trust doing the same with the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = pipeline.fit (df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.transform (df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.evaluate (prediction) # Answer 0.9384..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real nice, we are not over-fitting, so we get the similar score for both the test and the training set. And I would consider this to be done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crossvalidation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now let's imagine you have training data and validation data and your model is doing a good job on both. But then, your model actually never has seen your validation data and that means your model has lost the opportunity to learn something from your validation data.\n",
    "### So, let's try another approach. We create multiple sets of training and validation data. We do this by creating folds. So, the first fold contains 80 percent training data and 20 percent validation data. The second fold again contains 80 percent training data and 20 percent validation data back from different regions of your data. Again, the third fold will contain 80 percent training data, 20 percent validation data, but from another different region of your data\n",
    "### Once you have created those three different folds which each have different regions for training data and validation data of your original dataset, you can create three individual models on your training set.\n",
    "### Once you're happy, you can just combine those three models, in case of classification, you can do it by voting, in case of regression, you just take the average.\n",
    "### So in summary, cross-validation. Actually takes all your data into account to train the model and therefore you will get the maximum out of your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuing using GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You are training the model and you are not performing well, you tune it to perform better. in this case you use GridSearch\n",
    "### Machine Learning has many parameters to tune\n",
    "### Summary: Hyper-paramter is essential if your model doesn't perform well on the training set , \n",
    "### you can use grid search to search over the multidimensional hyper-parameter space \n",
    "### Multidimensional: if you are nesting for loops ,with every hyper-parameter you are tuning, you are increasing exponentially in commutational complexity (thats the only catch)\n",
    "### ApacheSPark is the solution (deals linearly with the amount of commute nodes and in the cloud, adding compute nodes tp apache spark cluster is just a matter of couple of mouse clicks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-96-00a69dcaf6e5>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-96-00a69dcaf6e5>\"\u001b[0;36m, line \u001b[0;32m8\u001b[0m\n\u001b[0;31m    evaluate model\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Grid Search Technique\n",
    "for p1 in paramters_set1:\n",
    "    for p2 in paramters_set2:\n",
    "        for p3 in paramters_set3:\n",
    "            for p4 in paramters_set4:\n",
    "                for p5 in paramters_set5:\n",
    "                    model=train (data,p1,p2,p3,p4,p5)\n",
    "                    evaluate model\n",
    "# The parameters is Hyper-paramter, which your mdoel take even before training starts\n",
    "# The parameters of a model you are learning from training using gradient descent \n",
    "# but hyper-parameters are those you specify during model instanciation\n",
    "# Test set after (training & validation set) \n",
    "# if you want tune your parameters of a model, you have to need validation set in order to prevent overfitting, \n",
    "# you can discover this problem when you use unseen validation data\n",
    "# if we tune hyper-paramters of a model, we can also overfit aganist validation and test data\n",
    "# u take some data apart, which is not a part of hyper-paramter tuning process and after everything is done  \n",
    "# we use Test set for  a double check if hyper-paramters prevent overfitting  or not aganist training and validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificaiton Evaluation Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision and Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In pattern recognition, information retrieval and binary classification, precision (also called positive predictive value) is the fraction of relevant instances among the retrieved instances, while recall (also known as sensitivity) is the fraction of relevant instances that have been retrieved over the total amount of relevant instances. Both precision and recall are therefore based on an understanding and measure of relevance.\n",
    "### Suppose a computer program for recognizing dogs in photographs identifies 8 dogs in a picture containing 12 dogs and some cats. Of the 8 identified as dogs, 5 actually are dogs (true positives), while the rest are cats (false positives). The program's precision is 5/8 while its recall is 5/12. When a search engine returns 30 pages only 20 of which were relevant while failing to return 40 additional relevant pages, its precision is 20/30 = 2/3 while its recall is 20/60 = 1/3. So, in this case, precision is \"how useful the search results are\", and recall is \"how complete the results are\".\n",
    "### In statistics, if the null hypothesis is that all items are irrelevant (where the hypothesis is accepted or rejected based on the number selected compared with the sample size), absence of type I and type II errors corresponds respectively to maximum precision (no false positive) and maximum recall (no false negative). The above pattern recognition example contained 8 − 5 = 3 type I errors and 12 − 5 = 7 type II errors. Precision can be seen as a measure of exactness or quality, whereas recall is a measure of completeness or quantity.\n",
    "### In simple terms, high precision means that an algorithm returned substantially more relevant results than irrelevant ones, while high recall means that an algorithm returned most of the relevant results.\n",
    "### In the field of information retrieval, precision is the fraction of retrieved documents that are relevant to the query:\n",
    "### In information retrieval, recall is the fraction of the relevant documents that are successfully retrieved.\n",
    "### For classification tasks, the terms true positives, true negatives, false positives, and false negatives (see Type I and type II errors for definitions) compare the results of the classifier under test with trusted external judgments. The terms positive and negative refer to the classifier's prediction (sometimes known as the expectation), and the terms true and false refer to whether that prediction corresponds to the external judgment (sometimes known as the observation).\n",
    "\n",
    "### Let us define an experiment from P positive instances and N negative instances for some condition. The four outcomes can be formulated in a 2×2 contingency table or confusion matrix, as follows:\n",
    "### Precision = tp/tp+fp, Recall = tp/tp+fn\n",
    "### Recall in this context is also referred to as the true positive rate or sensitivity, and precision is also referred to as positive predictive value (PPV); other related measures used in classification include true negative rate and accuracy.[6] True negative rate is also called specificity.\n",
    "### True Negative Rate = tn/tn+fp, Accuracy = tp+tn/tp+tn+fn+fp\n",
    "### Accuracy does not perform well with imbalanced data sets. For example, if you have 95 negative and 5 positive samples, classifying all as negative gives 0.95 accuracy score. Balanced Accuracy [7] (bACC) overcomes this problem, by normalizing true positive and true negative predictions by the number of positive and negative samples, respectively, and divides their sum into two. This is equivalent to the following formula:\n",
    "### Balanced Accuracy = TPR +TNR/2\n",
    "### Regarding the previous example (95 negative and 5 positive samples), classifying all as negative gives 0.5 balanced accuracy score out of the maximum bACC one, which is equivalent to the expected value of a random guess of a balanced data. Balanced Accuracy is suggested to use to measure how accurate is the overall performance of a model is, considering both positive and negative classes without worrying about the imbalance of a data set. Since most of the real data sets are imbalanced, Balanced Accuracy metric is suggested instead of Accuracy metric.\n",
    "### Additionally, the predicted positive condition rate (PPCR) identifies the percentage of the total population that is flagged; for example, for a search engine returning 30 results (retrieved documents) out of 1,000,000 documents, the PPCR is 0.003%.\n",
    "### Predicted positive condition rate=tp+fp/tp+fp+tn+fn\n",
    "### It is possible to interpret precision and recall not as ratios but as probabilities: Precision is the probability that a (randomly selected) retrieved document is relevant. Recall is the probability that a (randomly selected) relevant document is retrieved in a search.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  In statistical analysis of binary classification, the F1 score (also F-score or F-measure) is a measure of a test's accuracy. It considers both the precision p and the recall r of the test to compute the score: p is the number of correct positive results divided by the number of all positive results returned by the classifier, and r is the number of correct positive results divided by the number of all relevant samples (all samples that should have been identified as positive). The F1 score is the harmonic average of the precision and recall, where an F1 score reaches its best value at 1 (perfect precision and recall) and worst at 0.\n",
    "### Diagnostic testing: This is related to the field of binary classification where recall is often termed as Sensitivity. There are several reasons that the F1 score can be criticized in particular circumstances.\n",
    "### Applications: The F-score is often used in the field of information retrieval for measuring search, document classification, and query classification performance.[4] Earlier works focused primarily on the F1 score, but with the proliferation of large scale search engines, performance goals changed to place more emphasis on either precision or recall[5] and so {\\displaystyle F_{\\beta }} F_{\\beta } is seen in wide application. The F-score is also used in machine learning.[6] Note, however, that the F-measures do not take the true negatives into account, and that measures such as the Matthews correlation coefficient, Informedness or Cohen's kappa may be preferable to assess the performance of a binary classifier.[3] The F-score has been widely used in the natural language processing literature, such as the evaluation of named entity recognition and word segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the purpose of a test set in contrast of a train and validation set?\n",
    "### A test set is used to assess over-fitting hyper-parameters\n",
    "## When adding pipeline or model hyper-parameters to the search grid - what is the relation between number of tune-able hyper-parameters and the growth in computational complexity?\n",
    "### exponential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision trees are weak models. They're very expandable but they don't perform very well.\n",
    "### o, consider this dataset, it's a dataset which helps you to decide whether to go for tennis training or not.\n",
    "### For example, if the weather outlook is sunny, the temperature is hot and the humidity is high, and it is not windy, you don't play. On the other hand, if the outlook is overcast, it is hot, you have high humidity and it's not windy, you'll play.\n",
    "### So first of all you check the outlook. If it's overcast, you definitely never go. 1\n",
    "### But if it's sunny and humid, then you don't go., If it's sunny and normal, you go.2\n",
    "### If it's rainy and windy, you go. And if it's rainy and not windy, you don't go 3\n",
    "### So actually to build such a tree to check value, split first. So you're splitting the data based on a certain criteria, in this case on the outlook column. And then you basically have three different distinct values, sunny, overcast and rain. And downstream you of course only take example to consideration which are matching your branch criteria. \n",
    "### So decision trees are good for huge datasets. And they are good in ignoring redundant features, which is pretty cool. So you can just throw your data at a decision tree and it will basically ignore irrelevant data. Another cool thing is small trees you can easily interpret. You can basically look into the decision tree and it explains you a lot about your data.\n",
    "### On the other hand, really large trees are hard to interpret and also they have a poor prediction performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap Aggregation (Bagging) & Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### what bootstrapping or bootstrap aggregation or begging that's all the same is. I will use RandomForest as an example. \n",
    "### o bootstrapping is the following, let's consider this is you're creating data set. So what you do is, you are sampling or resampling and you just take one sample B_1, then you take another sample from your training data B_2, B_3, B_4 and so on. You're now using this data for RandomForest. \n",
    "### So a RandomForest, trains a decision tree on each of the bootstrap samples. So therefore we end up for each bootstrap sample with one decision tree, f_b and basically we take the average of the composite decisions of all the trees to obtain our real prediction. \n",
    "###  So f_b is the decision of one single tree. We sum all those decisions up and we'll divide it by the number of trees. \n",
    "### So clear advantage of RandomForest over decision trees is their improved model performance, and their assistance to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting & Gradient Boosted Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting is also known as Stagewise Additive Modeling. So, the idea is in contrast to random forest that all the decision trees are created parallel.\n",
    "### In boosting, only one model after the other is trained based on outputs from previous models. Again, this is non of weak models\n",
    "### So lets consider hi(x) to be a weak model. So i is an index of the model. So then you can define a strong learner capital F(x) as the sum of all the weak models weighted by gamma i. This is another notation which is maybe a bit more intuitive. So if you want to define the model capital Fm(x), it is equal to capital Fm-(1)x plus gamma m times hm(x). Where Fm- 1(x) is the previous model and gamma m times hm(x) is the weighted current model. So gamma m is a learned weight, and don't be scared. It looks a bit complicated, but it's relatively easy to understand how we derive gamma m\n",
    "### So we are taking the arg min. If you never have seen that notation, arg min means that we are optimizing, or minimizing in this case, a function with respect to gamma in this case.And then we have to understand what L means. So L basically is the sum of squared errors. \n",
    "### So that's a measure how good or how bad we are doing. We call this residuals. \n",
    "###  So we take the correct value of yn and we subtract the protected value, we square it and we add it up for all our training examples and we normalize it. \n",
    "### So let's consider our training data set. So we start with a weak learner, h0, and we just take the mean of y as a prediction. Of course, this is not a very good model. Therefore, we have error terms. So error 1 = y- F0(X). The output is basically a new training data set. Those are called residuals. It's basically the error of the previous model.\n",
    "### So now we define a new model F1(X) based on F0(X) plus a new weak learner h1, which we train under residuals. And we continue this step. So we compute the error of the newly obtained model, epsilon 2 = y- F1(X). And again, that we are using to train a weak learner, h2, and we just continue until we are convergent.\n",
    "### So gradient boosted trees are outperforming RandomForest, but they are computationally more expensive because of their sequential nature. Remember, in RandomForest, you can in theory train all the decision trees on the samples which you have obtained by bootstrapping.\n",
    "### But here each iteration is dependent of the residuals of the previous iteration. Therefore, it's hard to parallelize. But there's hope. There is a specific implementation which is called XGBoost, which is very fast. And it has a slightly different implementation, which makes it even more resistant to overfitting. \n",
    "### o in summary, boosting is different from bootstrapping because we are taking the error into account which we are doing during building up the model.\n",
    "### Especially we will create decision trees which are specifically dedicated to training data, where our previous model is doing a bad job. And to weight how strongly this weak learner per iteration, this edit to the model is learned with an additional weight, gamma-m."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosted Trees with Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So let's read the parquet file from object store. And then we can basically just copy and paste all the previous stages because that's not changing. And again we need to create a binary classification data set, because GBT at the moment only supports binary classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'instancemethod' object has no attribute '__getitem__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-ba9de09b705e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# The SparkSession object is already initialized for you.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# The following variable contains the path to your file on your IBM Cloud Object Storage.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hmp.parquet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'courseraml-donotdelete-pr-qve0ttzezdeodc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'instancemethod' object has no attribute '__getitem__'"
     ]
    }
   ],
   "source": [
    "import ibmos2spark\n",
    "\n",
    "# @hidden_cell\n",
    "credentials = {\n",
    "    'endpoint': 'https://s3-api.us-geo.objectstorage.service.networklayer.com',\n",
    "    'api_key': 'JgVr3TfkAXS1NLjS9FZfKSwmZJo67UYeoTiYEkab9p8t',\n",
    "    'service_id': 'iam-ServiceId-8d66ef68-3f6b-47ad-83db-53846e304673',\n",
    "    'iam_service_endpoint': 'https://iam.bluemix.net/oidc/token'}\n",
    "\n",
    "configuration_name = 'os_d677618706764068bebe9e144db03560_configs'\n",
    "cos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "# Please read the documentation of PySpark to learn more about the possibilities to load data files.\n",
    "# PySpark documentation: https://spark.apache.org/docs/2.0.1/api/python/pyspark.sql.html#pyspark.sql.SparkSession\n",
    "# The SparkSession object is already initialized for you.\n",
    "# The following variable contains the path to your file on your IBM Cloud Object Storage.\n",
    "df = spark.read.parquet[cos.url('hmp.parquet', 'courseraml-donotdelete-pr-qve0ttzezdeodc')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createorReplaceTempView('df')\n",
    "df_two_class= spark.sql(\"select * from df where class in ('Use_telephone', Standup_chair)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits=df_two_class.randomSplit([0.8, 0.2])\n",
    "df_train= splits[0]\n",
    "df_train=splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder  \n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler \n",
    "from pyspark.ml.feature import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer (inputCol='class', outputCol='label')\n",
    "vectorAssembler= VectorAssembler (inputCols=['x','y','x'], outputCol='features')\n",
    "normalizer = Normalizer (inputCol='features', outputCol='features_norm',p=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt=GBTClassifier(labelCol='label', featureCol='features', maxIter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline=Pipeline (stages=[indexer,vectorAssembler, normalizer, gbt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=pipeline.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction=model.transform (df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binEval  = MulticlassClassificationEvaluator().setMetricName ('accuracy').setPredictionCol ('prediction').setLabelCol ('label')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binEval.evaluate (prediction) # Ans 0.9091 or 91%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit (df_test)\n",
    "prediction=model.transform(df_test)\n",
    "binEval.evaluate (prediction)  # Ans = 0.9144  (Accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That means we're not overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameter Tuning using GridSearch and Crossvalidation in Apache Spark ML on Gradient Boosted Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So since above accuracy is not very good, especially worst than a conductor machine, can now try to tune the parameters of the gradient.\n",
    "### So we import from the tuning package CrossValidator and ParamGridBuilder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator,ParamGridBuilder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And we can specify parameters from each stage so in this case, from normalizer we try out three variables for p, 1, 2 and 10.\n",
    "### Then yet another dimension, you say addGrid, and from the gbt model.\n",
    "### You specify MaxBins We just use exponential numbers here, 2, 4, 8, and 16.\n",
    "### Notice that we are basically growing exponentially in complexity for every grid dimension we add.\n",
    "### So we add another parameter to search over gbt.maxDepth. And we also specify ranges 2, 4, 8 and 16.\n",
    "### So that's just a toy example, it will run for ages. And if you add more dimensions, it will run even longer, so therefore it's good that we can just add notes to this back cluster, with a single mouse click. So across with the data using Pipeline.\n",
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid=ParamGridBuilder() \\\n",
    "           .addgrid(normalizer.p, [1.0,2.0,10.0]) \\\n",
    "           .addgrid (gbt.maxBins, [2,4,8,16]) \\\n",
    "           .addgrid (gbt.maxDepth,[2,4,8,16]) \\\n",
    "           .build()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And then we have to specify the ParamGrid, which we have just defined and we have to specify which evaluator this cross validator has to use. So we specify the MultiClassClassificationEvaluator.\n",
    "### nd then we specify the folds for cross validation, So usually a range between 4 and 10 is a good choice\n",
    "### It's not called pipeline, it's called estimator because the pipeline is an estimator so you can pass any type\n",
    "### cross validation now is splitting the training data into folds. So that means testing and training is done in the validator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossval=CrossValidator(estimator=Pipeline,estimatorParamMaps=paramGrid,evaluator=MulticlassClassificationEvaluator(),numfolds=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel=crossval.fit(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ### But we have taken some data apart, stf_test. And that's basically a necessary in order to assess the overfitting of the hyper parameters against the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction=cvModel.transfrom(df_test) # Ans= 0.9154"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And we went up from 91.1% to 91.5%, and I'm pretty sure if you add additional dimensions to the parameter grid you can even improve the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enselble Learning Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How are Random Forest different in re-sampling from Gradient Boosted Trees?\n",
    "### Sampling is done using Bootstrapping in RandomForests wheres Gradient Boosted Trees use Boosting\n",
    "## Which model is mostly prune to overfitting?\n",
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So consider your training data and a machine learning algorithm which perfectly fits your training data.Of course, you have the risk of overfitting.\n",
    "###  So I've told you how to quantify overfitting by using a test and a training data set. But what can you do if you're actually overfitting?\n",
    "### f you're overfitting, means that your model is basically too good, so it can explain the data too well. So one thing, of course, you can do is you use a more simple model\n",
    "### For example, linear or logistic regression, in contrast to the machines, KNN or gradient boosting trees.\n",
    "### But there's another thing you can do. So what if I will tell you that there's actually a parameter in the model that you can tune in order to specify how good it should fit the training data. So you can actually tune this knob until you get also good performance under unseen data.\n",
    "### So let's have a look how this works. So this is the square of errors function, and you basically sum up the error for all your training examples by taking the real value and subtracting the predicted value. So this formula here is nothing else than linear regression. And you basically just take Xij, so i stands for the index of the training example. And j stands for the actual dimension or feature.And then you have beta j, which is the parameter you are learning.\n",
    "### So let's consider this overfitting example. So definitely this model is far too specific. So therefore we can add a regularization term as follows. So we take each parameter beta, and we take the absolute value, and we take the sum.\n",
    "### So this basically contributes to the overall sum. So remember, you're taking the first derivative of this in order to find the minimum based on the parameter's beta. That means the more we add to this overall value, the worse we are actually performing. And that's something we actually want, since we want to prevent overfitting.\n",
    "### We can specify the parameter lambda in order to specify how strong this effect should be. So lambda of 0 is ignoring this effect. So this is called 1 or LASSO regularization. . And LASSO stands for least absolute shrinkage and selection operator.\n",
    "### Same contrast to L1, the axis starts at 2 regularization, which is called rich. And you can basically remember it, because in L2 regularization, we don't take the absolute value of the beta parameters, we take the square. So the result of this is that we penalize large parameters in a model even more.\n",
    "### So it's all about penalizing large parameters. And if a model wants to learn large parameters, then we penalize. And therefore we will stick the model to overfit.\n",
    "### So in summary, L1 can be used to even remove features from the model. But it's computationally more expensive than L2. L2 you can't use for feature selection, but it's computationally more efficient. And again, it penalizes large values of parameters beta more. And therefore it's some sort of a stronger regularization. I'm personally only using L2 rigorization in my work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which regularization technique is penalizing large model parameters most?\n",
    "### L2 Regularization\n",
    "## When is it appropriate to use Regularization\n",
    "### To prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 with Spark 2.1",
   "language": "python",
   "name": "python2-spark21"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
