{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Machine Learning & Signalling Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fourier Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signal Decomposition, time, & frequency domains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the Fourier transform decomposes a signal into the frequencies that make it up.\n",
    "### o imagine that you have a signal, let's take an example, we have a signal s1, right? And on the y axis here, you have the amplitude of the signal, on the x axis you have the time. \n",
    "### So if you want to measure the strength of the signal, let's say at some specific time, you take a point here. And you look at the distance between the 0 on the y axis and that's your strength, that's the amplitude of the signal. If you look the same for another signal s2, let me pick another color. We select the same moment in time and we measure this distance. So in this case, this distance is 2 whatever it means. I haven't defined the scale of my axis is just the amplitude of the signal. And then for the 1 on the top, the distance is 1.\n",
    "### Now what happens if you emit these two signals of the same time, So what happens if you get a signal s1 + s2 is for the same point in time, let me pick another color, at the same point in time. Your distance here will be the sum of these two distances. So because these two signals are being added together. So you have one from s1, which is here. And then you will have two from s2, which is here. So you end up with a total distance of three. So when you add signals together, you are just kind of summing these of waves.\n",
    "### Fourier transform does. It takes a complex signal and it decomposes it to the frequencies that made it up, so these two signals, right?\n",
    "### the piano is essentially a signal generator. Each key on the piano generates a unique sound wave, because when the hammer hits the strings based on the thickness of the strings and also the tension. This inpart generates air pressure which is essentially a sound wave. And each key on the piano has its unique frequency. So the range of the piano goes from about I think 27 hertz here in the low end, and it goes all the way up to about 4200 hertz here in the high end. So what I like to do now, is I would like to pick a core which consist of different notes. \n",
    "### So we have different frequencies mix together and see if we can actually recover the region of frequencies using Fourier transform.\n",
    "###  if I pick a chord maybe this one, [SOUND] so this chord is comprised of three individual notes. We have three different original signals. Let's say this is your signal 1, s1. This is s2, and this is s3. And these signals here on the chart don't reflect the frequencies of the piano notes. This is just for illustration, but the idea is that you have three different signals. Each one generated by a piano key, and when we merge them together you get something like this. That's the signal of s1 + s2 + s3 and on the, sorry, s2 + s3, and on the y axis here you have amplitude, on the x axis you have the time, right? And the duration of my clip are something like 4 seconds, so we have 4 seconds here.\n",
    "### What Fourier transform does, is it kind of moves us from the time domain to the frequency domain. So we will get a plot like this one where we have on the x axis different frequencies, right? And we said that the piano varies between something like 20 hertz and let's say I'm interested to look up to 4200 hertz.\n",
    "### And then on the y axis, we will get the strength of the signal. So in this case, you can see we have three distinct signals. This is the strength, the amplitude of the first one. That's the one, With the lowest frequency, so that's why it comes first in the chart. That's s2, this signal is the in the middle between these two, in terms of frequency. So it come second on the chart because the frequency increases in this direction. And then s3 comes in the height of the bars, shows the amplitude of the signal. \n",
    "### Fourier transform does, it helps us transition between the time and frequency domain. And actually, there is also the inverse Fourier transform which I'm not going to cover in this session, but just to let you know, it exist. So if you want to transition from the frequency to the time domain to something like this, what you need is IFT inverse Fourier transform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourier Transform in Action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And the purposes of the first script is to plot the way file so that you see the combined signal. And then the second script will actually apply Fourier transform and decompose this signal to its original frequencies. So let me first run this script, which will plot our signal.\n",
    "### As you saw, the duration of the signal is about 4 seconds, so 4,000 milliseconds. You can see this on the x axis here, and then on the y axis, you have the amplitude of the signal, and the signal you can see slowly fades away because it gradually gets quieter. You can hear this in the recording.\n",
    "### Now, if I take the same file and I apply Fourier transform, my expectation is to get this plot, which we call the spectrogram, this is essentially the energy distribution of the signal over the frequency. So my expectation is, let me find the script, to get the spectrogram and see the individual frequencies of all the signals that make up this cord. \n",
    "### So if we now focus on the plot, you see here that we have different signals. It's not perfect, we won't see three perfect signals, three perfect bars for each individual key because this is an actual piano. And there are harmonics and there is also noise in the recording and so on and so on. But what I'm interested in is to focus on the top three signals in terms of strength.\n",
    "### So if I look at this plot, maybe I focus on all the signals over 750 or 800, whatever it means. \n",
    "###  will just trim this plot and look only at the signals above 800. So here I will put this restriction in my script, and because it's a bit difficult to accelerate the plot, I will also print the frequencies. Now, by the way, you can notice that all my signals are somewhere here between, I would say 20 Hertz and not above 4000 Hertz anyway. So this is the range of the piano, this is what I expect to see, right. So let's run the script with the filter and see what we end up with, right.\n",
    "### o now we have three signals, you can see them here. These are the most powerful signals in my recording. There are actually four, but the first two are super close together. The first one is something between 329, 330 hertz, so maybe 329.5, something like this, that are just kind of overlapping. The second one is 415 hertz, and the third one is 555 hertz.\n",
    "### When we compare with piano strings, frequency or specific word is matched for these 3 frequencies.\n",
    "### So in other words, mission accomplished. We used Fourier transform against a complex signal, and we were able to decompose it and verify that it actually works perfectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one of the codes, complex signal or comprised signal\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.io import wavfile # check the syntax\n",
    "\n",
    "fs, snd=wavfile.read(\"output.wav\")\n",
    "\n",
    "snd=snd/(2.**15)\n",
    "s1=snd[:,0]\n",
    "\n",
    "plt.figure(figsize(20,8))\n",
    "plt.style.use (\"Seaborn\")\n",
    "\n",
    "time=np.arange(0, s1.shape[0], 1)\n",
    "time=(time/fs) * 1000\n",
    "\n",
    "plt.plot(time,s1, color='b')\n",
    "\n",
    "plt.ylabel(\"Amplitude\", fontsize=16)\n",
    "plt.xlabel(\"Time(ms)\", fontsize=16)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signal Generation & Phase Shift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So if I go back here, right, in my case with the piano, I had three individual signals, having their own frequencies and amplitudes. And I merge them together, and then I decompose them using Fourier transform.\n",
    "### the signals were very simple because they all had the same frequency. The only difference was in the amplitudes. And also you will notice that all the signals here start at the same time. So, if you look at the second signal, s2, it starts zero zero. So there is one wave here, goes all the way down and back to the zero on the y axis. And this is the same case with s1. It starts at zero zero, and then you have one wave, goes back to zero. And the same with this guy here. The combined signal of course.\n",
    "### Now this is not always the case. First of all, signals can have different frequencies. So if I look at this more complex example, you see that the frequency of this signal here is not the same as the frequency of that signal. For the same period of time, if I said this is s2 and this is s1, for the same period of time in s1 I get, I don't know, n waves. Here in S2 I get something like 2 times n, right? I have more waves for the same period of time, because the second signal has higher frequency. That's what frequency is, the number of occurrences per unit of time.\n",
    "### if you look here, where the signal start, s2 starts at 0.0 and then you have the first wave. But S1 doesn’t start at 0,0. If you continue this until it reaches the zero on the y axis you will see it starts at minus something, right? So at zero, I already have the maximum amplitude for the signal. And this is what we call phase shift. The signal is slightly shifted, it doesn't start at 0.0. So this is called phase shift, I'll write it down here, phase shift.\n",
    "### o if you want to describe a signal, you need three things. First, you need the frequency of the signal, which shows you how many occurances in the period you have. Second, you need the amplitude. You need to know how high the signal goes, what is the strength of the signal. And three, you need the phase shift, you need to know where the signal starts.\n",
    "### ou can get y as function of t, and this will be A times sine of 2pi f t + phi, right.\n",
    "### his one, which is a very simple function called gen_wave that accepts the frequency, the amplitude, the period, and the time shift. And there is another parameter, sampling rate. I will talk about sampling rate later. So what this function does is it just generates the values for x the time period and then it computes he signal by multiplying the amplitude, times, you can see it here, amplitude times the sine of two pi times the frequency, the time, and the phase shift.\n",
    "### And we can use this function to generate all kinds of signals if you want. And it's interesting to see, if I go back here, how we can use this function to generate complex signals. So if this is my signal S1, S2, and S3, I can actually use the same function to get S1 + S2 + S3. So let's do this.\n",
    "### So these are the two signals summed up together. Signal one is frequency of 1 over 10 seconds. Signal two is frequency of 2 over 10 seconds. And I can do something even more interesting, and yeah, as you can see also, the amplitude of the second signal is 2. So that's why we have this like almost two waves overlapping, because the amplitude of signal one is 1, the amplitude of signal two is 2. And I can add a third one here that has a frequency of 3. And then the amplitude is, I don't know, maybe set it to 4, see what happens. And we have to add it, right. So this is how we can generate complex signals. And of course if you now apply Fourier transform to this signal, you will, hopefully, get the original three signals that are being added together to construct this more complex signal, all right,\n",
    "### We had this complex signal, we kind of passed the signal through a Fourier transform and then the output was the spectrogram that allows us to recover the original frequencies. And we can then actually reconstruct the original signals if we want.\n",
    "### \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The maths behid Fourier Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Any continuous signal in the time domain can be represented by an infinite series of sinuasoids (French Mathematician)\n",
    "### How to implement FT, How to deal with complex numbers i, how to deal with discrete signals\n",
    "### Sampling a signal, higher the rate, better the quality. sampling means data points, more data points, more flexible image you get\n",
    "### higher the sampling rate, more datapoints we get per second , densed resolution\n",
    "### less ampling rate means less datapoints and you will get a straight, all the points lie in on direction, less densed means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete Fourier Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we know how to sample signal, it's time to look at modification of the algorithm known as discrete Fourier transform.\n",
    "### So first the definition, well, it says that any sampled signal of length N can be represented uniquely by a finite series of sinusoids.\n",
    "### What is the difference? Well, in the standard Fourier transform, we used function of time x of t to generate a continuous signal. Now with the discrete case, we don't have this function, we have a data set.\n",
    "### A set of points which we get by sampling the continuous signal. So I will use lowercase x to denote this data set, I will enclose it here. And I will say that it contains, well, the readings from my sampling, x0 x1, and so on until let's say xN-1. So the length of my data set x = N. And what discrete Fourier transform will do for me is it will transform this data set of lowercase x into another data set of lets say upper case X, which contains the Fourier coefficients, right? So I will have things like X0, X1, And so on until XN-1. And each X here,\n",
    "### Now one question that you may ask is how come that these two data set have the same length, N-1? And that's an excellent question. If you think about it, what drives the length of this data set is the sampling grade, because over a period of time, the number of data points that I read is exactly the sampling grade, right? And then if you think about the other data set, we said that the frequency is the number of occurrences per unit of time. So if I am sampling with certain frequency, I can not recognize signals that have larger frequency than the sampling frequency, just because I don't get enough data points.\n",
    "### What will happen is, like the example I showed you, when I have too few data points, I lose the signal. It kind of turns into flat line. Because the resolution is low, the representation of the sampled signals doesn't actually follow the actual signals. So if I have signals with very high frequencies on a very low sampling grade, I won't be able to recognize these signals at all.\n",
    "### So the number of frequencies that I can recognize by applying the Fourirer transform is actually driven by the sampling rate as well.\n",
    "### I have a script that generates a signal. In this case I'm doing a frequency of 3 Hz, amplitude of 2, period of 1, sampling grade is 50, and then I also apply Fourier transform. And again, I'm treating it as a black box because that's discreet Fourier transform. We haven't talked about how to actually apply it. But let me run the script now and see what we get. So this is my signal, amplitude of two, and I have three waves over the time period. And here on my spectrogram, I have one bin with a height of two because it reflects the amplitude of the signal. And it's at position three, which is the frequency of the signal, right? Nothing new here. This is what we expect.\n",
    "### if I add more signals into the mix. So maybe I will generate second signal with a frequency of 5. Let's say same amplitude, same period, and I'll add this to the original signal. So now I have my signal with frequency of 3, amplitude of 2 added to signal with frequency of 5 and amplitude of 2. And I will plot this and apply Fourier transform.\n",
    "### So let's see what happens. Okay, so here you see that we have a more complex signal this time, but the transformation is correct here and it perfectly recognizes two distinct signals. One with frequency of three, the other one five, both having amplitude of two. If I add another one, let's say frequency of I don't know, frequency of maybe six, yeah. And let's leave the amplitude to one. See what happens. You have to add it to the signal, right? Again, we can now detect three signals. This one have the strength of the other two because its amplitude is one, right? So this is what you expect. \n",
    "### What do you think will happen if I change the frequency, let's say of my second signal from 6 to let's say 6.5. What do you think will happen?... Because we don't hear of a bin with the frequency of 6.5, this signal is being decomposed to some further signals. And when you add them together, you get this signal of frequency 6.5. Okay, this was a tricky question. But my point is about something grade and the number of bins, the resolution of the spectrogram. So if I remove these additional signals, if I go to back to my original signal that’s only one simple sinusoid, amplitude of two, frequency of three. Here you can see that I have a number of bins in my spectrogram. This is driven by my sampling grade because this tells me how many different frequencies I can recognize My sampling grade is 50 here.\n",
    "### I don't see 50 bins because there is something else called the Nyquist limit, and it has to do with the spectrogram being actually symmetrical. So, okay, I wasn't planning to do this in this session, but maybe I'll just show you the full spectrogram. So, right. So this is the full spectrogram from the Fourier transform, all right? And there is a specific point somewhere in the middle of the spectrogram called the Nyquist limit or Nyquist point. And the spectrogram is symmetrical around this point. It's almost like a mirror\n",
    "### So the left hand side of the spectrogram contains all the information that we need, so normally we just don't plot this right-hand side. But the point is I have a sampling grade of 50, and I actually have 50 bins here in the full spectrogram. However, it's symmetrical and all the information I need Is contained in the first 25, so I just kind of don't plot the others\n",
    "### I will get a number of bins with different amplitudes that I have to sum up to actually approximate the original signal. And probably the original signal won't actually represent the actual signal. So the sampled version won't actually represent the original version because of the lower resolution... less bins, less sampling rate\n",
    "### Euler formula used to negotiate with complex number i, as it is not recgnised , Because it's not supported in NLLib, it's not supported in SystemML, so we have to find a way around it.\n",
    "### o if I compute these two sums, I don't actually have to deal with the complex numbers. And this is how I can get this pair of coefficients for XK, right? And this what this grade Fourier transform looks like. Now we need to think about implementing it. And because I said that it doesn't come out of the box in MA lib, what I will do is how we used system ML, and how we write an implementation from scratch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourier Transform in SystemML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A discrete Fourier transform in SystemML. I have a file here, Which I'm saving to my home directory, called FT.dml, which will be a SystemML script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code may not be run on this platform\n",
    "\n",
    "# Gen_wave is the function that accepts frequency and amplitude. It accepts a period, and also something great, which I will name Hz.\n",
    "\n",
    "# Okay, I won't bother with phase shift, I just want to keep everything simple. And this function returns a matrix of doubles, called x.\n",
    "\n",
    "# Okay, and then for x, I will take the amplitude, multiply it by sine, we remember the formula. Sine of 2pi. We don't actually have the pi constant in SystemML,\n",
    "#so I will just pass it as an argument here.\n",
    "\n",
    "gen_wave = function(double freq, double amp, integer T, integer Hz, double pi_value) return (matrix[double] x) {\n",
    "    time = seq(0, T - T/Hz, T/Hz)\n",
    "    x = amp*sin(2*pi_value*freq*time)\n",
    "}\n",
    "\n",
    "PI=3.141592654\n",
    "\n",
    "# So let's say frequency of 2, amplitude of 1, a period of 150 Hertz and plus the pi value\n",
    "# I will use the standalone version of SystemML.\n",
    "x=gen_wave(2,1,1,50, PI)\n",
    "N=nrow(x)\n",
    "\n",
    "# Okay, the script works, I don't see any output so maybe I'll just also print my matrix x. \n",
    "#It's a matrix, so I have to convert it to string, this will be x, I will use two tops as separators.\n",
    "print (toString(x, sep=\"\\t\\t\", decimal=1))\n",
    "print (N)\n",
    "\n",
    "#So I have 50 entries in x because my sampling rate is 50 over the same period. And then if you look at that the datapoints,\n",
    "# you can actually see the wave. It starts 0, 0, I shift to 0, and then it goes all the way up to 1 and then it goes down to -1 \n",
    "#and then it goes back to 0. Right, and I have two of those because my frequencies too\n",
    "\n",
    "# And there are two ways of doing this. Probably the most simple one is if I look here, these k and n variables.\n",
    "#I could actually create a loop over k, and then a loop over n, then compute Ak and Bk for each x of f, right.\n",
    "\n",
    "# If I get these indicator variables and create matrix, which I then multiply by two pi over n, I'm pretty much done.\n",
    "#So what I can do is I can define a sequence, let's say n is a sequence that goes from 0 to N- 1, and K is also sequence\n",
    "# that goes from 0 to N- 1, and these are vectors, so if I take the product of N times k transpose, \n",
    "# this will give me a matrix of size N- 1 by N- 1. And this matrix, if I multiply by 2 pi over n, right, \n",
    "#this will give me exactly this thing here. So then if I name this this matrix M, let's say.\n",
    "# All I have to do is for my, Xa coefficient,And Xb coefficients, I can simply compute them as cosine of M, \n",
    "#the product of the cosine of M and x. Right, and for Xb, the sine of M and x, which is my dataset, right?\n",
    "#  will define my two sequences from 0 to N- 1 step 1 and K is a sequence from 0 to N-1 step 1.\n",
    "# And then my matrix M would be N times k transpose times 2 pi over n. Oops, right. And then Xa will be the cosine of M times x,\n",
    "# and xp will be the product of sine M and,X. And I can merge these two together. I will create a matrix called DFT, and\n",
    "# this will be Xa and Xb bonded together. And I can print this, just to see what the result is.\n",
    "\n",
    "n= seq(0,N-1, 1)\n",
    "k=seq(0, N-1, 1)\n",
    "\n",
    "M(n %*% t(k)) * (2*PI/N)\n",
    "\n",
    "Xa = cos(M) %*% x\n",
    "Xb = sin(M) %*% x\n",
    "\n",
    "DFT = cbind(Xa, Xb)\n",
    "\n",
    "# Okay, so this is my spectrogram in a way. I have a sampling rate of 50, so I get 50 bars.\n",
    "#The frequency of the signal is 2, so that's 0, 1, 2, that's the second bar.\n",
    "# And that's where I have my coefficients, and because the spectrogram is symmetrical,\n",
    "#I also get this 25 there with the negative sign. Now, You might be wondering, why is this 25 here not equal to the amplitude, \n",
    "# which is 1? And the reason is, because these are the Fourier coefficients, they don't reflect,\n",
    "#they are not the same as the amplitude or the phase shift of the signal. So these are the coefficients that we use here,\n",
    "\n",
    "\n",
    "#print (toString(DFT, sep=\"\\t\\t\", decimal=1))\n",
    "# For claculating Magnitude as the ouput is showing max sample on amplitude position like 25 instead of 1or3 according to amplitude defined.\n",
    "MAG = sqrt(DFT[,1]^2, DFT[,2]^2)\n",
    "MAG=MAG[1:cell(N/2),]*2/N\n",
    "print (toString(MAG, sep=\"\\t\\t\", decimal=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast Fourier Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikipedia\n",
    "### irect Fourier transform implementation is very simple, very basic and also slow. It's not fit for purpose if you really want to do something in production environment. It was just shown to you so that you can get a good understanding of what the Fourier Transform is and how we can use it to kind of decompose signals. \n",
    "### f you are really interested in having a fast implementation of DFT, there is something called fast Fourier transform, which is a modification of the DFT algorithm, and it is really fast compared to DFT\n",
    "### Now DFT, the competition of complexity of DFT is quadratic time. So it's O n2, it's quadratic time. FFT for comparison is quasi-linear time. So this will be O of nlogn and FFT does this by exploiting asymmetry in the Fourier transformation.\n",
    "### many different ways of actually using Fourier transform in signal processing and analytics. Probably the most simple way is to use it for feature extraction, \n",
    "### Because here you can see a bunch of signals for example. And what you can do is you can apply the Fourier transform. This time the box is open because you know so much about it already. And you can generate your spectrograms, you can generate your magnitudes. You can get your phase shift and so on, and so on. \n",
    "### And because these guys here, they have the same dimensions, you can treat them as a feature vector, right? You can get any signal, arbitrary long, you don't care about this, use a Fourier transform, get the distribution of energy of the frequencies, and then use this as a feature vector. \n",
    "### And then maybe, I don't know, apply something like k-means, see how close these signals are to each other and maybe classify them. Or maybe you can feed these two in your network and digital network distinguish between different types of music. Maybe a classical music when a piano is playing compared to pop music or whatever.\n",
    "### So this is probably the most obvious use case for Fourier transformations. Virtually, you can take any signal. It doesn't have to be an audio signal, it can be a sensory signal, it can be anything. You can decompose it to signals, get this almost like a fingerprint of the frequencies and their magnitudes.\n",
    "### And then feed this to a typical machine learning classifier and see what we can do with it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz Fourier Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Fourier transform is an invertible transformation between the time and frequency domain representations of a signal.\n",
    "### True\n",
    "## The figure above shows two signals A and B, that have the same frequency and phase shift, but different amplitudes. What would the sum (A+B) of these two signals look like?\n",
    "### the amplitude is 1.5\n",
    "## The plot above shows a signal in the\n",
    "### Time DOmain\n",
    "## The reduction of a continuous time signal to a discrete time signal is known as\n",
    "### Sampling\n",
    "## You have the following continuous signal, but when sampled its plot looks like this:What is the most likely explanation for this effect?\n",
    "### The sampling rate is too low\n",
    "## You have the following frequency domain plot of a signal that's been generated by adding two separate signals (A and B) together.\n",
    "### The frequencies of A and B are 3.0 and 5.0 Hz., The amplitudes of A and B are 2.0 and 3.0\n",
    "## A known limitation of FT/DFT is that it requires an infinite series of sinusoids to represent a signal, so it cannot be used efficiently in a discrete setting.\n",
    "### False\n",
    "## Discrete Fourier Transform is slower compared to Fast Fourier Transform\n",
    "### Correct. The computational complexity of DFT is O(n^2) in contrast to O(nlog(n)) for FFT.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wavelet Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonstationary Signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourier transform allows us to transition between the time and the frequency domains. And if you remember, if we have a signal composed of multiple frequencies and we look at the signal in the time domain, we see a picture like this one that shows us the strength of the signal as a function of time. And if we apply Fourier transform, we end up in the frequency domain where we can isolate the individual frequencies in the signal.\n",
    "### The catch is that Fourier transform works real well, but it works really well for signals generated by stationary processes.\n",
    "### his is a signal generated by a stationary process, which means that this signal, simply put, never changes, right? If I look at the signal now, it looks like this. If I go do something else, come back to the signal in one hour, take another look, so in another moment in time the signal looks exactly the same. If I look at the signal in one day or one week or after one year, it looks exactly the same, the signal never changes. Another way to put this is to say that the signal contains all its frequencies always.\n",
    "###  The problem is that in real life, that's not always the case. You might end up with a signal that looks like this. And if you look at the signal, say at this moment in time, right? You see something like this. If you look at the signal at this moment in time, you see something completely different. And then again, something more similar to the first observation. And then something again completely different. And the problem is that Fourier transform doesn't work very well in this type of situations. \n",
    "### And quite often these bursts in the signal like this guy here and this guy here are exactly what we are looking for. This could be some kind of a sensor reading, this could be some kind of an anomaly. We want to be able to detect those things and to handle them in a more intelligent way.\n",
    "### f we have a very simple signal generated by a stationary process. You see here I have frequency of 1 amplitude of 3 over a period of 10. The phase shift is zero, and that's a very simple, very basic signal, this guy here. And then I apply Fourier transform, and that's what I see. Exactly what I expect to see here over this period. I have ten repetitions of the signal, so you know frequency of ten. And I can apply the inverse Fourier transform and reconstruct the signal.\n",
    "### Now imagine that to this signal, I add a second component, right? I add another signal with a frequency of four, amplitude of 20, right? But I'm not simply adding this in parallel to the first signal. I'm adding this component at a specific time at T2. So then what happens is, my signal will look like this. At time T2 I have this burst in the signal, right? And if I try to apply Fourier transform I get something like this.\n",
    "### Okay, I still have my main frequency in the signal. I can identify it, it's the dominant frequency. But then this short burst kind of factorizes into all these small frequencies here. Which if I add them back together, if I applied the inverse Fourier transform, I will get the same thing. I will reconstruct the signal. \n",
    "### ut if I look at this image down here in the second part of the plot, if I look here only, I have no idea what happens in the signal. What frequencies, I see the frequencies in the signal but I can't identify that this was actually a short burst in the signal. If I look at the top of the plot, if I look here, right? I can identify the burst, it's here, but I have no idea at what specific point, what frequencies are part of the signal in this specific moment in time. So, I am kind of confined either to the time domain or to the frequency domain, but I never get the complete picture. That's the problem with signal-generated by non-stationary process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaleograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I talk about the mathematics of continuous wavelength transform, I would like to give you an example. I would like to actually use the transform on this specific signal, on this guy here. And show you the result so that you know what we are after. In a way, I would like to give you the intuition before we actually look at the internals.\n",
    "### So, if I applied continuous wavelength transform to this signal, I will get a bunch of coefficients. And the best way to give you the intuition is to plot this coefficients on something called a scalogram. I need to plot that looks like this, and I'll try to explain it to you. It's kind of a heat map, right? But you will notice first that here we have a time component.\n",
    "### If you remember in the Fourier transform, we only get frequency and signal strength. Here, we start with time. And if you go back to this signal, you will notice that it starts at 0, goes up to 10, and it's pretty much the same on the scalogram. It starts at 0 goes up to 1000 because of the sampling rate of 100. It shows this plot covers the entire signal. The coefficients of the transform cover the entire signal.\n",
    "### Then up here we have the scale, and the scale is more very intuitive. And I'd like to think about the scale as inverse of the frequency. If it helps, you can think about the scale as a scale on a map, right? So if I use these screenshots from Google Maps, for example, this is a scale of 20 meters, right? And it shows you, for example, this is the IBM office\n",
    "### Now, if I zoom out a bit, and I set my scale to 500 meters, so my scale is bigger. I don't see the fine details anymore. he scale on the scaleogram shows you different details in different scales. And the lower the scale is, the higher frequencies you see on the plotted this region. So we have the time, we have the scale and the color represents the strength, the magnitude of the signal. So it goes up or down\n",
    "### nd what can this plot tell us? Well, I see here that, in the low frequencies area, I have a signal that starts around 0. And then it goes up because of the color here. Then it comes back to 0, then it goes down, becomes negative. Then up again, down again, up again, down again and so on. And this kind of continues for the whole duration of the signal. And this information is actually this component in the signal. This guy, right? That goes up and down and it continues from the beginning of the signal to the end of the signal.\n",
    "### nd what can this plot tell us? Well, I see here that, in the low frequencies area, I have a signal that starts around 0. And then it goes up because of the color here. Then it comes back to 0, then it goes down, becomes negative. Then up again, down again, up again, down again and so on. And this kind of continues for the whole duration of the signal. And this information is actually this component in the signal. This guy, right? That goes up and down and it continues from the beginning of the signal to the end of the signal.\n",
    "### nd what can this plot tell us? Well, I see here that, in the low frequencies area, I have a signal that starts around 0. And then it goes up because of the color here. Then it comes back to 0, then it goes down, becomes negative. Then up again, down again, up again, down again and so on. And this kind of continues for the whole duration of the signal. And this information is actually this component in the signal. This guy, right? That goes up and down and it continues from the beginning of the signal to the end of the signal.\n",
    "### here is a catch, the catch is that the continuous where the transform gives us different resolutions and different scales. But I'm not going to go into these details. The idea is that we can have all the information in one place and this is very, very useful. And I want to actually show you another variant of this plot. I'm using the same coefficients but I will do this in 3D and you will see that it's even more interesting to kind of look at the signal from this perspective.\n",
    "### Okay, so this is our signal in a 3D plot. Again, we have the Amplitude. We have the Scale. And we have the Time. And you can look at the plot now. And you can see that in the higher scale region, we have this continuous wave that goes through time. And it's always here. And then in the lower scales where we have high frequencies, we have this sharp berth, here. If we rotate this way, you will see them better, right. Okay, so these are our bursts and here we have the discomponent that goes through the entire signal that's always there. \n",
    "### Or you can take a look from this perspective and that's the time, right? And you see something very similar to actually the time domain, right? You have this constant components on the background and then you have the sharp burst at the foreground. And these are the same coefficients that I used for the scalogram. So you can plot the coefficients in very interesting and exciting ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COntinuous Wavelet Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's say Continuous Wavelet Transform is a function of two variables, tau and tau. Okay let me write it tau and s, and tau stands for translation, and s, as you have already probably guessed, is the scale. Right. The formula looks like this, it's one over the square root of the absolute value of s. Don't think about this, this is just used for energy normalization so that we have the same energies across all the scales.\n",
    "### There is an integral because it's a continuous transform. It goes from minus infinity to plus infinity of xt and xt is your signal as a function of time times Psi t minus Tau over s dt. So, this is your signal as a function of time, and psi here, this guy, is what we call a mother wavelet.\n",
    "### it's function of time, let me write it here, is a function of time. It is continuous in both the time and frequency domains and it generates a wavelet. A wavelet is like a small wave. \n",
    "### This function determines the main form of the wavelet that you use in your transformation and this function will shape the form of the wavelet. And then based on the coefficients that we pass to the function here, we will get different kind of shapes of the wavelet if this makes sense. Based on the scale, we can kind of shrink or expand the form of the wave. These are called like child wavelets.\n",
    "### So, that's why we say mother wavelet because that's the main one. You can select from very various different wavelengths. Various different small waves like for example, the Haar-wavelet, or the Gaussian wavelet, and this is what they look like. This function psi will give you something like this. And based on the coefficients that you pass to the function, you can control the shape of this wavelet. \n",
    "### o, using the scale parameter, you can squeeze this into a short interval or you can stretch this wave over a longer period of time. That's the idea. So, using this formula, what we do is we go over the signal and we use a combination of different wavelengths to actually compute the coefficients. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling & Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So this is my signal generated by a non-stationary process, and let's say that you can see here in the upper right corner. I have selected a mother wavelet which is a Gaussian wavelet, and what I do is I start with this wavelet with a translation of zero and certain scale, maybe one. And this wavelet kind of projects here on my signal, right? And I can treat this small wave as a sliding window, right. I can just start moving this wavelet over my signal. And at each step I moved the wavelet over the signal, I multiply it by the signal, right?\n",
    "### Then, when I finish with this wavelet, when I kind of continue this process until I reach the end of the signal, what I will do is I can stretch the wavelet a bit, increase the scale a bit, and then go back to zero and repeat the process. So same wavelet but this time, stretch a bit, and then I start translating it again.\n",
    "### I keep repeating the process multiplying with each step, and here maybe you can see, that now this part of my signal kind of overlaps with the shape of my Gaussian wavelet. And this is where I will get a nonzero coefficient, right. And I keep repeating the process, I keep repeating the process until I reach the end of the signal, and then what I do, I stretched the wavelet a bit more. I go back to the beginning and I start moving again, over the signal. And at certain point, my wavelet will be stretched sufficiently so that it captures, for example, this persistent component, the low frequency component in my signal. \n",
    "### this is how we actually generate the wavelet coefficients, right. We just pick a wavelet for different signals, we can use different shape for the mother wavelet. And then we just apply iteratively this process of starting a translation at point 0, tau 0. And then, we start with a very low scale. We go over the signal. We multiply at each step. These are our coefficients for this specific scale band, if you wish. And then we can stretch the wavelet a bit, we increase the scale, we repeat the process again, and again, and again, and again, until we exhaust all the possible scales. And this is it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Wavelets and Maching Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You now have a good understanding of what the wavelet transform is and how we use it to analyze signals and you have a good understanding of how we can compute the coefficients using continuous wavelet transform and this guide here\n",
    "### continuous wavelet transform is predominantly used for time-frequency analysis. There is also another way of computing wavelet transform coefficients, that's the discrete wavelet transform here and it does the same thing, but it does it in a different way, all right. \n",
    "### One thing with continuous wavelet transform is that it's a continuous, this undetermined integral is continuous. When we do it using computers, we don't use continuous functions. Our scales are not continuous, we just changed the scale by a very small amount at each iteration and so on. But again, then we end up with a whole bunch of coefficients. So, in our example, we had a signal that goes from 0 to 1000 and we had scales going from 0 to 100. So, we ended up with actually 100,000 coefficients, because we have one pass over the signal for each scale. \n",
    "### With the discrete wavelet transform, they use different tricks. One is similar to what we do in fast Fourier transform where we use the Nyquist limit to remove some of the redundant coefficients. They also don't actually do the pro sync in this way. They use two filters high-pass and low-pass filter and then they branch out low-pass filter almost growing at like a tree\n",
    "### the idea is that the discrete wavelet transform is more efficient in terms of the size of the coefficient, so it's often used for data compression as mentioned here and also for noise reduction for example in images.\n",
    "### The question now is: how can we use wavelet transform for machine learning? And probably very basic pipeline could look like this, you have some signal s, and what you do is you apply wavelet transform. Let say continuous wavelet transform and then from the coefficients if you want to be clever, you can do something like principal component analysis. \n",
    "### Select the important components or maybe if you use a discrete wavelet transform, then you can apply PCA on the different sub bands and then you extract features which you can feed to some kind of a classifier, let say support vector machine and then using support vector machines, you can classify the incoming signal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wavelet Transform and SVM Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e data set that I'm using here comes from the urban sound data set. It's a freely available data set. You can go. You can download it, play with it. There are hundred of examples from different urban noises that you can analyze and play with. And what I've done here is, I have sampled two categories, two different types of sounds. One is coming from air conditioners, the other from drills. And I have selected 20 audio clips from each category. So I have something like 40 clips in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ZipFile' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-29ff83e0d087>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mZipFile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamelist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ZipFile' is not defined"
     ]
    }
   ],
   "source": [
    "ZipFile.namelist(zip_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_data=[]\n",
    "labels=[]\n",
    "sampling_rate=[]\n",
    "file_names=[]\n",
    "for file_name in ZipFile.anmelist(zip_file):\n",
    "    # skip directories\n",
    "    if not os.path.basename(file_name):\n",
    "        continue\n",
    "    \n",
    "    audio_file=None\n",
    "    \n",
    "    if file_name.startswith(\"audio_data/ac/\"):\n",
    "        labels.append(0)\n",
    "        audio_file=zip_file.open(file_name)\n",
    "    elif file_name.startswith(\"audio_data/drill/\"):\n",
    "        labels.append(1)\n",
    "        audio_file = zip_file.open(file_name)\n",
    "    else:\n",
    "        print (\"unknown file class. Skipping.\")\n",
    "    \n",
    "    if audio_file is not None:\n",
    "        file_names.append(file_name)\n",
    "        tmp=BytesIO(audio_file.read())\n",
    "        data, samplerate = sf.read(tmp)\n",
    "        audio_data.append(data)\n",
    "        sampling_rate.append(samplerate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And once this is done, I notice that some of the clips have different sampling rate. So what I do here is I kind of re-sample them so I have everything the same sampling rate, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(len(audio_data)):\n",
    "    if (sampling_rate[index]==48000):\n",
    "        audio_data[index]=librosa.resample(audio_data[index], 48000, 44100)\n",
    "        sampling_rate[index]=44100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Okay, I also noticed that some of the recordings are in stereo. Other recordings are mono so I kind of convert everything to mono to keep it simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_mono (data):\n",
    "    if data.ndim > 1:\n",
    "        data=np.mean (data, axis=1)\n",
    "        return data\n",
    "for index in range (len(audio_data)):\n",
    "    audio_data[index] = to_mono(audio_data[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Okay, and I want to give you an example. So this is an audio clip from the first category. This is an air conditioner and that's a plot that comes from, as you can see, the time domain, right? We had those spikes here when, I don't know, the AC, maybe the pump is running or the fan is spinning or something. But we have these rhythmic noise here, right?\n",
    "### And there is a clip from the other category. That's the drill. You see kind of a more constant signal, probably and also louder because this one goes up to 0.6. This one goes to 0.4. They're probably not on the same scale, but it doesn't matter anyway. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure (figsize=(14,6))\n",
    "plt.plot(audio_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure (figsize=(14,6))\n",
    "plt.plot(audio_data[21])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So what I would do now is I'm using this library PyWT, which is, again, a freely available library for computing wavelets in python. And what I do is I kind of parse just a couple of seconds actually from each audio clip. Because if you look at the AC data for example, I'm just interested in the beginning. I just want to capture these spikes because they are characteristic for the signal. I don't really have to go over the entire signal. So I'm just looking at the first of 25,000 data points. This should be sufficient. I want to keep things simple and it's pretty much the same here with the drill. Once I get up to 25,000 it kind of, it doesn't make sense to continue. So what I do is I kind of compute the wavelet coefficients and then I plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scales=np.arange(1,101)\n",
    "coeff1, freqs1=pywt.cwt(audio_data[1][:25000], scales, 'morl')\n",
    "coeff1, freqs1=pywt.cwt(audio_data[21][:25000], scales, 'morl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I plot them using two scaleograms and you can immediately see. Now you know how to read this. For the AC signal we have these three spikes in the lower scales at about, what is this, 15,000? So if I go back to the signal, here, about 15,000, we have these three very characteristic spikes, right? And if you look at the other signal that comes from the drill, well we have this spike here, but you know it's kind of all over the place, plenty of different frequencies mixed together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, figsize(20,10))\n",
    "plt.subplot(121)\n",
    "plt.imshow(coeff1, cmap='coolwarm', aspect='auto')\n",
    "plt.subplot(122)\n",
    "plt.imshow(coeff1, cmap='coolwarm', aspect='auto')\n",
    "\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  will also do a 3D plot just to get a better view. And this will take a while because if you think about this, now we have a scale that goes from 0 to 100. We have 25,000 points in the signal data points. So we have to multiply 25,000 by 100 to get the number of coefficients. And that's just the first plot, right? And we have two of them because we have two audio clips. So this will take a while but it will give you really nice view of how these two signals differ. So let's give it a time. It's ready.\n",
    "### So on the left hand side we have our AC. And you can see these spikes here in the the high frequency area of the plot. And on the right hand side you'll see this drill which is all over the place with this spike here. Where is this? Close to 25,000 if we go back to the signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "\n",
    "fig=plt.figure(figsize=(40,15))\n",
    "\n",
    "ax1=fig.add_subplot(1,2,1, projection='3d')\n",
    "\n",
    "Y= np.arange(1,101,1)\n",
    "X=np.arange(1,25001,1)\n",
    "\n",
    "X,Y = np.meshgrid(X,Y)\n",
    "\n",
    "ax1.plot_surface(X,Y, coeff1, cmap=cm.coolwarm, linewidth=0, antialiased=True)\n",
    "\n",
    "ax1.set_xlabel (\"Time\", fontsize=20)\n",
    "ax1.set_ylabel (\"Scale\",fontsize=20)\n",
    "ax1.set_zlabel (\"Amplitude\", fontsize=20)\n",
    "\n",
    "ax1.set_zlim3d(-1,1)\n",
    "\n",
    "ax2=fig.add_subplot(1,2,2, projection='3d')\n",
    "\n",
    "ax2.plot_surface(X,Y, coeff2, cmap=cm.coolwarm, linewidth=0, antialiased=True)\n",
    "\n",
    "ax2.set_xlabel (\"Time\", fontsize=20)\n",
    "ax2.set_ylabel (\"Scale\",fontsize=20)\n",
    "ax2.set_zlabel (\"Amplitude\", fontsize=20)\n",
    "\n",
    "ax2.set_zlim3d(-1,1)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  want to keep it simple for this example. I just apply principal component analysis and just a single component using the scale. So instead of having to work with 2 million coefficients, I get something like 100. And I do this for each audio clip in my data set, so this will take a while. And then once I'm done, I split the data set into training and test at the ratio of 80:20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA (n_components=1)\n",
    "\n",
    "features = np.empty((0,100))\n",
    "\n",
    "for ind in range (len(audio_data)):\n",
    "    print ('.', end='')\n",
    "    coeff, freqs = pywt.cwt(audio_data[ind][:25000], scales, 'morl')\n",
    "    features = np.vstack([features, pca.fit_transform(coeff).flatten()])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And then I train an SVM classifier, and then I make predictions and see what the accuracy of my classifier is. So there the date is now split. Now I'm feeding my support vector machines model. And then I will make a prediction, which is surprisingly accurate. So this is one way of using wavelet transform to extract features from really complex signals generated by non-stationary processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split (features, labels, test_size=0.20, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "print (\"Accuracy: %.2f%%\" % (accuracy_score(y_test, y_pred) * 100 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUIZ Wavelet Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A signal that does not change in time is said to be generated by\n",
    "### stationary process\n",
    "## Which of the following signals are generated by a stationary process\n",
    "### white noise (a signal containing many frequencies with equal intensities), a sum of multiple sine waves, each having a fixed frequency and amplitude\n",
    "## A key limitation of Fourier transform is that it cannot provide information on when specific frequencies occur in the signal.\n",
    "### True\n",
    "## The visual representation of a wavelet transform is called\n",
    "### scalogram\n",
    "## The wavelet defined by the function ψ(t) and used in the scaling and translation process is called\n",
    "### Mother wavelet\n",
    "## The x and y axes of a 2D scaleogram represent\n",
    "### scale time\n",
    "## Passing the signal through a series of low pass and high pass filters is a step in the calculation of\n",
    "### Discrete Wavelet Transform (DWT)\n",
    "## The signal shown on the plot above has been generated by\n",
    "### a non-stationary process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 with Spark 2.1",
   "language": "python",
   "name": "python2-spark21"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
