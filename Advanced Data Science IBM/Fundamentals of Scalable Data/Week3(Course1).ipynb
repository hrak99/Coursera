{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3\n",
    "Note: Use 2.7 pythpn with 2.1 spark here to avoid errors like (list + range) or lambda (x,y) syntax\n",
    "mean & median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mean & median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean is pulled into the direction of outlier\n",
    "# Median is resistant to outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.5\n"
     ]
    }
   ],
   "source": [
    "rdd=sc.parallelize(range(100))\n",
    "\n",
    "sum=rdd.sum()\n",
    "mean=sum/float(rdd.count())\n",
    "print ((mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortandindexed=rdd.sortBy(lambda x:x).zipWithIndex().map(lambda(value,key):(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=sortandindexed.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74.0\n"
     ]
    }
   ],
   "source": [
    "if(n%2==1):\n",
    "    index=(n-1)/2\n",
    "    print (sortandindexed.lookup(index))\n",
    "else:\n",
    "    index1=(n/2-1)\n",
    "    index2=n/2\n",
    "    value1= sortandindexed.lookup(index1)[0] # lookup return in list so use index[0]\n",
    "    value2= sortandindexed.lookup(index2)[0]\n",
    "    \n",
    "    print (float(value1+value2/2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard deviation or variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## how wide the spectrum of the data around the mean. More wider the spectrum, higher the sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.86607004772212"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import sqrt\n",
    "sqrt(rdd.map(lambda x: pow(x-mean,2)).sum()/n)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# note: mean will remain the same but standard deviation will change...\n",
    "# if you add an outlier, e.g range(100) + [1000], standard deviation will also increase with mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variance\n",
    "\n",
    "#### Standard deviation to the power of 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skewness"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# asymmetrical data spread aorund the mean\n",
    "# tail determines the positive (right side) or negative skew (left side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.7853275241795927e-12"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import sqrt\n",
    "sd=sqrt(rdd.map(lambda x: pow(x-mean,2)).sum()/n)\n",
    "\n",
    "skewness=(n/(n-1)*(n-2))*rdd.map(lambda x: pow(x-mean,3)/pow(sd,3)).sum() # please use 1/n\n",
    "skewness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kurtosis"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Shape of the data outliers presence. Highest the tail, higher the kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd=sqrt(rdd.map(lambda x: pow(x-mean,2)).sum()/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7997599759975997"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kurtosis=rdd.map(lambda x: pow(x-mean,4)/pow(sd,4)).sum()/(n) # kindly divide by n not by (1-n)\n",
    "kurtosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covariance & Correlation\n",
    "#determine the dependency between pair of data and correlation normalize the data by the standard deviation of each source in order for comparable mearsure (-1,0,+1)\n",
    "\n",
    "#how two columns intereact each other\n",
    "\n",
    "#how all columns intereact each other\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.5\n",
      "6.7\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "a=[1,2,3,4,5,6,7,8,9,10]\n",
    "b=[7,6,5,4,5,6,7,8,9,10]\n",
    "rddX=sc.parallelize(a)\n",
    "rddY=sc.parallelize(b)\n",
    "\n",
    "meanX = rddX.sum()/float(rddX.count())\n",
    "meanY = rddY.sum()/float(rddY.count())\n",
    "print(meanX)\n",
    "print (meanY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.65"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddXY=rddX.zip(rddY)\n",
    "covXY= rddXY.map(lambda (x,y) : (x-meanX) * (y-meanY)).sum()/rddXY.count()\n",
    "covXY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.87228132327\n",
      "1.79164728672\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "n=rddXY.count()\n",
    "sdX=sqrt(rddX.map(lambda x: pow(x-meanX,2)).sum()/n)\n",
    "sdY=sqrt(rddY.map(lambda x: pow(x-meanY,2)).sum()/n)\n",
    "print (sdX)\n",
    "print (sdY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.709272912083725"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrXY= covXY/(sdX*sdY)\n",
    "corrXY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Matrix\n",
    "To view all columns at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pyspark.mllib.stat import Statistics\n",
    "column1=sc.parallelize(range(100))\n",
    "column2=sc.parallelize(range(100,200))\n",
    "column3=sc.parallelize(list(reversed(range(100))))\n",
    "column4=sc.parallelize(random.sample(range(100),100))\n",
    "column1=sc.parallelize(range(100))\n",
    "column2=sc.parallelize(range(100,100))\n",
    "column3=sc.parallelize(list(reversed(range(100))))\n",
    "column4=sc.parallelize(random.sample(range(100),100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          1.         -1.          0.09177318]\n",
      " [ 1.          1.         -1.          0.09177318]\n",
      " [-1.         -1.          1.         -0.09177318]\n",
      " [ 0.09177318  0.09177318 -0.09177318  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.stat import Statistics\n",
    "import random\n",
    "column1 = sc.parallelize(range(100))\n",
    "column2 = sc.parallelize(range(100,200))\n",
    "column3 = sc.parallelize(list(reversed(range(100))))\n",
    "column4 = sc.parallelize(random.sample(range(100),100))\n",
    "data = column1.zip(column2).zip(column3).zip(column4).map(lambda (((a,b),c),d) : (a,b,c,d) ).map(lambda (a,b,c,d) : [a,b,c,d])\n",
    "print(Statistics.corr(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 100, 99, 26],\n",
       " [1, 101, 98, 54],\n",
       " [2, 102, 97, 89],\n",
       " [3, 103, 96, 0],\n",
       " [4, 104, 95, 18],\n",
       " [5, 105, 94, 25],\n",
       " [6, 106, 93, 32],\n",
       " [7, 107, 92, 24],\n",
       " [8, 108, 91, 58],\n",
       " [9, 109, 90, 77]]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  1.        , -1.        ,  0.09177318],\n",
       "       [ 1.        ,  1.        , -1.        ,  0.09177318],\n",
       "       [-1.        , -1.        ,  1.        , -0.09177318],\n",
       "       [ 0.09177318,  0.09177318, -0.09177318,  1.        ]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Statistics.corr(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 with Spark 2.1",
   "language": "python",
   "name": "python2-spark21"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
