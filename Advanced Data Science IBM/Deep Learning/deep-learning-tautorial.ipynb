{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Neural Netwrok"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# importing from keras\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import adam\nfrom sklearn.model_selection import train_test_split\n# seed for reproducing same results\nseed = 20\nnp.random.seed(seed)\n\n# load pima indians dataset\ndf = np.loadtxt(\"../input/Attachment_1556072961 - Copy.csv\")#, delimiter=\",\")\n# split into input (X) and output (Y) variables\n# split into input and output variables\nX = df[:,1:256]\nY = df[:,0]\n\n# split the data into training (80%) and testing (20%)\n(X_train, X_test, Y_train, Y_test) = train_test_split(X, Y, test_size=0.20, random_state=seed)\n\n#X_train = X_train.astype('float32')\n#X_test = X_test.astype('float32')\n\n#X_train /= 255\n#X_test /= 255\n\n# create the model\nmodel = Sequential()\nmodel.add(Dense(100, input_dim=255, init='uniform', activation='relu'))\nmodel.add(Dense(100, init='uniform', activation='relu'))\nmodel.add(Dense(1, init='uniform', activation='sigmoid'))\n\n# compile the model\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# fit the model\nmodel.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=10, batch_size=5)#, verbose=0)\n\n# evaluate the model\nscores = model.evaluate(X_test, Y_test) #29.27\nprint(\"Accuracy: %.2f%%\" % (scores[1]*100))\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true},"cell_type":"markdown","source":"# Applied AI with DeepLearning"},{"metadata":{},"cell_type":"markdown","source":"# Introduction to TensorFlow"},{"metadata":{},"cell_type":"markdown","source":"## Introduction to Tensor Flow"},{"metadata":{},"cell_type":"markdown","source":"### TensorFlow is an open-source library in numerical computation and it's using data flow graphs. It allows us to express machine learning and deep learning algorithms and prints along an execution engine, which allows these algorithms to run at scale on multiple nodes in a cluster backed by CPUs, GPUs, TPU's and mobile devices. So every numerical computation is a graph, better note that computations, and on the excess out flowing 10 slots between them, they are from in tens of flow"},{"metadata":{"trusted":false},"cell_type":"code","source":"from tensorflow.examples.tutorials.mnist import input_data\nmnist=input_data.read_data_sets(\"MNIST_data/\", one_hot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"import tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We used the jupiter metric command matplotlib inline, to display to plots directly in the notebook. Then we accessed the first image out of the 55,000 from the MNIST training set using a built-in iterator, which is quite useful for later usage, but now we just accessed one image at a time. Images in the MNIST data set are 28 by 28 pixels, but we are only obtaining a vector of 784 pixel's length. Therefore, we have to reshape accordingly."},{"metadata":{"trusted":false},"cell_type":"code","source":"% matplotlib inline\nimport matplotlib.pyplot as plt\nbatch_xs, batch_ys=mnist.train.next_batch(1)\nX=batch_xs\nX=X.reshape([28,28])\nplt.gray()\nprint batch_ys\nplt.imshow(X)\n\n# This is a one hot encoded vector, this means at the index of the actual corresponding number, we see one, and all the other elements are zero.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now let's start with Tensorflow coding by expressing the state to computation graph using Python. We start with a so-called placeholder. This is a tense service in Tensorflow that data is fed in during execution time. So basically, this is used to add data during training which takes place after this computation graph is constructed. Those placeholders are typed, and we can use either single or double position. So this placeholder would take our training vectors, representing the images, the 784 elements inside. \n### Now will create Tensorflow variable. A variable is something Tensorflow retrieve during training, whereas the placeholder is meant to keep training data. In addition, a variable can be saved to disk during and after training for check pointing and water transfer. So we create our wait matrix W with 784 Baitz on one X's. Just one for each element of X, and we do it 10 times.\n### Since you are basically running 10 soft mix regression motors in parallel, one for each possible digit. Finally, we end up with a bias that draw one foot each soft next regression model. \n### Now we create the actual model. So please be aware that no computation is happening at this stage. We are not basically hooking up the notes together to form a computation of graph. Softmax as well as Matmul expect Tensorflow variables as Polanyi does. So that's the reason TensorFlow court is so hard to read. We are not computing anything, we just despite the expressive computation of graph, which is executed by the TensorFlow engine in the background. \n### TensorFlow is not having it's own domain specific language for doing so, but it's relying on language bindings in different programming languages, like python for example. System ML on the other hand, which is introduced in another module is a bit better here, since System ML is having its own domain specific language and are syntax of Python, which looks found more nature."},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"x=tf.placeholder(tf.float32, [None, 784])\nW=tf.Variable(tf.zeros([784,10]))\nb=tf.Variable(tf.zeros([10]))\ny=tf.nn.softmax(tf.matmul(x,W) + b)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Anyway, let's continue with creating another placeholder for a training level spike. These are often mentioned 10, because why? It is the one hot encoded vector labeling the image. Now we defined the cost function as cross-entropy. Therefore, let me just walk you through the formula and we will see later how to implement it in TensorFlow. So we take a predictive value of Y head and multiply it to the log of the desired value of Y and some of those values up. So we start with the reduce mean function of TensorFlow, because we are now calculating 10 individual cross-entrophy values, one for each softmax regression model. Then we use Reduce Sum, to calculate the sum of the individual values of a Tensor. And this Tensor is the product of the desired value, and the luck of the actual prediction. Reduction indices defines that the dimension of the Tensor that aggregation should take place. Since Y is a matrix of 10 columns and N rows, the N stands for a number of creating examples, the sum over the columns to obtain the value for each digit. This reside has no past to an argument to reduce mean, so that the overall prediction error is calculated all of the individual prediction errors for each number between zero and nine."},{"metadata":{"trusted":false},"cell_type":"code","source":"y_=tf.placeholder(tf.float32, [None, 10])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now, we use TensorFlow GradientDescentOptimizer with the learning rate of zero to five, to tweak W and B with respect to the cross-entropy function. So TensorFlow will take care of calculating the back propagation and gradients for this task automatically. A feature called automatic differentiation, does the job for us. Now we create a TensorFlow session, since we are in an interactive context within a Jupiter notebook we use the interactive session. A session is the way to deploy a TensorFlow execution graph, onto a specific execution context like a CPU or GPU. \n### hen we initialize all global variables, since this hasn't been done. Remember, the chest has only expressed the computation of graph. Now it's time to bring it to life. After the variables have been initialized, it's time to create our GradientsDescent loop. So this is batch Gradienst Descent, since on each iteration, we graph a hand that randomly selected examples from returning set and using the session object, the ExecuteGradientDescent for those hand out examples. Note that if you pass the training example as parameters to this function call, in order to assign them to the previously defined placeholders. So this runs very fast."},{"metadata":{"trusted":false},"cell_type":"code","source":"cross_entropy=tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\ntrain_step=tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sess=tf.InteractiveSession()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"tf.global_variables_initializer().run()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"for _ in range (1000):\n    batch_xs, batch_ys = mnist.train.next_batch(100)\n    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now let's evaluate our classification performance using the test set from MNIST. So argmax returns the index of the tensor, in this case a vector, which the maximum value. This may be can transform back from one hot encoding scheme to a scaler. We use reduce mean in order to determine the amount of correctly predicted values, but since correct prediction is a full in vector, we have to cast it to float in order to calculate the mean over this vector. And again, accuracy is a note in computation of graph. Therefore, we need to use the TensorFlow session in order to execute it. Now the placeholders does become handy, because now we assign the test dataset to the graph. And as expected, this timber regression model gives us 92 percent of accuracy. "},{"metadata":{"trusted":false},"cell_type":"code","source":"correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_ ,1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"accuracy=tf.reduce_mean(tf.cast(correct_prediction, tf.float32))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print (sess.run(accuracy, feed_dict={x:mnist.test.images, y_:mnist.test.labels}))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### So as you've seen training of new networks can be quite complicated. But it's okay if you didn't understand "},{"metadata":{},"cell_type":"markdown","source":"## Neural Network Debugging with Tensor Board"},{"metadata":{},"cell_type":"markdown","source":"### o Neural Network often is some sort of black box and it's very hard to see what's going on during training. So usually people tend to print out all sorts of measures during the Gradient Descent Loop in order to the debug and make sense of the training phase. So what are the most important parameters to be looked at in training Neural Network? \n### So the most important is definitely the loss overtime. So as you remember from the previous video, loss defines how good or bad a Neural Network fits the data. And by the way, this here is a quite good loss function because it converges to a local optima quite fast. The blue line instead is indicating a learning rate which is too small. We are slowly converging against the optimum, but it takes too long. \n### So let's run this for 10,000 iterations and we obtained the following diagram. It's hard to say, but chances are high, that with a too low learning rate, we never reach to decide local optimum. Finally, if you have chosen the learning rate which is too high, we notice a considerable amount of bouncing, and the only reason why we are still conversion is that the underlying model we're optimizing over is rather simple\n### Another may be even more important measure is accuracy. Here you see accuracy over training iterations. And as loss goes down, you should see accuracy going up. So those are scalar times series, but now let's have a look at the weight matrices. This is far more interesting. Those contain many values, and we have to find a way to visualize them at once.\n### Fortunately, TensorBoard does a pretty good job in creating a summary, overweight matrix of arbitrary shape. You can find more on how these summaries are calculated in the video description. But here, we see a pretty healthy distribution of weights. Bad examples include cases where all values are very close to zero or a uniform distribution resembling a random weight initialization. This can be an indication that the actual layer hasn't done anything. Besides monitoring weights, we can also monitor activations.\n### Fortunately, TensorBoard does a pretty good job in creating a summary, overweight matrix of arbitrary shape. You can find more on how these summaries are calculated in the video description. But here, we see a pretty healthy distribution of weights. Bad examples include cases where all values are very close to zero or a uniform distribution resembling a random weight initialization. This can be an indication that the actual layer hasn't done anything. Besides monitoring weights, we can also monitor activations.In this simple case, the [inaudible] Y. Remember, that Y is the output of the Softmax function. So any K dimensional vector, with value range between plus and minus infinity is squashed into a K dimensional vector if value ranges between zero and one. This is the default activation function for output layer and classification tasks.\n### So here we see what we expect in a healthy classifier. Most of the values are close to zero because those are assigned part of probability, a particular input is not in the class. And we see a fair amount of values near to one. Those are the cases where particular input is in the class. Since we have 10 different classes. It is obvious that we see more values close to zero than close to one. \n### So let's have a look at TensorBoard. Instructions on how to access TensorBoard from data science experience are given in the description section of this video. TensorBoard can visualize multiple run simultaneously, in order to compare among those. But let's only have a look at a single run for now. First, you have to look at the Scalar view. So in this step, all summaries of scalars and how they evolve over training time are displayed. In our example, we've recorded the two most important measures. Loss and Accuracy. As you can see, they are in loss, which is actually a very good sign. The lower the loss and therefore the arrow of the neural network is, the better the accuracy. We can maximize the plot and also adjust the smoothing parameter. If we set it to one, we obtain a line which isn't displayed here. So let's decrease it slowly. As we can see, this line, more and more fits the actual trajectory and with 0.96 we can clearly see a trend without getting lost too much into details. And here again, we can have a look at those two important measures and see that they are inverse.\n### emember that loss is based on a defined loss function. Cross-Entropy in this case and accuracy tells us the fraction of correctly classified examples over a total number of examples. So now we have a look at the graph tab. \n### his graph should be read bottom up. So we start with a weight matrix W and the placeholder X for your input data. This is multiplied and then the bias where it will be is added. This result is quashed using Softmax and the final result is thought in Y. Note that Y underscore and Y are then used to calculate accuracy by taking the Argmax of both. By comparing those and taking the mean over this vector, we obtain the accuracy.\n### The upper branch of the graph computes loss using the Cross-Entropy function. Note that parts of the graph are hidden to us in an externalized sub-graph. As you can see, the connection points of the sub-graph have turned red now. You can include the sub-graph but then the graph becomes more complex. This is because the gradient computation is reading values from variables and placeholders all over the place and doesn't add more information at that point.\n### Therefore, let's remove it again. Now, let's turn to histograms and have a look at the weight matrix. This is a highly compressed view of what's going on inside the matrix during training. So in the C axis, training iterations are reflected. The closer, the more recent they are. The x axis informs us about the values, elements in the matrix have been assigned to. And the Y axis shows frequency. So in this case, we are looking at a pretty healthy chart.\n### It is important to understand how this condensed view is created. Therefore, a link containing further explanations can be found below. But intuitively, lacking the histogram in a pure mathematical sense, the plot gives you an intuition but value ranges at what frequencies are assigned to each element of the weight matrix. So we see that the values are symmetrically centered around the mean of zero, but although they are close to zero but not exactly zero. This is important since gradients are correlating with weights and with zero gradients we cannot work.\n### The other observation is, that on the left and the right extremes, close to minus one and plus one, there's nothing much going on. And therefore, we are definitely not over saturating. Finally, this also definitely doesn't look like a uniform distribution. Therefore, training worked pretty well. Now we have a look at Y. Y in the Softmax regression model corresponds to the newer activations of an output player. \n### You see that most of the values are close to zero and some are close to one and in between. That's nothing much going on. And this is also a very good sign, since this is exactly what Softmax should do in a mighty class classifier, because class probabilities in the output vectors are either zero for the wrong classes or one for the correct class. Since there are more incorrect classes and correct ones, nine versus one in this case, you see much more zero values than one's. "},{"metadata":{},"cell_type":"markdown","source":"## Automatic DIfferentiation"},{"metadata":{},"cell_type":"markdown","source":"### one of the key features of TensorFlow. '\n### So, if you take the first derivative of cosine of x, it turns out that this is just minus sign of x. Let's create a little plot to illustrate this. So, the green line is cosine of x and the blue line is the first derivative of cosine of x. As you might remember, the first derivative tells us something about a slope of the function. So here at this point, where the slope is zero, the value of the first derivative is zero as well. At the this deepest position, the derivative becomes minus one. Again, the cosine function reaches zero gradient. And this is reflected by the first derivative as well. Finally, when the cosine function reached its maximum ascending gradient, the first derivative becomes one.\n### o now, let TensorFlow take care of computing the first derivative of the cosine function. We start with a value x and apply TensorFlow's cosine operation on it. Then we tell TensorFlow to minimize overbuy. In order to achieve this, TensorFlow must compute the first derivative of the cosine function. Note that until now, no competitions have happened. Only the execution graph has been created. Let's have a look at this graph in TensorBoard now."},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"import numpy as np\n%matplotlib inline\nimport matplotlib.pyplot as plt\nx=np.arange(0, 5, 0.1)\n\ncos = np.cos(x)\nsin = -np.sin(x)\n\nplt.plot(x,sin) # blue line sin\nplt.plot(x,cos) # green line cos","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### o, this is the graph of our example, which contains the created operation. Within this section, the gradient of the cosine function is computed, which is actually nothing else than the first derivative of the function. As you can see, this is reflected by the appropriate operations. It turns out that for every atomic mathematical operation, it is guaranteed that there exists a derivative operation. And with a chain rule of differentiation, we are guaranteed to obtain a solution for the first derivative, no matter how complex the functions are getting. \n### Just remember, every operator which is available within TensorFlow has the richest and appropriate created function. Here in this case, an operator called, ZeroOut, is RegisterGradient function called zero_out_grad. This way, TensorFlow is capable of automatically compute the derivative of any function defined in a TensorFlow graph. So, we only need to come up with a creative idea of a model, and TensorFlow does the rest."},{"metadata":{"trusted":false},"cell_type":"code","source":"import tensorflow as tf\nx=tf.Variable(initial_value=3.0)\ny=tf.cos(x)\n\ntrain=tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(y)\n\nwith tf.Session() as sess:\n    writer=tf.summary.FileWriter('logs', sess.graph)\n    writer.close()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Quiz Tensor Flow"},{"metadata":{},"cell_type":"markdown","source":"## Which statement is correct?\n### TensorFlow is a library for arbitrary numerical computation not limited to DeepLearning only\n## What is a TensorFlow placeholder?\n### A way to add data to the TensorFlow execution graph at a later stage\n## What data is usually stored in a TensorFlow variable?\n### The weight matrix W\n## What statements are correct in respect to a unhealthy value distribution (histogram) of values in the weight matrix?\n### A uniform distribution indicates a lack of parameter updates and therefore problems with training, Most of the values centered very close to zero forces gradients to become very small, Most of the values centered very close to zero forces gradients to become very small\n## What is the relationship between accuracy and loss in a healthy and working neural network?\n### Both values should behave inverse during neural network training\n## Which statements about automatic differentiation in TensorFlow are correct?\n### Every operator in TensorFlow has registered the first derivative of it's operation as well. Therefore TensorFlow can apply the chain rule of automatic differentiation in order to compute the 1st derivative of any complex function"},{"metadata":{},"cell_type":"markdown","source":"# Introduction to Keras"},{"metadata":{},"cell_type":"markdown","source":"## Introduction"},{"metadata":{},"cell_type":"markdown","source":"### I will introduce you to a popular high level deep running library written in Python called Keras."},{"metadata":{},"cell_type":"markdown","source":"## Overview Keras"},{"metadata":{},"cell_type":"markdown","source":"### Keras is a popular deep learning framework written in Python.\n### t has been open sourced in 2015, and it's excellent documentation can be found on the keras.io. In case you're interested, the source code can be found on GitHub under the following link. \n### github.com/fchollet/keras\n### Okay, why would you choose Keras as your debugging tool? There's quite a few aspects to it, and one thing I want to mention is community. Keras is a very popular library. And as an active and positive community that might help you with any questions that you might have. So for instance, you might direct your questions towards the following Google group or you join the following Slack channel, and people could help you there.\n### google.com/forum/#forum/keras-users\n### keras-slack-autojoin.herokuapp.com\n### Alright, another crucial aspect that makes Keras a strong pick is its design. Keras has an intuitive high-level API that makes it very easy for you to get started with first examples and deep learning. With this intuitive API, you can quickly prototype new models and have a very fast development and prototyping cycle. Also, Keras is built with very modular building blocks and easy to extend. So for instance, if you want to build new custom layers, that's quite easy to achieve with Keras.\n### Another thing that makes Keras quite interesting is the ecosystem. For instance, under the following link you'll find many resources on top of Keras, that includes third party libraries, as well as full-blown applications built on top of Keras. Also, Keras comes shipped with many end to end examples that you can simply check out and run. On top of that, in Keras you have many pre-built models that have all ready been trained for you.\n### And there's quite a few data sets available in Keras. So that means, you don't have to manually download many of the standard data sets out there and pre-process them. But you can just load them through the Keras API.\n### github.com/fchollet/keras-resources\n### github.com/fchollet/keras/tree/master/examples\n### github.com/fchollet/deep-learning-models\n### github.com/fchollet/keras/tree/master/keras/datasets\n\n"},{"metadata":{},"cell_type":"markdown","source":"### Another interesting aspect to Keras is the concept of backends. So one way to view Keras is, you can see it as your deep learning front-end. So essentially what Keras provides is your high level entry point to deep learning, but in the back, what actually runs Keras is different engines that do all the heavy lifting.\n### So there's a choice of backends available for Keras. First of all, there's Google Tensorflow, which is also the default engine for Keras. And we will exclusively use Tensorflow in this course. Then there's theano, and also Microsoft CNTK. You can quite easily swap backends and depending on your configuration, Keras runs seamlessly on CPUs or GPUs.\n## What is the default backend of keras?\n### Tensorflow"},{"metadata":{},"cell_type":"markdown","source":"### So before we introduce the concepts of Keras in more detail, let's just have very first look, a glimpse, at an example built with Keras. First thing you do is, you load the data set, in this case, the mnist data set 400 digits, directly through the Keras API. Then, you define a model, in this case, a sequential model. After we have done so, you can just add new layers to your model one by one. After that, we compile our model with a loss function and an optimizer and fit our model on training data. On a high level, that's all there is to it. Every Keras example, more or less looks like this.\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"# keras example\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\n\n# load data\n\nfrom keras.datasets import mnist\n(x_train, y_train), (x_test, y_test)  = mnist.load_data()\n\n# more preprocessing \n\n# Define Model\n\nmodel = Sequential ()\n\n# Add layers\n\nmodel.add(Dense (256, activation='sigmoid', input_shape=(784,)))\n\nmodel.add(Dense (10, activation='softmax'))\n\n# Compile Model with loss & optimizer\nmodel.compile (loss='categorical_crossentropy', optimizer=SGD(), metrices=['accuracy'])\n\n# Train network\nmodel.fit(x_train, y_train, batch_size=128, epochs=10, validation_data=(x_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Which layers should have an input_shape argument in an MLP?\n### Just the first layer"},{"metadata":{},"cell_type":"markdown","source":"### All right, to install Keras, we first have to install a backend, as we said before we're using Tensorflow here. So go ahead and install Tensorflow with pip install Tensorflow. Optionally, we also need to install other dependencies. [COUGH] For instance, we need the library h5py for model. So go ahead and install it through pip installer h5py. As last step, we need to install Keras itself through pip installer keras."},{"metadata":{"trusted":false},"cell_type":"code","source":"# install tensorflow\n!pip install tensorflow","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Optionally install other dependencies\n!pip install h5py graphviz pydot","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Install keras\n!pip install keras","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Sequential Models in Keras"},{"metadata":{},"cell_type":"markdown","source":"### In Keras, you have essentially two types of models available. One is called Sequential and you use it to define sequential models, meaning you simply stack layers one by one, sequentially. And this is the focus of this lecture. There's another model available in Keras that is mainly used for non-sequential models, and it goes by the name Model.\n### Those more general models use a functional API. We're going to introduce this API later on. Okay, let's take a step back. Regardless of which model you define, the core abstraction to each Keras model is the notion of a layer. So, each model consists of a bunch of layers. In a sequential model, we stack layers sequentially. So, each layer has unique input and output, and those inputs and outputs then also come with a unique input shape and output shape. \n### If you want to retrieve the weights a layer has, you can simply call get weights on layer and retrieve all those weights as as a list of numpy arrays. If you want to set weights, that's also possible by simply calling set weights with a given list of numpy arrays weights. Also, each layer has its defining configuration which you can get by calling get config on a layer.\n### layer.get_weights(), layer.set_weights(weights), layer.get_config()\n### To build a sequential model, you have to carry out a few steps. First, we instantiate a sequential model, then we add layers to it one by one. Afterwards, we need to compile a model with a mandatory loss function and a mandatory optimizer, and if we like, also with optional evaluation metrics such as accuracy. Afterwards, we fit our model to training data that we provide. And after that, well, it really depends what you want to do. Most of the time we evaluate our model but, sometimes you also want to maybe serialize or persist model or deploy it somewhere or start a new experiment altogether. \n### We have to provide loss function and we have to provide an optimizer. Defining a loss function can be done in two ways. First, we can import a specific loss function, in this case, mean squared error, from the Keras losses module. This is the recommended way. If you compile a model with this, you simply specify the loss keyword here, we set it to mean squared error, and we left out the optimizer for now. The second option is simply call the respective loss function by name. That is, you provide a string that stands for the loss function. This is somewhat error prone, because if you have a typo in that, it simply won't work. So, we really recommend the first way.\n"},{"metadata":{},"cell_type":"markdown","source":"# option 1: importing from loss module (preferred)\n## from keras.losses import mean_squared_error\n## model.compile (loss=mean_squared_error, optimizer='')\n# option 2: Using Strings\n## model.compile (loss='mean_squared_error', optimizer='')"},{"metadata":{},"cell_type":"markdown","source":"### To defend an optimizer again, there's two way of doing this. The first and preferred way for us is you import a specific optimizer from the Keras optimizers sub module. In this case, we import stochastic gradient descent, SGD. Then, we instantiate an SGD object by specifying a few parameters. So, for instance here, we set the learning rate to 0.01 and also put decay factor that will decrease the learning rate after each parameter update and we also set momentum parameter to 0.9 for this particular optimizer.\n### Then, we can compile our model with this particular instance SGD. The second option is, you simply pass a string, pretty much the same way as we did for losses, but in this case, it's crucial to see that if you pass SGD as a string, you cannot specify any specific parameters but, all the default parameters would just be valid. "},{"metadata":{},"cell_type":"markdown","source":"# option1: Load optimizers from module (preferred)\n##cfrom keras.optimizers import SGD\n## csgd= SGD (lr=0.01, # learning rate >=0\n##           decay=1e-6, # lr decay after updates\n##            momentum=0.9  # Momentum Parameter Used for SGD\n##         )\n\n## model.compile(loss='', opptimizer=sgd)\n\n# option 2: using strings (default paramteres will be used\n## model.compile(loss='', optimizer='sgd')"},{"metadata":{},"cell_type":"markdown","source":"### Once you're done with compiling your model, you simply fit it to training features and labels, and you also always have to specify a batch size, the number of epochs you want to train, and optionally you can also specify validation data. Afterwards, you can evaluate your model or simply predict on new features, as you like. Okay"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Fit a model to train the data\nmodel.fit(x_train, y_train, batch_size=32, epochs=10, validation_data=(x_val, y_val))\n\n# Evaluate on test data\nevaluate(x_test, y_test, batch_size=32)\n\n# Predict Labels\npredict(x_test, batch_size=32)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feed Forward Networks"},{"metadata":{},"cell_type":"markdown","source":"### you're going to learn to implement feed-forward networks with Keras and build a little application to predict handwritten digits. In the introduction to deep learning in this course, you've learned about multi-layer perceptrons or MLPs for short. These kinds of networks are also sometimes called densely-connected networks. And to build them, we essentially have to sack layers of so-called dense layers on top of each other with activations.\n### A technique that we're going to use for regularization is called a Dropout, and we build Keras Dropout layers into our network to achieve that. We then build a sequential model from both Dense and Dropout layers to arrive at a application. "},{"metadata":{},"cell_type":"markdown","source":"### To initialize the Dense layer, we'll have to do a few things. First off, you always have to specify the number of output neurons or units that the layer is going to have. Secondly, usually, you want to provide an activation function. So, if you don't, there won't be any. Okay. So, there's None right there as a default keyword. And if you want to have a Sigmoid, then simply put the name Sigmoid there, or [inaudible] , whatever you like. The third argument is use_bias which set to true, which indicates that we are using a bias term. And you probably shouldn't touch that unless you know what you're doing. And the last two keywords in this signature are the kernel_initializer and the bias_initializer which are set to specific initialization that meets, so the kernel, or the weights of this dense layer are set to glorot_uniform initialization and the bias's are simply set to zero. So, unless you know a lot about initialization which we don't really cover in this lecture here, you probably shouldn't touch many of the keywords that are provided in Keras for you. "},{"metadata":{"trusted":false},"cell_type":"code","source":"from keras.layers import Dense\nDense (units, # Number of output neurons\n       activation=None, # Activation Function by name\n       use_bias=True, # use bias term or not\n       kernel_initializer='glorot_uniform',\n       bias_initializer='zeros')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### So Dropout layers are much easier to specify. Actually, you just have to specify rate. Meaning, a value between zero and one, which indicates the fraction of units to drop in each forward pass. If you want, you can also specify random seed for reproducibility."},{"metadata":{"trusted":false},"cell_type":"code","source":"from keras.layers import Dropout\nDropout(rate, # Fraction of units to drop\n        seed=None) # Random seed for reproducibility)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ll right, let's move on to a building, an actual application. We're going to use the mnist dataset of hundred digits. The mnist datasets consist of 60,000 train sample and 10,000 samples for tests. And each individual sample is a 28 by 28 image which has a handwritten digits on it. The labels are simply encoded as the actual digits, 0-9. So to built this application, you first, import the mnist dataset from Keras and also import the [inaudible] function of these later on. And our sequential model, and the two layers that we're going to use, Dense and Dropout. Okay. The first thing, we specify here is the batch_size. And we set it to 128. This batch_size will be used in the forward pass and also for the predictions. The number of classes is the number of digits there are [inaudible]. And we are going to train on that for 20 epochs in total. So, and something what we have to do to load data is called, mnist.load_data to retrieve training and test features and [inaudible]."},{"metadata":{"trusted":false},"cell_type":"code","source":"from keras.datasets import mnist\nfrom keras.utils import to_categorical\nfrom keras.layers import Dense, Dropout\nfrom keras.models import Sequential\n\nbatch_size=128\nnum_classes=10\nepochs=20\n\n(x_train, y_train), (x_test, y_test) = mnist.load_data()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  Okay. Next step is data pre-processing. So I mentioned before that, the mnist samples of 28 by 28 images, and we need to flatten them to 784 vector to feed them into dense layer. So first, we're going to reshape both train and test data. Then, set them to a float type, and divide them by 255 to arrive at values that lie between zero and one. As a last step in pre-processing, we're going to [inaudible] the labels that we have, with our function two categorical. So that means, for instance, if we have a label with the number zero on it, is that with zero, this is going to be transformed into a vector of length 10 that has all zeros but one at the first place."},{"metadata":{"trusted":false},"cell_type":"code","source":"x_train=x_train.reshape(60000, 784)\nx_test=x_test.reshape(10000, 784)\nx_train=x_train.astype('float32')\nx_test=x_test.astype('float32')\nx_train /=255\nx_test /= 255\n\ny_train=to_categorical(y_train,num_classes)\ny_test=to_categorical(y_test,num_classes)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Next, we can proceed to defining and running our model. So we start by initializing a sequential model and then, adding Dense and Dropout layers one by one. All right. In first layer, you see that we also specify the input shape, which is essentially 784, the length of our vectors. This input shape has to be provided only in the first layer and succeeding shapes and other layers are then referred by Keras for you. So as you can see, we have three Dense layers in total. Then one with output length of 512. Another one with 512. And then, the final layer as 10 output classes. And we also adds two Dropout layers with a drop rate of 20 percent.\n\n### ll right. Once we have specified our model, we can get a summary, print it on the command line by imposing model of that summary. Next, we compile our model with categorical_crossentropy, and specify the optimizer as to [inaudible] and also evaluate the accuracy metric Okay"},{"metadata":{"trusted":false},"cell_type":"code","source":"model=Sequential()\n\nmodel.add(Dense(512, activation='relu', input_shape=(784, ))) # First layer only\nmodel.add(Dropout(0.2))\nmodel.add(Dense(10, activation='relu'))\nmodel.add(Dropout(0.2))\n\nmodel.summary()\n\nmodel.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### So we can then fit our model with the train data that we have. We set the batch size as defined previously in the epochs. And we can also specify validation data namely the test data that we've updated. So the last step we do in this model is we create a score by evaluating the model. \n### In this case, we get back a pair when we did test loss and the accuracy which we print to the command line as well. If you do so, you should achieve about 98 percent accuracy with this model."},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))\n\nscore = model.evaluate(x_test, y_test, verbose=0)\n\nprint ('test loss', score[0])\nprint ('test accuracy', score[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Recurrent Neural Networks"},{"metadata":{},"cell_type":"markdown","source":"### we're going to introduce the Recurrent Neural Networks in Keras, and in particular talk about an esteems. As an application, we're going to classify sentiments from movie reviews. So there's a few architectures available for Recurrent Neural Networks in Keras. The first one is a class called simpler RNN, which is a basic plane on the nose recurrent neural network, which suffers from problems like vanishing and exploding radiant. So you see those very rarely used in practice that gated recursive units introduced in 2014, has certainly its used cases. Also, an LSTM are a long short term memory models introduced in 1997 by Huffington Schmidhuber. Those are the most popular Recurrent Neural Networks in Keras in maybe overall. So, oftentimes when people talk about using RNNs usually they mean finding LSTMs. "},{"metadata":{},"cell_type":"markdown","source":"### How do we build LSTM layers with Keras? There's a few things we have to specify. Recall that with LSTM layer, you have two sets of weights. First, regular set of weights and then also recurrent set of weights. So in Keras that's called recurrent RNN. And apart from specifying the units, so the number of units and then activation fronting for all layer, we also have to specify everything recurrent. So for instance we need a recurrent initializer or recurrent activation and so on\n### So if you want to specialize or specify a regularizer or constraint, you can do that. Something of note here is that LSTM layers allows you to specify a dropout within the layer. So you don't have to specify a dropout layer separately, but you can specify a dropout rate for a LSTM. Both for direct set of weights, specified by dropout and for the recurrent weights specified by recurrent neural network. \n### he last key words are we mention here is return sequences which are set True, False, by default. So, what that means is if you started to true, then return of your LSTM won't be a simple vector but a matrix instead. So, where you run an LSTM is by you apply different time sets. Right? You go through different time sets and at the end the output is one, one back to usual. If you set return sequences to true, you will also get returns, all at the immediate values adding up at the matrix time."},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"from keras.layers.recurrent import LSTM\nLSTM (units, activation='tanh', recurrent_activation='hard_sigmoid', recurrent_initializer='orthogonal', recurrent_regularizer=None,\n     dropout=0.0, recurrent_dropout=0.0, return_sequence=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Okay. To compute our application we need to introduce another layer which is quite useful for many such applications. And so embedding letters, are used as a very first layer in Keras. And what you use them for is you transform integers into vectors and like. To you give you one example, these two numbers are three and 12, those can be embedded into vectors of length two, which are on the right. A prototypical example that you would let us join which is going to do, is we want to embed a certain vocabulary into a vector space, meaning each word in those vocabulary has to be embedded or has to be given a representation of a vector space, and then you apply those embedments to sequences of words.\n### This is when you can map a 2D input to a 3D output which connects to LSTM. So meaning, if you have a mini-batch of sequences of IDs, you can map those to mini-batch of sequences of vectors by a mini-batch of matrices. And this can be set into an LSTM."},{"metadata":{},"cell_type":"markdown","source":"### How do we initialize on embedding with Keras? There's a few things we have to specify. First off, the input dimension is our vocabulary size that we have to define before length. We also need to specify the output vector length, of the day. In our example from the previous lines, which show two dimensional vectors that usually returns much larger vectors as I put in dimension. Third item we need to specify here, is this so called embeddings initializes. So the weights of embedding in Keras are called Embeddings Voids and we initialize them like that.\n### A false key word what's quite interesting is the mask zero flag. So, your input sequences in an embedding layer, may or may not have different lengths. So, if you think about sentences of different length, and we can use the value zero as a special value that we can then mask out. So for instance, you start out with sequences of various length, and then you paart those sequences with zeros to make them of the same size. Only do them mask out the zero values."},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"from keras.layers.embeddings import Embedding\nEmbedding(input_dim, # Vocabulary size\n          output_dim, # output vector length\n          embeddings_initializer='uniform',\n          mask_zeros=False) # mask zero values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Okay. Our use case here is sentiment classification from movie reviews, which is a database of 25000 movie reviews from IMDB. Those are labeled either as good or bad. The data status is also visible through the Keras dataset module. And the good thing is that the data is already preprocessed sequences of integers. So, we don't actually have to work with the words vocabulary but those have already been marked as integers. Our task then is to classify our sentiment so good or bad from the review content. \n### And the way we are going to tackle this is we first embed our sentences with an embedding layer and then learn the sequential structure with an LSTM. So how do we do this? So we have a few imports of sequential model maxlen layer, embedding layer and LSTM and we also import our data set. So first off, we spent specifying the maximum number of features that goes to 20,000. So meaning, we only choose to model 20000 most common items from our vocabulary. \n### We specify a maximum length for our features. So, we only allow sequences of length 80 which would be quite short. With movie reviews, that's how we choose. So then we can simply load IMDB data into memory, by imposing IMDB low data with the maximum of features as specified to get back training and test data. "},{"metadata":{"trusted":false},"cell_type":"code","source":"from keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding\nfrom keras.layers import LSTM\nfrom keras.datasets import imdb\n\nmax_features = 20000\nmaxlen=80\n\n(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Alright. Next, we pad on sequences according to the maxlength we have specified. So that would mean if the sequence training and testing of data sets is shorter than 80 items, we would pad them with zeros at the end, and if they're longer, we crack them. Next, we initialize the sequential model and add on the embedding layer with the given maximum number of features and 128 equal dimension. \n### This then connects to LSTM layer with also 128 equal dimension, and we also specify 20 percent dropout rate for both regular and recurring weights. At very end, we are at the dense layer of the single output dimension and the sigmoid function which would specify a value between zero and one, zero meaning bad and one good.\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"x_train=sequence.pad_sequences(x_train, maxlen=maxlen)\nx_test=sequence.pad_sequences(x_test, maxlen=maxlen)\n\nmodel=Sequential()\nmodel.add(Embedding(max_features, 128))\nmodel.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### What's left is to compile them all which we again do with binary crossentropy, suppress the gradient and we also have the same accuracy as before. Then we set our model with batch size 32 and let it run for 15 epochs, and again we use our test service validation data. Afterwards, we can evaluate our model."},{"metadata":{"trusted":false},"cell_type":"code","source":"model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n\nmodel.fit(x_train, y_train, batch_size=32, epochs=15, validation_data=(x_test, y_test))\n\nmodel.evaluate(x_test, y_test, batch_size=32)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Beyond Sequuentials Model : the funciton API"},{"metadata":{},"cell_type":"markdown","source":"### In this video, we're going to talk about more general, non-sequential models in Keras using the functional API. So there is two types of models in Keras. We've seen the sequential already. There is another type called model, which you would use if you were in need of non-sequential models. Once defined, and we will see how to do that in just a second. The model can be trained and evaluated exactly like sequential models. So you don't have to learn anything about that part. Using the functional API, you would usually start with model and configuring one or several inputs. Once you have those inputs defined, we define transformations for those inputs, until you arrive at one or several outputs."},{"metadata":{},"cell_type":"markdown","source":"### his is an example we kind of have already seen using the sequential model. Mainly we do predictions of 100 predictions using amnest. We've got two layers here first. First one is Dense time layers which we have already seen and the second one is a new layer type called Input. On top of that, we import our new model class. As before, we define the number of classes as 10 because we have 10 digits here. And we define an input layer of shape 784, which is of vector shape length 784. Then what we do next is new, we define a Dense of output dimension 512, but instead of simply defining it, we call it on our inputs. So we define x as a Dense layer applied to our inputs. So what that means is, every layer in Keras, actually, every model for that purpose can be called in tensors to output tensors, and we do that here. So this is part of the functional API. We can do this step again and apply our intermediary x to a Dense layer again to arrive at yet another x.\n### Finally, we can do this a third time, this time with the number of classes as output dimension to arrive at our predictions. So this procedure of calling layers on different inputs, you can do that in any fashion, and slowly build up very complicated models."},{"metadata":{"trusted":false},"cell_type":"code","source":"from keras.layers import Input,Dense\nfrom keras.models import Model\n\nnum_classes=10\ninputs= Input(shape=(784,))\n\nx=Dense(512, activation='relu')(inputs)\nx=Dense(512, activation='relu')(x)\n\npredictions=Dense(num_classes, activation='softmax')(x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now, to initialize and run a model is quite simple, you simply specify the inputs and the outputs, that's it. Compilation step, fitting, evaluating and so on, it's just the same thing as before."},{"metadata":{"trusted":false},"cell_type":"code","source":"model=Model(inputs=inputs, outputs=predictions)\nmodel.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n\nmodel.fit (...) # same as before","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Saving and Loading Models"},{"metadata":{},"cell_type":"markdown","source":"### we're going to talk about Serializing and Deserializing Keras Models. Each Keras model, whether it's a sequential model or a more general on non sequential model, can be saved and loaded back again into memory. There's a few ways to do that. \n### And the first option is to serialize the full model as HDF5. But for model, we mean the architecture, meaning its configuration. All the weights as well as the training configuration and the state of the training configuration, which would allow you to, if you pick up the model again, continue training from where you left it of. You can also just persist the architecture as JSON or YAML, or you can just persist the weights as a HDF5"},{"metadata":{},"cell_type":"markdown","source":"### Okay, let's have look at the architecture or the weights separately. To save the adjacent configuration of a model is simply called model.to_json to arrive at the JSON string. If you want to rather have it serialized as a YAML format, you call model.to_yaml. If you want to save the weights, you simply code model.save_weights on the model with a specified path to your file. \n### If you want to load back your JSON configuration into a model architecture, you first import model from JSON from Keras model's sub module, and call model_from_json on your previously defined JSON string. If you want to load back the weights again, you do that by calling model.load_weights on the previously defined file path."},{"metadata":{"trusted":false},"cell_type":"code","source":"from keras.models import model_from_jason\n\n# Save Model as json and wights as HDF5\njason_string=model.to_json() # model.to_yaml\n\nmodel.save_weights('weights.h5')\n\n# load from json and set weights\n\nmodel=model_from_json (json_string)\nmodel.load_weights('weights.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### All right, if you want to persist the full model, that's actually a little easier, you simply call model.save on a path to an HDF5 file that you can choose on your own. And if you want to load it back again, you first import load_model, and then load the model with the specified file. That's pretty much it. This can be quite useful and we will see in the later lecture how to import serialized Keras models into."},{"metadata":{"trusted":false},"cell_type":"code","source":"from keras.models import load_model\n\nmodel.save('full_model.h5')\nmodel=load_model('full_model.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Introduction to Apache SystemML"},{"metadata":{},"cell_type":"markdown","source":"## SystemML"},{"metadata":{"trusted":false},"cell_type":"code","source":"!pip install --upgrade https://github.com/niketanpansare/future_of_data/raw/master/systemml-1.1.0-SNAPSHOT-python.tar.gz","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SystemML provides a R-like language called Declarative Machine Learning, or DML, for data scientists to implement machine learning algorithm. Here is a sample of DML script. In this script, we generate a random matrix X, with 1,000 cols and sparsity 0.5. The number of rows is provided by the user using the dollar parameter nr. We then multiply the matrix X with the transpose and compute the sum.\n### It is important to note that DML simply simplifies the development and deployment of the machine learning algorithms. It does so, by separating algorithm semantics from the underlying data representation and runtime execution plan.\n### DML is expressive enough to cover a broad class of algorithms. Such as Descriptive Statistics, Classification, Clustering, Regression, Matrix Factorization, Dimensionality Reduction, Survival Model, and Deep Learning.\n### These algorithms are pre-packaged with SystemML and available on GitHub. SystemML also supports commonly used data transformation task such as recording, demicording, bending, scaling, and missing values imputation.\n### SystemML has a cost less compiler that automatically generates hybrid runtime execution plans. That are composed of single node and distributed operations. These plans are generated depending on data and cluster characteristics, such as data size, data sparsity, cluster size and memory configuration.\n### DML Script, Language -> Compiler-> Run\n### n our previous example, if the user specifies a small linear, then SystemML might compile a single node plan. On the other hand, if linear is large, then SystemML might compile Hadoop or a Spark plan. The language component passes the DML script into a hierarchy of statement blocks and statements. It also performs syntactic analysis, live variable analysis, and semantic validation. We then construct a directed cyclic graph of high-level operators, called as HOPs, per statement.\n### SystemML optimizer performs various optimizations on this HOP DAGs. For example, algebraic simplifications rewrite, interprocedural analysis, and matrix multiplication chain optimization. Each HOP DAG is compiled into a dug up low level operators called a slopes, based on memory estimation, data and cluster characteristics.\n### Loops are back in specific operator and have corresponding real-time implementation called instructions. SystemML runs in an embeddable, standalone, and in cluster mode. It also supports various APIs in Scala, Python, and Java."},{"metadata":{},"cell_type":"markdown","source":"## How to perform a matrix multiplication X and Y in DML ?\n### X %*% Y, Correct answer. DML follows R-syntax for matrix multiplication."},{"metadata":{"trusted":false},"cell_type":"code","source":"!pip install systemml","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Which of the following statements is true ?\n### SystemML provides an API called MLContext (in Scala, Java, and Python), that allows the user to register RDDs and DataFrames (previously created through Spark SQL or other libraries) as input and output variables of a DML script.\n### SystemML allows for algorithm reusability across data-parallel frameworks (such as Hadoop, Spark) and simplified deployment for varying data characteristics and runtime environments."},{"metadata":{},"cell_type":"markdown","source":"### To invoke SystemML from command line, the user has to provide the location of the ML script via -f option. The command line and argument such as dollar sign And now, can be parsed via -nvargs option. The user can also specify the execution mode using -exec option. In most cases, you will end up using hybrid_spark mode or hybrid mode\n### Let's use the Linear Regression as an example and consider three cases: In the first case, the data resides in on HDFS or local file system. In the second case, we'll assume that SystemML is part the whole spark workflow and data is available as RDD or DataFrame. In the third case, we'll assume that SystemML is called by other Java program for scoring. Hence, the data is available as Java double array. \n### The Linear Regression given here and available via this URL takes at least three parameters: X, Y and B. X refers to the location of input features, Y refers to the location of the response variables, and B refers to the location where SystemML should store the estimated regression coefficient. \n### he first case can be further divided into three subcases. The first subcase, the data resides on local file system and the user wants to use single node backedn. In the second subcase, the data resides on HDFS, and the user wants to use hadoop backend. In the third subcase, the data resized on HDFS again, and the user wants to use spark backend. \n###  Let's examine the first subcase. The other user invoke SystemML using Java by providing the DMLScript, using -f option. And the named arguments X, Y and B, are provided using -nvargs option. It is important to note that the second command preserves algebraic rewrites, but now also spawn local MR jobs. If you want to run the same Linear Regression script without a single line of change on a hadoop, you'll invoke it with hadoop jar command.\n### The argument will remain exactly the same except in this case you'll provide hybrid as the execution mode. If you want to run the Linear Regression script on spark cluster, you will use spark-submit command. In this case, you may not provide -exec option, as per SystemML will infer that you are running on spark cluster. \n### Now let's examine the second case. Where SystemML is part of a spark workflow and data is available as an RDD or a DataFrame. Here, you can use the MLContext API that offers a programmer tech interface for interacting with SystemML from spark using languages such as Scala, Java, and Python. MLContext API allows the users to registers RDD's and DataFrame as input and output variables of a DMLScript. This enables SystemML to seamlessly integrate into a entire spark ecosystem. A MLContext object can be created by parsing its constructor, a reference of spark context or spark session\n### This ScripFactory class allows a DMLScript to be created from strings, files, URL's and input strings. Here, we will use DML from file method to create the script object. The input RDD X can be part using the inner input command. Finally, we execute the script using the execute method of MLContext object. The Python MLContext is similar to the Scala MLContext discribed earlier.\n### SystemML's Python package is available on PyPi and can be installed using \"pip\" command. MLLearn API allows a Python programmer to invoke SystemML's algorithm using a scikit-learn like API or Spark's ML pipeline API. Hence, the input data can be a NumPy array, Scipy array, a Panda DataFrame, or a Spark DataFrame. Since these API conforms to the ML pipelines estimator interface, they can be used in tandem with MLLearn's feature extractors, transformers, coding and cross validation classes.\n### he use of force created Linear Regression object given here, and then invokes fit and predict method. This shows the ML pipeline-like API where the input to the fit method is the DataFrame, and the next shows the scikit-like API where the input to the fit method are two NumPy arrays. \n### There are three different ways to implement a deep learning modeling system. Using a DML bodied ML library, using the experimental Caffe2DML API, and using the experimental Keras2DML API. Keras2DML API and Caffe2DML API are instances of the MLLearn library we discussed earlier. Hence, they have fit and predict method and can take a Numpy array or a Spark DataFrame as input. Underneath, Keras2DML API takes a keras_model and generates an equivalent DML script. \n### Similarly, Caffe2DML takes, as an input, a deep learning model expressed in caffe format and generates underneath the equivalent DML script. We will skip the JMLC's coding API in this video."},{"metadata":{},"cell_type":"markdown","source":"## How to use APache SystemML on IBM DSX"},{"metadata":{},"cell_type":"markdown","source":"### I will explain how to use SystemML. We'll walk through full example program that will explain how to write a DMS script and use SystemML's Python API. First, we'll understand how to implement a simple, Hello World and matrix multiplication example using DML. The goal of this example is to understand how to write a simple DML script and use ML Context API. Then we'll move on to a more complicated example where we will understand how to implement three different algorithms to train a linear aggression model. \n### he goal here is to understand how System ML enables data scientist to implement that custom algorithm, using for this specific uses. The example, three and four explains, how a data scientist who prefers to invoke a pre-implemented algorithm, can use SystemML. Finally, we understand how to drain a simple neural network, using kiosk to DML API. "},{"metadata":{"trusted":false},"cell_type":"code","source":"!pip show systemml","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# import systemml API\nsc.version","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### In this cell, we import MLContext class and DML method. From this system a mini package we create an American text object by passing either a spark context or a spark session object. Next, we check the version of SystemML using American text information.\n### We create a simple DML script, that brings hello word. Notice DML script object is created by passing the DMS strength, to the DML material being reported earlier. We then execute the script. Using American text, execute matter. Let's say that instead of using system mms building script print method. We want to use Python's print method. In this case, we create a string in DML. And specify that we want to fetch that string as an output. Then we use American text get method after execution."},{"metadata":{"trusted":false},"cell_type":"code","source":"# MLCOntext Class and DML Method\nfrom systemml import MLContext, dml\n# Create MLContext object\nml=MLContext(sc)\n# print information of ssytem version\nprint (ml.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Create DML Script for a 'Hello World', example and execute it using MLContext\nscript=dml(\"\"\"\nprint ('Hello World');\n\"\"\")\nml.execute(script)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Lets modify the above script to get the Hello World String\nscript=dml(\"\"\"\ns=\"Hello World\"\n\"\"\").output(\"s\")\n\nhello_world_string = ml.execute(script).get(\"s\")\nprint (hello_world_string)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's load numpy and sklearn which we'll use in later examples."},{"metadata":{"trusted":false},"cell_type":"code","source":"import numpy as np\nimport sys,os\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nplt.switch_backend('agg')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now that we know how to execute a DMS script and fetch the output using ML context, let's understand how to pass input to SystemML using ML context input method. Four, we can yield a random matrix X with a thousand columns and sparsity .5. The number of throughs are passed by the user by the dollar parameter [INAUDIBLE]. We then multiply the matrix X with the transpose in computer sum.\n### The pass the value of the dollar parameter enough, in this case, 1e raised to six. Using the input method, we now execute the script using American text. As you can see, it spawned a spark job likely to generate a random matrix. When this job is finished, the value of s will be greater."},{"metadata":{},"cell_type":"markdown","source":"### Example 1 Matrix Multiplication"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Systemml script to generate a random matrix, perform matrix multiplication and compute sume of the output\nscript=\"\"\"\nX = rand(rows=$nr, cols=1000, sparsity=0.5)\nA=t(X)%*%X\ns=sum(A)\n\"\"\"\n\nprog=dml(script).input('$nr',1e6).output('s')\ns=ml.execute(prog).get('s')\nprint s","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Load Diabetes dataset from sklearn"},{"metadata":{},"cell_type":"markdown","source":"### Let's load the diabetes dataset from scikit-learn. To visualize the dataset, we use Macro List scatter plot. The training data points are colored black, and the test data points are colored red. The x-axis denotes the input features, and the y-axis denotes the response. As you can see, the response values are between 0 and 400. Our goal here is to train a linear regression model which attempts to find a line somewhere here. Which we can then use to predict the response value which is this given the input features. In data cell, we'll denote the slope of this line by the V W and the intercept with biased B"},{"metadata":{"trusted":false},"cell_type":"code","source":"%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"diabetes=datasets.load_diabetes()\ndiabetes_x=diabetes.data[:, np.newaxis,2]\ndiabetes_x_train= diabetes_x[:-20]\ndiabetes_x_test = diabetes_x[-20:]\ndiabetes_y_train = np.matrix(diabetes.target[:-20]).T\ndiabetes_y_test = np.matrix(diabetes.target[-20:]).T\n\nplt.scatter(diabetes_x_train, diabetes_y_train, color='black')\nplt.scatter(diabetes_x_test, diabetes_y_test, color='red')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"diabetes_y_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Before we dive into the linear regression code, let's look at a couple of preliminaries. The built-in function solve computes the least squares solution, Ax = b, such that the norm, Ax- b, is minimized. It is important to note that this function can operate only on small to medium sized input matrix. Linear regression model assumes that the relationship between input feature and the response variable is linear. The goal, then, is to estimate the regression coefficient w. We use the square loss, given here differentiating the loss with respect to w, we get this expression. We set this expression to 0 and we arrive at the expression given here. The expression here can be computed using solve built in function, where A is x is transpose x and b is x transpose y."},{"metadata":{"trusted":false},"cell_type":"code","source":"script=\"\"\"\n# add constant feature to x to model intercept\nones=matrix(1, rows=nrow(X), cols=1)\nX=cbind(X,ones)\nA=t(X) %*% X\nb=t(X) %*% y\nw=solve(A,b)\nbias=as.scalar(w[nrow(w), 1])\nw=w[1:nrow(w)-1,]\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### For training, we need to pass the values for the variables x and y using the input method. \n### n this case, x is the NumPy matrix, diabetes_x_train, and y is the NumPy matrix, diabetes_y_train. Since we want to find the line with the slope w and intercept bias, we mark the WM bias as output and we get democratic seclusion.\n### Now that we have WM bias let's plot the line on our scikit scatter plot which is this."},{"metadata":{"trusted":false},"cell_type":"code","source":"prog=dml(script).input(X=diabetes_x_train, y=diabetes_y_train).output('w', 'bias')\nw,bias=ml.execute(prog).get('w','bias')\nw=w.toNumPy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.scatter(diabetes_x_train, diabetes_y_train, color='black')\nplt.scatter(diabetes_x_test, diabetes_y_test, color='red')\nplt.plot(diabetes_x_test, (w*diabetes_x_test)+bias, color='blue', linestyle='dotted')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Algorithm 2: Linear Regression - batch Gradient Descent (no regularization)"},{"metadata":{},"cell_type":"markdown","source":"# Algorithm\n### Step1: Start with an initial point \n### while (not converged){\n###    Step 2: compute gradient dw\n###    Step3: Compute stepsize alpha\n###    Step 4: Update: w_new=w_old- alpha*dw\n### }"},{"metadata":{},"cell_type":"markdown","source":"### A drawback of direct sole method is that the inputs to the sole building function should be small enough. That is, both X transpose X, and X transpose Y should fit in the driver here. Hence, this method does not work for larger dataset. To get around this problem, we'll implement two iterative algorithm, that's key to a larger data set: Batch Gradient Descent and Conjugate Gradient Method. Batch Gradient Descent is an extremely simple algorithm.\n### Assume that you are somewhere on the hill, and you want to reach to the bottom of the hill. Unfortunately, you don't have a GPS but you have a device that tells you slope at a given point. For higher dimensional surfaces, the slope is called as a gradient. In other word, gradient is just another name for derivative of a function and is a vector that points in the direction of the greatest increase of the function. \n### Therefore, to reach the bottom of the hill, all one has to do is compute the gradient at the current location and take a step in the opposite direction of the gradient. Then, compute the gradient at that location and take a step in the opposite direction of that gradient, and so on and so forth. \n### It will go out quickly, it is represented here. Step one, start with an initial point W. Continue until not converged. Step two, compute gradient DW. Step three, compute step size alpha. Step four, compute new W by subtracting alpha times DW from old W.\n### As shown in the figure, if the surface has multiple local minimum, different initial point can lead as two different minimum."},{"metadata":{},"cell_type":"markdown","source":"### As discussed earlier, the gradient for the least cost function is X transpose X times W minus X transpose Y. To compute step size, one has to perform a line search along the gradient. I will leave the derivation of this expression as homework. "},{"metadata":{},"cell_type":"markdown","source":"### The DML script for the batch gradient descent is given here. We initialize the starting point W to zero, then we take exactly 100 steps. In each step, we compute the gradient DW, and the step size alpha using the above formula. Then, we compute the new W by subtracting alpha times DW from old W. Again, as in previous example, we plot the line from the line W. It's given here. "},{"metadata":{"trusted":false},"cell_type":"code","source":"script=\"\"\"\n# add constant feature to x to model intercept\nones=matrix(1, rows=nrow(X), cols=1)\nX=cbind(X,ones)\nmax_iter = 100\nw=matrix(0, rows=ncol(X), cols=1)\nfor (i in 1:max_iter){\nXtX= t(X) %*% X\ndw= XtX %*% w - t(X)%*%y\nalpha= (t(dw) %*% dw /(t(dw)) %*% XtX %*% dw)\nw=w- dw * alpha\n}\nbias=as.scalar(w[nrow(w), 1])\nw=w[1:nrow(w)-1,]\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"prog=dml(script).input(X=diabetes_x_train, y=diabetes_y_train).output('w').output ('bias')\nw,bias=ml.execute(prog).get('w','bias')\nw=w.toNumPy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.scatter(diabetes_x_train, diabetes_y_train, color='black')\nplt.scatter(diabetes_x_test, diabetes_y_test, color='red')\nplt.plot(diabetes_x_test, (w*diabetes_x_test)+bias, color='blue', linestyle='dashed')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Algorithm 3: Linear Regression- Conjugate gradient (no regularization)\n### Problem with gradient descent: Take very similar directions many times"},{"metadata":{},"cell_type":"markdown","source":"# Algorithm\n### Step1: Start with an initial point \n### while (not converged){\n###    Step 2: compute gradient dw\n###    Step3: Compute stepsize alpha\n###    Step 4: Compute next direction p by enforcing conjugacy with previous direction\n###   Step 4: Update: w_new=w_old- alpha*dw\n### }"},{"metadata":{},"cell_type":"markdown","source":"### Next, we'll look at conjugate gradient method. This method benefits from using conjugacy information during optimization and usually requires far fewer steps. To converge, lets compact two batch gradient descent. The exact algorithm is given here. I'll skip the details of the algorithm and refer you to the key gradient. So, the exact algorithm is given here. And here is the DML script for conjugate gradient method. Like previous method, we'll plot the learned line. If you prefer not to write the custom algorithm, but instead invoke standard of the shelf algorithm, then you can use by ten cord given in example three and example four. "},{"metadata":{"trusted":false},"cell_type":"code","source":"script=\"\"\"\n# add constant feature to x to model intercept\nX=cbind(X, matrix(1, rows=nrow(X), cols=1))\nm= ncol(X); i=1;\nmax_iter=20;\nw=matrix(0, rows=m,cols=1); # initialize wieght to 0\ndw=-t(X)%*%y; p=-dw; # dw=(XtX)w -(Xty)\nnorm_r2=sum(dw^2);\nfor (i in 1:max_iter){\nq=t(X) %*% (X%*%p)\nalpha=norm_r2/sum(p*q); Minimizes f(w-alpha*r)\nw=w+alpha*p;   # update weights\ndw=dw+alpha*q;\nold_norm_r2=norm_r2; norm_r2=sum(dw^2);\np=-dw + (norm_r2/old_norm_r2) * p;  # next direction - conjugacy  to previous direction\ni=i+1;\n}\nbias=as.scalar(w[nrow(w), 1])\nw=w[1:nrow(w)-1,]\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"prog=dml(script).input(X=diabetes_x_train, y=diabetes_y_train).output('w').output ('bias')\nw,bias=ml.execute(prog).get('w','bias')\nw=w.toNumPy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.scatter(diabetes_x_train, diabetes_y_train, color='black')\nplt.scatter(diabetes_x_test, diabetes_y_test, color='red')\nplt.plot(diabetes_x_test, (w*diabetes_x_test)+bias, color='blue', linestyle='dashed')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Example 3: Invoking existing SystemMl algorithm script LinearRegDs.dml using ML context API"},{"metadata":{},"cell_type":"markdown","source":"### In example three, we invoke, we implemented algorithm by using dmlFromResource method and ML context object. The pre-implemented algorithms have a label under script folder in our GitHub. All one has to do is create a script object from dmlFromResource, pass it the input feature and the response variable, that is X and Y using the input method."},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"from systemml import dmlFromResource\nprog=dmlFromResource('scripts/algorithms/LinearRegDS.dml').input (X=diabetes_x_train, y=diabetes_y_train).input ('$icpt',1.0).output('beta_output')\nw=ml.execute(prog).get('beta_out')\nw=w.tonumPy()\nbias=w[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.scatter(diabetes_x_train, diabetes_y_train, color='black')\nplt.scatter(diabetes_x_test, diabetes_y_test, color='red')\nplt.plot(diabetes_x_test, (w[0]*diabetes_x_test)+bias, color='blue', linestyle='dashed')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Example 4: Invoking existing SystemMl algorithm using Scikit-learn/SparkML pipeline like API"},{"metadata":{},"cell_type":"markdown","source":"### xample four is targeted for a scikit-learn user. A scikit-learn user may want to only create a linear regression object and call the fit method. The fit method accepts the input features and response variable as Numpy matrices. That user can simply use our mllearn API, mllearn API allows a Python programmer to invoke systemML'S algorithm using a scikit-learn like API, where the input data can be Numpy Arrays, scifi matrices, all Panda data frame as well as Spark's MLPipeline API where the input data is a Spark data frame. Since these APIs conform to MLPipeline's estimator interface, they can be used in tandem with ML as feature extractors, transformers, scoring, and cross validation classes. "},{"metadata":{"trusted":false},"cell_type":"code","source":"from pyspark.sql import SQLContext\nfrom systemml.mllearn import LinearRegression\nsqlctx=SQLContext(sc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"regr=LinearRegression(sqlctx)\n# Train the model using training set\nregr.fit(diabetes_x_train, diabetes_y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"predictions=regr.predict(diabetes_x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# use train model to perform predictions\n%matplotlib inline\nplt.scatter(diabetes_x_train, diabetes_y_train, color='black')\nplt.scatter(diabetes_x_test, diabetes_y_test, color='red')\nplt.plot(diabetes_x_test, predictions,  color='black')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### (Optional) Install OpenBlas"},{"metadata":{"trusted":false},"cell_type":"code","source":"!wget https://github.com/xianyi/OpenBLAS/archive/v0.2.20.tar.gz\n!tar -xzf v0.2.20.tar.gz\n!cd OpenBLAS-0.2.20/ && make clean\n!cd OpenBLAS-0.2.20/ && make USE_OPENMP=1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Example: Invoking a Keras model with SystemMl"},{"metadata":{},"cell_type":"markdown","source":"### Next, we see how to use Keras inside SystemML. There are three different ways to implement a deep learning model in SystemML using the DML bodied ML library, using the experimental Caffe2DML API, and using the experimental Keras2DML API. Keras2DML and Caffe2DML accept the deep planning model expressed think Keras or Caffe former and then underneath genre T because learning DML script. In this example, we train a LeNet network using MNIST dataset. We first load the MNIST dataset, then we create LeNet using Keras API. Keras model given you has two convolution layers with ReLU activation and same padding with MaxPooling layer in between. They are followed by two densely connected layers with dropout. Once we have created the Keras model, we can pass it to SystemML using Keras2DML class. We can then call fit or predict method other than mllearn API."},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"from mlxtend.data import mnist_data\nimport numpy as np\nfrom sklearn.utils import shuffle\n# Download the Mnist datasets\nx,y = mnist_data()\nx,y = shuffle(x,y)\n# split the data into training and test\nn_samples = len(x)\nx_train=x[:int(-9 * n_samples)]\ny_train=y[:int(-9 * n_samples)]\nx_test=x[:int(-9 * n_samples):]\ny_test=y[:int(-9 * n_samples):]\n\nfrom keras.model import Sequential\nfrom keras.layers import Input, Dense, Conv2D, Maxpooling2D, Dropout, Flatten\nfrom keras import backend as K\nfrom keras.models import Model\n\ninput_shape = (1,28,28) if K.image_data_format() == 'channel_first' else (28,28,1)\nkeras_model=Sequential()\nkeras_model.add=(Conv2D(32, kernel_size=(5,5), activation='relu', input_shape=input_shape, padding='same'))\nkeras_model.add=(Maxpooling2D(pool_size=(2,2)))\nkeras_model.add=(Conv2D(64, (5,5), activation='relu',padding='same'))\nkeras_model.add=(Maxpooling2D(pool_size=(2,2)))\nkeras_model.add=(Flatten())\nkeras_model.add=(Dense(512, activation='relu'))\nkeras_model.add=(Dropout(0.5))\nkeras_model.add=(Dense(10, activation='softmax'))\n                       \n# Scale the input featur\nscale=0.00390625\nx_train=x_train*scale\nx_test=x_test*scale\n                       \n                       ","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"from systemml.mllearn import Keras2DML\nsysml_model=Keras2DML(spark, keras_model, input_shape=(1,28,28), weights='weights_dir')\nsysml_model.setConfigProperty('sysml.native.bias', 'openbias')\nsysml_model.setConfigProperty('sysml.native.bias.directory', os.path.join(os.getcwd(), 'OpenBLAS-0.2.20/'))\n# sysml_model.setGPU(True).setForceGPU(True)\nsysml_model.summary()\nsysml_model.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## QUIZ Apache SystemML"},{"metadata":{},"cell_type":"markdown","source":"## Which execution environments are supported by SystemML?\n### TensorFlow Backend, Single Node Java Virtual Machine, ApacheSpark, ApacheHadoop, GPU\n## Which programming language environments are supported by SystemML?\n### Jave, scala, python\n## What options exist to implement a DeepLearning model in SystemML?\n### Using the Caffe2DML importer, Using the Keras2DML importer, Using the DeepLearning library available in DML script\n## What's the value of c after computation completes?\n### 32\n## What is the advantage of the SystemML code over the pure numpy code?\n### Using SystemML the computation gets pushed to the SystemML backend which in this case makes use of the parallel execution framework of ApacheSpark whereas numpy runs on a single CPU only\n## What's are the most important similarities between SystemML and Tensorflow?\n### Both are a linear algebra computing framework using a declarative approach of expressing computations and shipping those to to execution engine, Both are OpenSource and totally free of charge, Both support execution on top of ApacheSpark natively, "},{"metadata":{},"cell_type":"markdown","source":"# Introduction to DeepLearning4J"},{"metadata":{},"cell_type":"markdown","source":"## Introduction to DeepLearning"},{"metadata":{},"cell_type":"markdown","source":"### In this section I'm going to provide an overview of the tools that the DeepLearning4J project provides. Any code that I write for this Coursera course will be shared in this Git repo. \n### github.com/SkymindIO/dsx\n### DeepLearning4J does is it provides a toolkit for using DeepLearning on the JVM. It consists of a number of subprojects. A large part of our work as data scientists is spent in terms of ingesting, processing, preprocessing, normalizing, standardizing, and otherwise manipulating our data source.\n### And our data source might be a comma separated value file, might be a collection of images, or video, or audio files, and DataVec is our subproject in DL4J that provides tools for ETL.\n### The processing that takes place when a neural network trains is numeric processing of arrays. So it's matrix to matrix multiplication, and ND4J handles that.You can kind of think of ND4J as the equivalent of what NumPY has for Python. You can think of it as NumPY for the JVM.\n### Since we're doing matrix to matrix multiplication, so libnd4j provides the native libraries for execution on GPUs and or CPUs. And then DeepLearning4J, this is where we define our neural net, configure it, and then train it. \n### DataVec needs to get your data into a numeric array, sometimes called a tensor. Sometimes perhaps a more appropriate term would be an indexed n-dimensional array, a multi-dimensional array of values. And DataVec helps you get from your data source into that numeric array. Your data source might be log files, text documents, voice samples, tabular data, images, video, and more. \n### Some of the features that DataVec provides: transformation. I may need to transform a list of classes or a numeric representation of classes to a one-hot representation. I may need to join data sets. I may need to transform values. I may need to reorganize the data into another schema. I will need to scale my data, perhaps between values of zero and one so that we have consistent ranges of values. So normalizing and standardizing DataVec provides tools for that. \n### A neural network trains best if the data is shuffled, so DataVec provides tools to assist with shuffling the data at many points along the pipeline, and then splitting our data into test and train. \n### n order to train a neural net if we're doing supervised learning, we need some way to get the label of the data. And the label might be stored as part of the file path. So we might have a directory of images of cats and a directory named Cats. We would extract that. And collection of images of dogs might be in a directory named Dogs. So some tools for doing that. If the labels in the path, perhaps the name of the file, we can use Path Label Generator. If the label is in the parent path, like I described, the directory of cats and dogs, then we could use Parent Path Label Generator.\n###  If the label's stored as a column in your CSV data, then we provide the labelindex to the RecordReaderDataSetIterator.So some commonly used features in DataVec, a RecordReader to read our files or input, converting it to a list of writables. So we'll pass our record reader in input split which says where in the file system and that file system could be HDFS or S3, any Java interpretable file path.\n### Normalizing our data, standardize, scale, transform processes to modify the schema of the data, join datasets to replace strings, extract labels.\n### So here's a quick diagram of some of the available ETL paths. There's more, but there you see some of the classes or some of the tools that you would use depending upon our data source, where the label is, what type of RecordReader we're going to use to read it depending upon how the data is stored. And down here you'll see where we convert to an INDArray. That's our RecordReaderDataSetIterator. \n### It takes what the RecordReader provides, a list of writables, a list of records you can think of that, and converts that into a multi-dimensional array of features, and then if they're doing supervised learning, an additional multi-dimensional array of labels.\n### Table of available Record Readers: deeplearning4j.org/etl-userguide\n### You may need to preprocess your data. So an example would be images where the pixel values might be 0 to 255 to determine the range of a color in that pixel. To process that data in a neural net you may want to scale those two values between zero and one. And we could use MinMax scaling. We could use NormalizerMinMaxScaler where we find the observed men and the observed maps. I\n### In the case of the images we know what the potential Max and the potential Min is, but your data might not necessarily be images and you might need to extract the observed Max and the observed Min and then apply those and set the observed Max to one and the observed Min to zero.\n### NormalizerStandardize, NormalizerMinMaxScaler needs to read through the whole data set to extract them in an max, the global min and max. NormalizerStandardizer prevents that initial pass by providing a moving column wise variance and mean, thereby eliminating the need to preprocess the data. \n### So CSVRecordReader is commonly used if we have CSV data and we'll have an example of that. We'll also have an example in this course of CSV sequence RecordReader where we're generating a time series structure out of the data that's stored, image RecordReader if you are reading images, and there's examples of that in the GitHub DeepLearning4J, GitHub repo providing examples\n###  JacksonRecordReader if I was reading JSON parent path label generator quite commonly used. Transform, TransformProcess, Transform Process Builder always to choose which columns you'd like to use. Perhaps perform computation transformation of those columns.\n## What RecordReader class would be used to read Comma Separated Values from a Text File\n### CSVRecordReader\n## What tools allow for extracting Label from the File Path\n### ParentPathLabelGeneratorm , PathlabelGenerator\n### ND4J for is our numeric scientific computing libraries. One of its main features is a versatile and dimensional array object. So we'll be creating indexed and dimensional arrays and then our neural net will be processing those and generating its output which will also be an indexed and dimensional array. \n### Multi-platforms functionality and support including GPUs. Neural nets take a lot of computing power, though perform significantly faster on graphic processing units or GPUs and in order to switch from CPU to GPU and DL4J it's as easy as changing a configuration of what the ND4J backend is.\n### So it's a simple change of your POM file and then an execute on GPUs rather than CPUs. And the tools we're frequently using from ND4J, DataSet, and a DataSet is a collection of INDArrays, one for the features and one for the labels, and then DataSetIterator, that RecordReader DataSetIterator is where we move from DataVec parsing, processing, configuring our input into generating the INDArray. So RecordReader DataSetIterator for processing that data and getting it into ND4J to pass it to our neural net.\n### Libnd4j this is a C++ engine that powers ND4J. We need the speed and the native processing support of C++ and libnd4j provides that for us. DeepLearning4J, this is where we built our neural nets and we can configure our neural nets to execute on CPUs or GPUs by changing that line in our POM file. \n### We can specify we want standalone or parallel processing so you can build a simple neural net that runs locally on CPUs, switch our POM file and it's executing on GPUs. \n### Wrap that neural net in ParallelWrapper or SparkDl4JMultilayer and it will now execute in parallel across a collection of Spark nodes on your Spark cluster. And that's one of the focuses of this course. We're going to demonstrate deploying your code or neural nets code onto the Spark cluster that IBM's data science experience provides. And then ParallelWrapper. \n### f we had a collection of GPUs on a single machine we need to do parallel processing, across those GPUs we would use ParallelWrapper.\n### Github: github.com/deeplearning4j/\n### Gitter Chat: gitter.im/deeplearning4j/deeplearning4j\n### Web: deeplearning4j.org"},{"metadata":{},"cell_type":"markdown","source":"## Demo: Running Java in DataScience Experience"},{"metadata":{},"cell_type":"markdown","source":"### I am going to demonstrate some of the skills that you will need to execute Java code, particularly deep learning for J Neural network code in the data science experience.\n### \n### github.com/SkymindIO/dsx\n### Check current directory through \"ls\" command"},{"metadata":{"trusted":false},"cell_type":"code","source":"ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"!wget https://github.com/SkymindIO/dsx/releases/download/1.0/dl4j-quickstart-1.0-SNAPSHOT-jar-with-dependencies.jar","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"!wget https://raw.githubusercontent.com/SkymindIO/dsx/master/iris.txt","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"!ls *jar","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"!ls iris.tx*","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### One way to execute it is just use the local Java. If I type java version, bang java version, exclamation point java version, we see we have Java locally. And then, to execute that, I would just type \"I need to specify the class path to include the jar,\" and then \"I need to specify the class that I want to execute in that jar."},{"metadata":{"trusted":false},"cell_type":"code","source":"!java -version","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"!java -cp dl4j-quickstart-1.0-SNAPSHOT-jar-with-dependencies.jar skymind.dsx.IrisClassifier","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### From above execution\n### We have seen that So, if I execute that code, we will see the output of that. It's going to take a little bit of time, but we see it's building the model, and then it will go through many iterations on that model. As the model finishes, I will see it goes through thousand iterations. We see the score is generally decreasing, getting towards zero, meaning the model's getting better at making predictions. And when the model's done, we operate an evaluation of the true table, how accurate we were. So, Class zero, we got correct. Class one, we got correct 22 times. Class one, we got incorrect two times. And class two, we got correct 10 times. So, there you have executing the model using the local Java.\n## The java code that is executed in this example is packaged in a jar file. In order for the code to execute what else is needed in that jar besides our target class?\n### Dependencies, the jar must include dependencies\n"},{"metadata":{},"cell_type":"markdown","source":"### I could also submit the jar and my data file to Spark. I would do that using this command. So, Spark homes an environmental variable that points to the location of the spark-submit executable. So, I start with that, and once again, I start to command with an exclamation point. Spark-submit allows you to specify which class you're going to execute, so I specify that here. \n### When I'm submitting a job to Spark, I need to specify the master, and once again, the data science experience has an environmental variable set named Master that points to the URL of the current Spark Master. We need to ship it the file iris.txt and we can do that using the files command.\n### And then, the last argument, we need to include the jar. So, this command will say, \"Ship this jar up to this Spark master, execute this class.\" That class will be looking for this file, iris.txt, and that command, --files, ships it up and makes it available to each worker. \n### Note that here, we're really just using Spark as an execution environment. We're not building a Spark context, doing fully distributed work. We're just shipping a single class to execute once, so we're not taking full advantage of Spark, but we are executing our code in Spark. \n## In the notebook the following command is executed\n### MASTER is an environment variable and will be replaced with the url of the current master when the command executes"},{"metadata":{"trusted":false},"cell_type":"code","source":"!$SPARK_HOME/bin/spark-submit \\\n--class skymind.dsx.IrisClassifier \\\n--master $MASTER \\\n--files iris.txt \\\ndl4j-quickstart-1.0-SNAPSHOT-jar-with-dependencies.jar","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  From above Execution\n### And there you see the output, as it begins to run. As that code finishes, you'll see the same thing. You'll see the score at the final iteration, you'll see the truth table and there you have it. So, we can execute our Java code in two ways. In both cases, we need to build a Hoover Jar, a jar with dependencies, ship the jar and any needed text files into the data science experience.\n### Once we have that content in the data science experience, we can either execute Java, ship it to class path of the jar, specify the class you want to execute and execute it locally in the data science experience, or we can ship the jar and specify the class to execute when we ship that to spark-submit. "},{"metadata":{},"cell_type":"markdown","source":"## DL4J Neural Network Code Example, Mnist Classifier "},{"metadata":{},"cell_type":"markdown","source":"### 28by 28 gray scale images = 784 neurons , 60000 test images, 10000 training images, hand written digits 0-9, means labels 0-9\n### Not a Convolution Network, value of each pixel  = Feature,  28x28=784 features, training the neural net to predict betterdigits\n### FeedForward Network Diagram: input layer=features=784 neurons, Hidden layer=may be 1000 features more than the input and output layer = 10 o/p as 10 digits to predict\n### github.com/deeplearning4j//dl4j-examples\n### Dozen of Working Examples of java based Neural Networks\n### github.com/deeplearning4j//dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/feedforward/mnist/MLPMnistSingleLayerExample.java\n## In a Feed Forward Neural network the number of Nodes in the Input Layer is equal to\n### The number of features in the input, in this case 784\n### Standard Steps FOr All Neural Networks: Read the data, Extract Feartures & labels, Crerate a dataset Object to pass into our Neural Network ( Basically we need to format our data as tensor or IND array that contains features and another one contains labels and convert them to numeric values, Build the neural net and then train it\n### ND4j reading the data through: DataVec (a collection of tools), Provide RecordReader, CSVRecordReader (for csv files), imageRecordReader for images, SequenceRecordReader for time series (sequience of events)\n### deeplearning4j.org/etl-userguide\n### Extract Feature & Label:  For csv file, first 7 digits will be fearture and the last 3 digits will be labels, which can extracted through LabelIndex, and if we classfy images and those are stored in directory, we will use PathLabel generator\n### Now you have to convert the data (which is in numeric) into tensor or IND array, we use RecordReaderDataSetIterator, take data from DataVec and build IND array\n### Build a Neural Network: Two ways, MultilayerNetwork or Computation  Graph. MultilayerNetwork includes feedforward, recurrent,convolitional neural nets. If you use combination or multitasking you use Commuptation Graph\n### Train: we use model to fit\n### Documentation: deeplearning4j.org/documentation\n### DataVec JavaDoc: deeplearning4j.org/datavecdoc\n### Deeplearning4j JavaDoc: deeplearning4j.org/doc\n"},{"metadata":{},"cell_type":"markdown","source":"### Example: MLPMnistSingleLayerExample.jave, Classification of mnist digits using Feedforward NeuralNetwork"},{"metadata":{"collapsed":true,"trusted":false},"cell_type":"code","source":"# number of rows and columns in the input images\nfinal int numRows=28;\nfinal int numCols=28;\nint outputNum=10; # number of output classes\nint batchsize=128; # bacthsize for each epoch\nint rngseed=123; # random number seed for reproducibility\nint numEpochs=15; #  number of epochs to perform (how many total passes of the data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Get the Data\nDatasetIterator mnistTrain=new MnistDatasetIterator(batchsize,True)\nDatasetIterator mnistTest=new MnistDatasetIterator(batchsize,False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Configuration the Network using global parameters\nMultilayerConfiguration conf = new NeuralNetConfiguration.Builder().seed(rngseed) \\\n.optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).iterations(1) \\\n.learningRate(0.006).update(Updater.NesTEROVS).momentum(0.9)\\\n.regularization(true).12(1e-4).list()\\\n.layer(0,new Denselayer.Builder().nIn(numRows * numColumns).nOut(1000) \\\n.activation(Activation.RELU).weightInit(WrightInit.XAVIER).build()) \\\n.layer(1, new Outputlayer.Builder (LossFunction.NEGATIVELOGLIKELIHOOD) \\\n.nIn(1000).nOut(outputNum).activation(Activation.SOFTMAX)\n.weightInit(WeightInit.XAVIER).build()) \\\n.pretrain(false).backprop(true).build();\n\n# Build Model from configuartion\nMultilayerNetwork Model  = new MultilayerNetwork(conf);\nmodel.init();\n\n# add a listener to print progress after 1 iterations before train\nmodel.setListeners (new ScoreIterationsListener(1));\n\n# train the model\nlog.info(\"Train model ... \");\n for (int i=0; i<numEpochs; i++) {\n     model.fit (mnistTrain);\n     \n }\n\n# Evaluate\nlog.info(\"Evaluate Model ... \");\nEvaluation eval = new Evaluation (outputNum);\n# create an evaluation object  with 10 possible classes\n\nwhile (mnistTest.hasNext()) {\n    Dataset next =mnistTest.next();\n    INDArray output=model.output(next.getFeatureMatrix());\n    eval.eval(next.getLabels(), output);\n}\n\nlog.info(eval.stats())\n    \n}\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Quiz DL4J Fundamentals"},{"metadata":{},"cell_type":"markdown","source":"### Which DeepLearning4J sub project is used to manage reading and transforming data?\n## DataVec\n### When importing a Keras Model built using the Functional API into DeepLearning4J using KerasModelImport the DL4J model created will be a..\n## ComputationGraph\n### A Model trained to classify events into 3 classes is trained and deployed for inference. The input is stored in an INDarray named \"input\". The following code generates the output. INDArray output = model.output(input); System.out.println(output); Which of the following outputs are valid, and show 90 percent probability of class 0,the first class.\n## output[0.92, 0.13, 0.03]\n### In order to run a job in Spark using spark-submit which of the following is needed. (Choose all that apply)\n## An Uberjar (also called Jar with dependencies), URL for the spark master, \n### When building a neural network to train on Sequence data which network type is considered the best and most widely used.\n## Recurrent Neural Network\n### Which Process takes more compute resources.\n## Training a Neural Network\n\n"},{"metadata":{},"cell_type":"markdown","source":"# Introduction to PyTorch"},{"metadata":{},"cell_type":"markdown","source":"## Pytorch Installation"},{"metadata":{},"cell_type":"markdown","source":"### PyTorch is a new Deep Learning Framework which is highly dynamic in tons of ways. This framework is is a rising star on the sky of Deep Learning and very, very popular especially by research\n### But first of all, we just going to download and install PyTorch in our Watson Data Science Experience workbook. Just go to pytorch.org then scroll down to the Get Started section and then here, you have a selection. For our Watson Data Science Experience notebooks, we need operating system Linux, we need package manager pip, and we select Python 2.7. And we don't select CUDA because, at least at the moment, we don't have GPU acceleration there."},{"metadata":{"trusted":false},"cell_type":"code","source":"!pip install https://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp27-cp27mu-linux_x86_64.whl","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"!pip install torchvision","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"import torch\nimport torch.autograd as autograd\nimport torch.nn as nn\nimport torch.optim as optim\n\ntorch.manual_seed(123)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Pytorch Pacakges"},{"metadata":{},"cell_type":"markdown","source":"### irst of all, this is the torch library, which is actually the Tensor library of PyTorch. This library is very similar to NumPy. The second one is the torch-autograd library. This is at tape-based automatic differentiation library. What is the meaning of this tape-based and automatic? We're going to see in our next sessions. Here I have torch.nn. This is a neural network library. We are not going to build up a neural network in this introductory sessions. \n### But it's important to know that this library is actually the most important if you would like to build neural network. And here the last one is the torch optim library. This is a torch optimization library which contains all those famous algorithms, optimization algorithms like SGD, Stochastic Gradient Descent, RMSProp, Adam, and so on which we all know from in other frameworks like Keras, and TensorFlow, and so on"},{"metadata":{},"cell_type":"markdown","source":"## Tensor Creation and Visualization of High Dimensional Tensors"},{"metadata":{},"cell_type":"markdown","source":"## Creating Tensors\n### Tensors can be created from Python Lists with the Torch.Tensor Function\n### t's very simple with PyTorch. So first of all, you create an object which is a vector or a matrix, and then you pass this object to tensor class as an argument. So you see here, we have created one vector, one, two and three. And then, we are passing this vector as argument here for the torch class. The same thing with the two or three matrix and higher dimensional object like tensor of a three-by-three-by three. This is the same. In here, we are creating bigger tensor which is dimension of four-by-three-by-three-by-three.\n### So, let us execute this cell. And here, we see, it has indeed created a tensor of size three which is a float tensor. You can see here, torch.FloatTensor, and the second variation has created tensor of dimension of size two-by-three. And here, we have created a tensor of size three-by-three-by-three. And last one, of course, it has dimension four-by-three-by-three matrix. It's simple and straightforward. "},{"metadata":{"trusted":false},"cell_type":"code","source":"# Create a torch.Tensor object from Python List\nv=[1,2,3]\nprint (type(v))\nv_tensor=torch.Tensor(v)\nprint (v_tensor)\n\n# Create a torch.tensor object of size 2x3, from 2x3 matrix\nm2x3 = [[1,2,3],[4,5,6]]\nm2x3_tensor=torch.Tensor(m2x3)\nprint (m2x3_tensor)\n\n# Create 3D torch.Tensor object of size 2x3 from 3x3x3 matrix\nm3x3x3 = [[[1,2,3],[4,5,6],[7,8,9]], [[10,11,12],[13,14,15],[16,17,18]], [[19,20,21],[22,23,24],[25,26,27]]]\nm3x3x3_tensor = torch.Tensor(m3x3x3)\nprint (m3x3x3_tensor)\n\n# Create 4D Tensor from random data and given dimensions  (in this case 3x4x5x6) with torch.randn()\nm4x3x3x3_tensor = torch.randn((4,3,3,3))\nm4x3x3x3_tensor.shape\nprint (m4x3x3x3_tensor)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now, we're going to discuss the question of multidimensional tensors. It's actually important because we are dealing a lot in PyTorch and not only with PyTorch but also with another frameworks like Keras, we are dealing with highly dimensional tensors.\n### So, how can we understand it? For a lower dimensional objects, it's pretty straightforward. Here, we're indexing into a vector which was actually tensor of size three. So now, if we're indexing into this vector, into the stands of size three, and getting the first element with the index zero, we're getting just a scalar. Here, we're indexing into actually matrix but it's not the matrix, it's now a tensor with dimension two-by-three. So, we are getting here first element with index zero, and this first element is, by itself, a tensor which has size three. It's actually a column vector, you can see that column vector.\n### Here, we're indexing into a tensor of dimension three-by-three-by-three or size three-by-three-by-three. And here, we're getting a matrix out of it. It's not a matrix, it's a tensor of size three-by-three. And last one, we are getting an object, a float tensor of size three-by-three-by-three. Geometrically speaking, it's a cube actually."},{"metadata":{"trusted":false},"cell_type":"code","source":"# index into v_tensor and get a scalar\nprint (v_tensor[0])\n\n# index into m2x3_tensor and get a VECTOR \nprint (m2x3_tensor[0])\n\n# Index into m3x3x3_tensor and get a matrix\nprint (m3x3x3_tensor[0])\n\n# Index into m4x3x3x3_tensor get a 3D rectangular of size 4x5x6\nprint (m4x3x3x3_tensor[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  have tried to represent a 4D tensor. If we have sliced into the dimension, into this 4D tensor, we've got out of it a cube actually. It's a tensor of size three-by-three-by-three. So, you see here three colors, it's just for fun, and you can see here, three cubes, and this whole image is actually a representation of 4D tensor. So the fourth dimension, you can see as a container for the tensors of dimension minus one of lower dimension, like if you have a tensor which has a size of four-by-three-by-three-by-three, so you have the highest dimension is four.\n### o, this fourth dimension is actually, you can see it as a container for the tensors of lower dimension. Like in our case, we have here cubes like three cubes and we can slice into this tensor, and we're getting out an object, a tensor of dimension three-by-three-by-three. I think it's pretty important to be able to visualize at the beginning some basic structures and we deal with high dimensional objects. "},{"metadata":{},"cell_type":"markdown","source":"## Math Computation and Reshape "},{"metadata":{},"cell_type":"markdown","source":"## Operations with Tensors \n### You can operate on tensor in the ways you would expect. web: pytorch.org/docs/torch.html for complete list of operations\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"x=torch.Tensor([1,2,3])\ny=torch.Tensor([4,5,6])\nprint (x)\nprint (y)\n\nw=torch.matmul(x,y)  \nprint (w)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  the result of matmul is scalar. Why it so? Because matmul is actually scalar multiplication which we know from linear algebra. "},{"metadata":{},"cell_type":"markdown","source":"## Concatenation"},{"metadata":{"trusted":false},"cell_type":"code","source":"# By default, it concatenate along the axis with 0 (rows). its stacking  the rows\nx_1=torch.randn(2,5)\nprint (x_1)\ny_1=torch.randn(3,5)\nprint (y_1)\nz_1=torch.cat([x_1,y_1])\nprint (z_1)\n\n# concatenate columns\nx_2=torch.randn(2,3)\nprint (x_2)\ny_2=torch.randn(2,5)\nprint (y_2)\n# second arg specify which axis to concatenate along. here we select 1(columns). its attaching the columns\n# Another way is to concatenate tensors column wise. For this, we have to specify axis.\n# So if we specify axis of one as a second parameter, it will align the tensors, so it will concatenate column wise.\nz_2=torch.cat([x_2,y_2],1)\nprint(z_2)\n\n# if your tensors are not compatible , torch will complain. Uncomment to see the error\ntorch.cat([x_1,x_2])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### above error\n### So, if we define wrong number, it will not be able to execute this concatenation, I mean wrong number of rows for example. Let's define the second tensor of size three by five. Instead of two by five, and then we execute this, so we get error, inconsistent tensor sizes. It's pretty simple. "},{"metadata":{},"cell_type":"markdown","source":"## Reshaping Tensors\n### We can use the .view() method to reshape a tensor. Often we will need to reshape our data before passing it to neural network.\n### let assume we have 64000RGB images with the size of 28x28 pixels. We can define an array to shape (64,000, 3, 28,28) to hold them where 3 is number of color channels\n### We have 64,000 images which we can for example, analyze with our convolutional neural network.\n### We have 64,000 images which we can for example, analyze with our convolutional neural network. And here, if we want to reshape this tensor, so initially we have a tensor of shape (64,000, 3, 28, 28) what could it be? It could be here on the second position, it could be number of color channels, three, it's actually a usual way to define images. And the last two, we'll use 28, 28, this is the image size. Now, let's assume we have to reshape the whole tensor or this is a tensor, actually torch number, we'll create a tensor, we'll have to reshape it, and we want to introduce a batch size for example. So, we want to see how many batches we can construct out of 64,000 images. \n### o, we have here on the first position, we will define the batch size. Now, we can left unchanged the number of color channels which is three and the image size 28, 28. But here, we can infer the second dimension, we can left unfilled, so with minus one, and it will cost the pytorch will automatically infer the correct second dimension. It takes some time because it calls a random number. Sometimes it takes a moment. So now, we see batch size 32 and then it has insert 2,000, it's great. So, if you multiplied 2,000 with 32, you will get exactly 64,000. "},{"metadata":{"trusted":false},"cell_type":"code","source":"x=torch.randn(64000, 3,28,28)\n# now we want to add a batch dimension of size 32. We can then infer the second dimension by placing -1:\nx_reshaped = x.view(32,-1, 3,28,28)\nprint (x_reshaped.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Computation Graph, CUDA"},{"metadata":{},"cell_type":"markdown","source":"## Commutation Graph & Automatic Differentiation\n### A computational graph is a specification of what parameters with which operations are involved in the commutaiton to give the output . The fundamental class of Pytorch  autograd.Variable keeps track of how it was created\n### oday, we're going to discuss a very important topic. This is the topic of Computational Graphs in PyTorch. As we know in other deep learning frameworks, we also deal with computational graphs like in Keras or in TensorFlow. But, in these frameworks, the computational graph are fixed, so you create a model, and as in Keras for example you define , for example, if it is a neural network you define different layers, optimizer, you define loss function, number of epochs, the batch size, and then you call compile. If you call compile, the run time will create a computational graph of this model and this computational graph is fixed, so you cannot change, if you call model.fit it will execute the computational graph, and during run time you cannot change it anymore, it's fixed. And PyTorch has created something completely different.\n### The creators of PyTorch have decided that they need a flexibility to change the computational graph at the runtime. How did they accomplish this? They have created a component which is called autograd.Variable. This is the main building block of computational graph in PyTorch. How it works? Let us see, let us execute this next cell. So, first of all, we are creating autograd.Variable, we are passing two parameters. One parameter is actually data of this autograd.Variable. This is a torch Tensor of size three, and then we can pass the second argument which is requires_grad, which is the meaning requires gradient, true or false. It means following, it will say requires_grad true, we are saying that this autograd.Variable should track how it was created. This is very important. So, we can print out here the data, which is inside of this autograd.Variable.\n### This is simply the tensor of size three; one, two, three. Then we are creating the next autograd.Variable, which is Y. autograd.Variable, y the same. We are passing this argument, data, this is the first parameter, then we are passing the second parameter, requires gradient true or false, we're passing true. And then we are adding up x and y, and here we are printing the data in the component z, which is the sum of x and y. So, this is the sum, so we just have summed up element wise two vectors. But here, what is very interesting here is that we can also see how z was created. If we write z.grad, function grad_fn, and print out this operation, we see that z was created by add operation. It's very interesting.\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Variable Wrap tensor objects \nx=autograd.Variable (torch.Tensor([1,2,3]), requires_grad=True)\n# you can access the data with the .data attribute\nprint (x.data)\n\ny=autograd.Variable(torch.Tensor([4,5,6]), requires_grad=True)\n\n# with autograd.Variable you can also perform all the same operations you did with tensors\nz=x+y\nprint (z.data)\n\n# we know all that its result of addition of zm laments (Addbackward)\noperation=z.grad_fn\nprint (operation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# let sum up all the entries in z\ns=z.sum()\nprint (s)\nprint (s.grad_fn)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### if we call s grad function, grad fn, we see that it was created as a sum. So, the PyTorch knows exactly for every variable how it was created. Here we are with the next example, in here we have a function s. This function s is the sum of two variable vectors of size three, so we are summing up this vectors element wise. \n### o, we are summing the first element of vector x, with the first element of vector y. The second element with the second and third with the third. So, now if we create partial differentiation, so if we differentiate s with respect to x_0, for example. Due to the rules of partial differentiation, we would deal with other variables other than x, y as with the constants. So, the partial differentiation for s, for the function s, with respect to variable x_0 is one."},{"metadata":{},"cell_type":"markdown","source":"### First we need to run backpropagation and calculate gradients with respect to every variable. Note if you run backward multiple times, the gradient will increment. That is because Pytorch accumulates the gradient  into the .gradproperty , since for many models this is very convenient. Lets now Pytorch computes the gradient and see that we were right with our guess of 1"},{"metadata":{},"cell_type":"markdown","source":"### when u run the below two cells you will get values, but if you run for the second time, the gradient value will be incremented by 1. Second time the value is 2, the third time the value is 3 and so on. lets check it\n### The reason is, that every time you call (Backward>, the gradient property is accumulated. This is a technology of PyTorch which is used for special, there are some models where it's very convenient to use. \n"},{"metadata":{"trusted":false},"cell_type":"code","source":"# calling.backward() on any variable will run backprop, starting from it.\ns.backward(retain_graph=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print (x)\nprint (x.grad)\nprint (y)\nprint (y.grad)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## How to not Break the computational graph (in other words how to preserve )"},{"metadata":{},"cell_type":"markdown","source":"### Lets create tweo troch tensors and add them"},{"metadata":{"trusted":false},"cell_type":"code","source":"x=torch.randn((2,2))\ny=torch.randn((2,2))\nz=x+y # Thses are tensor types , and backprop would not be possible\n\nprint (z)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now we wrap the torch tensors in autograd.Variable. The var_z contains the informaiton for backpropagation"},{"metadata":{"trusted":false},"cell_type":"code","source":"var_x=autograd.Variable(x, requires_grad=True)\nvar_y=autograd.Variable(y, requires_grad=True)\n# var_z contains enough informaiton to compute gradients, as we saw above \nvar_z= var_x+var_y\nprint (var_z.grad_fn)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### But now, what we do is the Following: we are extracting the data out of this sum. Just the data. And passing it to a new variable which is called var_z_data. And then, we are creating actually new variable, new autograd.Variable, and passing this new data variable in it and then we are printing out and then we are trying to print out how it was created. So we're actually printing out new_variable_z.gradient_function.\n### And then you see, none. It's lost, because here, if we are extracting the data. We have extracted only data but not the gradient function. And, yes, then the computational graph at this point is already broken so the grad_fn was not passed and we have retained only data but not the gradient function. "},{"metadata":{},"cell_type":"markdown","source":"### But what happens if we extract the wrapped tensor object out of var_z and rewrap the tensor in new autograd.variable"},{"metadata":{"trusted":false},"cell_type":"code","source":"var_z_data=var_z.data\nnew_var_z=autograd.Variable(var_z_data)\nprint (new_var_z.grad_fn)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ow, if we tried to call (Backward> on this new autograd variable, new_var_z which was created again with the data out of var_z, we will get exception because there's nothing there. Here we have a runtime error that the element of variable does not require grad and does not have a gradient function. This is the important point.\n### The variable chain is not existing anymore, since we have extracted only data and the whole operaitons chain was lost. if we try now to compute on new_var_z, it will throw an error"},{"metadata":{"trusted":false},"cell_type":"code","source":"new_var_z.backward(retain_graph=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## CUDA\n##  would like to briefly mention the CUDA functionality of the Torch Tensor. As you probably remember, the keras in Tensor flow, automatically detect whether you have GPU acceleration on your machine or not. And they will execute everything on the GPU, if your GPU is available, or on CPU if GPU is not available, \n## for PyTorch, you can very granually decide what to execute on GPU and which Tensors you want to execute where, on GPU or on CPU. Here you have a check. And you can check torch.CUDA.is available(). And this check you can run every time if you want to decide where to execute the Tensor. \n### If you want to execute the tensor on CUDA, and CUDA is available, you just add CUDA function,.cuda(). And then it will run this Tensor on GPU. If you don't want to run the Tensor on GPU, you don't add this CUDA function at the end and it will run on CPU. This is very, very flexible. So in my opinion, the main advantage of PyTorch is its huge flexibility. \n### Check whether GPU Acceleration with CUDA is available"},{"metadata":{"trusted":false},"cell_type":"code","source":"# let run this cell only IF CUDA is available\nif torch.cuda.is_available():\n    # create long tensor and transfer it \n    # to GPU as torch.cuda.LongTensor\n    a=torch.LongTensor(10).fill_(3).cuda()\n    print (type(a))\n    b=a.cpu()\n    # transfers it to CPU, back to \n    # being a torch.LongTensor","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Linear Model "},{"metadata":{},"cell_type":"markdown","source":"### In the last session, we have discussed the autograd.Variable element of PyTorch and how to create and preserve the computational graph. We've also briefly mentioned the CUDA functionality of PyTorch. \n### we're going to create a Linear Model with PyTorch. So let us start. So first, we are importing their required libraries, which is torch Library.  torch library is the Tensor library of PyTorch. Then we are importing nn module, neural network module and then we importing autograd from autograd.Variable. As you remember, autograd.Variable is the main building block of PyTorch, a computational graph and here we're importing just numpy. \n### So, first of all, we're creating x, x is just the numbers from zero to 20. Here, we use a nice feature of Python, which is a list comprehension. In the next step, we are So we have created from zero to 19, 20 elements of x and then we have reshaped it to the form 20 to 1. It's a column vector. Almost the same thing with y. But y is now the linear combination of x. So every element of y is five multiplied by x or by index i, which is the same in this case plus two. And then we are creating numpy array out of it of type float32. And then we have reshape to be a column vector. converting this x list to a numpy array and then we reshape it to be a column vector.\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"import torch \nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"x=[i for i in range(20)] # list comprehension\nx_train=np.array(x, dtype=np.float32)\nx_train=x_train.reshape(-1,1)\nprint (x)\nprint (x_train.shape)\n\ny=[(5*i+2) for i in x] # list comprehension\ny_train=np.array(y, dtype=np.float32)\ny_train=y_train.reshape(-1,1)\nprint (y)\nprint (y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### This is the linear combination, actually linear function and the printout is again, of this numpy array, 20 to 1 numpy array. Now, the important point, now, we're creating a linear model with PyTorch, every model with PyTorch is created with class. So you have to create a class. \n### t is pretty straightforward and simple especially for those of you who has already some experience in programming especially in object-oriented programming like in Java or C Sharp or in Objective C, or whatever. You are creating classes. A class actually is a blueprint of an object. Object is a special instance of a class. So if you create a class, you create a template for an object. Object is an acting instance. \n### So now, we're creating class, which we are calling LinearRegressor. So the name you can choose freely whatever you like. But here's the important point, we are inheriting this class from module class of Neural network package of nn library. This is important point, it must be always inherit from module, every PyTorch model class inherits from module. \n### The next, we are defining init() method. Init() is equivalent to a constructor in other programming languages like Java or C Sharp. Here, we don't call the class name but __init__. It just flavor of Python to call this. Constructor will initialize the class, will initiate the class with the values which we are passing here. \n### So we are passing here the instance of this class itself. Then we are passing input dim, input data dimension and here, we are passing output data dimension. And then we call this super class, super is the constructor of a class from which we are inheriting. So we are inheriting from module and we call super is calling to the module class. Super meaning give me the class from which we are inheriting. \n### In here, we are calling the init() method of the super class. Init meaning constructor. And here, the next line of code, we are creating linear object, linear model from this super class. We don't want to create every nitty-gritty. We are here with just getting older stuff from the super class how linear our model is designed? And we are storing this in our variable, which is called linear, self.linear. And this class has only one method, which is called forward. And for this method we are passing the instance of the class itself and we are passing x data. \n### So in here, we see again, calling self.linear and we are printing out what linear function computes. That's it actually and for every model which you are going to create with PyTorch, this is very similar structure which you create. So you always create a class, which you are inheriting from a model and in construct that you are processing what you are actually going to do in this special model. \n### So in this case, it's linear model but there are a lot of other things, which you can take from this. And then module for example, you see here Conv1d, Conv2d, whatever. So even GRU, Gated Recurrent Unit. If you want to create RNN, Recurrent Neural Network of GRU or LSTM. For example, you can also create LSTM, LSTM itself. It's very, very granular. You can create a lot of things but the structure remains always the same.\n### So if you've init, you have backward. So now, we're defining the input dimension, as you see, we need here the input dimension if we want to run this class. Input dimensions is one. So actually, we have one column vector with 20 elements. And it has a one column dimensions is one and the output dimensions is also one. In here, we instantiate this class, meaning we create an instance of a class running object of a class. And two, we're passing input dimension and output dimension. This we have defined in our constructor.\n### So we have defined that we are passing self, self will be passed automatically from the class itself. But for input dimension and output dimension, we have to pass them. So let us execute the cell. You'll see what is model? Model is a LinearRegressor, which inherits from linear, inherits from module and has linear function init, and linear class which is, sorry.\n### A linear class init, and we are passing input features dimension one and output features dimension one. So this is the main building block of our regressor."},{"metadata":{},"cell_type":"markdown","source":"## Create Model Class"},{"metadata":{"trusted":false},"cell_type":"code","source":"class LinearRegressor(nn.Module):\n    def __init__(self,input_dim, output_dim):\n        super(LinearRegressor, self).__init__()\n        self.linear = nn.Linear (input_dim, output_dim)\n    def forward(self, x):\n        out=self.linear(x)\n        return out\ninput_dim=1\noutput_dim=1\n\nmodel= LinearRegressor(input_dim, output_dim)\n\nmodel\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loss & Optimizer"},{"metadata":{},"cell_type":"markdown","source":"### Now, we need to specify loss and optimize the functions. It's actually in any other Deep Learning Framework. So we specify loss function with that MSE, Mean Squared Error Loss. This is also a class which is stored in module and then in neural network and we specify that optimizer with SGD, Stochastic Gradient Descent. This is in a torch.optim package. And here, we have to pass two arguments which are the model parameters, meaning model, which we have created already, and we have also to pass the learning rate, which is here, 0.001. \n### Let us print out this. Execute and print out. So we have created optimizer, and we have created loss function. We can also print loss function. Okay not so much information, MSE loss. It's okay."},{"metadata":{"trusted":false},"cell_type":"code","source":"loss_function = nn.MSELoss()\n\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\noptimizer\nloss_function","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### So we have created both. Now, we are coming to the training. And here, we have only to specify the number of epochs, which I have created pretty high, 500, but our data is very small, and it will be very very fast. And then, we are creating a full loop in the range epochs, so 500 epochs.\n### Now, we are coming to the inner structure of this full loop, and this is very important. This is also pretty the same for any other PyTorch model. So first of all, we increment epoch and then we are creating inputs. Inputs, we have to convert to torch from numpy array, which is done here, torch.from_numpy, and we are passing the training data. This is the data which we are passing to our autograph variable. \n### As you can remember, autograph variable is required to build computational graph which we need to compute gradients later. So we are passing this variable, and we're getting their outputs. Then, we have to reset gradients with zero. So every epoch, we are resetting gradients for the optimizer. The next step, we are creating formats. So we are actually predicting. So we have already our model. \n### We have instantiate our model, which is here, and using this model, we can predict already. So we are predicting. So we are passing the inputs again. We are passing the inputs, and we are getting predicted outputs. The next step is computing the loss. So now, we can compute the loss. We know what are the real outputs. Here, we're passing actually because it's y_train actually.\n### o we have defined, specified y_train. We know what it is, and here, we just converting it to a numpy array and passing to a variable, but these are the real outputs. And here, we have created the predicted outputs and now we are specifying the loss function. And we're passing to the loss function, predicted outputs and real outputs.\n### And then if you'll compute loss, and then we use loss function to go backward and to compute gradients and then we have to go optimizer.step. This is actually the skeleton of every single model training in PyTorch. You will always run the same steps even if you want to create the most, most complicated neuronal network, it will be the same. \n### In PyTorch users also this optimization technique to optimize even the simplest model like linear model or logistic model. It's the same. It will always run the same steps and actually apply the same technique of optimization. And then you have to print out. Actually in PyTorch, you will not have this convenient methods like in Keras where you just specify what do you like to be printed out, and Keras will print out everything for you. Here, you just have to write a small line of code to see the output, but this is the price for this flexibility. \n### So you have here complete flexibility. How do you build your computational graph. You can decide its run time what be in the graph. You can specify whether to run the tensors on cuda, on GPU or on CPU. Everything you can specify the sizes of the tensors. Whatever you like, its run time and you can change its run time. And this is great for the flexibility and my opinion is that the price which you have to pay for it that you have to run some boilerplate code but it's not too much in my opinion. \n### So let us execute this, full loop and train our model. It's very fast. It's already trained. So we have 500 epochs and you see that the loss decreases continuously, it decreases, and then we have we stock by loss 0.2, and actually, we have very good accuracy here. We can actually be proud of us. It's a very simple model, of course, but if you like to create something more complex, you will actually use the same structure"},{"metadata":{"trusted":false},"cell_type":"code","source":"epochs=500\nfor epoch in range(epochs):\n    epoch += 1\n    # convert input and output variables to torch variable\n    inputs=Variable(torch.from_numpy(x_train))\n    \n    real_outputs=Variable(torch.from_numpy(y_train))\n    \n    # Reset Gradients\n    optimizer.zero_grad()\n    \n    # Forward = compute the output\n    pred_outputs = model(inputs)\n    \n    # Loss\n    loss= loss_function(pred_outputs, real_outputs)\n    \n    # Backward - compute gradients\n    loss.backward()\n    \n    # update parameters\n    optimizer.step()\n    \n    print ('epoch {}, loss {}'.format(epoch, loss.data[0]))\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Quiz Pytorch Installation"},{"metadata":{},"cell_type":"markdown","source":"## On which objects the PyTorch operations are running?\n###  On tensors\n## Which size has the tensor created by this operation?, m_tensor = torch.randn((6, 3, 28, 28))\n### 6x3x28x28 , \n## The tensor of wich size returns the following indexing of a tensor of size 5x4x4x4?, m_tensor = torch.randn((5, 4, 4, 4)) m_tensor[0]\n### 4x4x4\n## Given are the following two tensors:\n## x = torch.randn(4, 3), y = torch.randn(3, 4)\n## z = torch.cat([x, y], 1)\n### z = torch.cat([x, y], 1)\n## Which function(s) can you call to reshape a PyTorch tensor?\n### view(args)\n## How is the computation graph created with PyTorch?\n### Computation graph is defined dynamically via autograd.Variable PyTorch components"},{"metadata":{},"cell_type":"markdown","source":"## Supplementary Materials"},{"metadata":{},"cell_type":"markdown","source":"### https://github.com/romeokienzler/developerWorks/tree/master/coursera/ai/week2"},{"metadata":{"trusted":true},"cell_type":"code","source":"m_tensor = torch.randn((5, 4, 4, 4))\nm_tensor[0]","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 2 with Spark 2.1","name":"python2-spark21","language":"python"},"language_info":{"mimetype":"text/x-python","nbconvert_exporter":"python","version":"2.7.14","name":"python","pygments_lexer":"ipython2","file_extension":".py","codemirror_mode":{"version":2,"name":"ipython"}}},"nbformat":4,"nbformat_minor":1}